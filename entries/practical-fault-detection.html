<h1>Practical fault detection & alerts.  You don't need to be a data scientist</h1>

This post covers some observations made around how people think of the path towards better fault/anomaly detection and alerting, debunks some of the myths that ask for over-complicated solutions, and provides some practical solutions that any programmer or sysadmin can implement without becoming a data scientist .
<br/>
<br/>

First of all, a shout out to <a href="http://bosun.org/">bosun</a>, an alerting frontend ("IDE") by Stack Exchange.
I started drafting this post months ago, but I recently got to know bosun and adopted it (and implemented graphite support while I was at it).
It helps showcasing some of the points I'm making here by providing a hands-on implementation for them, and addresses some workflow issues with alerting in general (see below).  This piece of software is a nice leap forward in our monitoring toolkit, and you should check it out.
<br/><a href="https://www.usenix.org/conference/lisa14/conference-program/presentation/brandt">Go watch the presentation video from Lisa14</a>

<br/>
<br/>
That said, the meat of this post aims to help you set up practical fault detection, and to remind you that you don't need to complicate things: sometimes the simple solutions work better than the hard ones.

<h2>It's not all about math</h2>
<p>
I've seen smart people who are good programmers decide to tackle anomaly detection on their timeseries metrics.
This is a good reason to brush up on statistics, so you can apply some of those concepts.
But ironically, in doing so, they often seem to think that they are now only allowed to implement algebraic mathematical formulas. No more if/else, only standard deviations of numbers.  No more for loops, only moving averages. And so on.
<br/>When going from thresholds to something (<i>anything</i>) more advanced, suddenly people only want to work with mathematical formula's.  Meanwhile we have entire Turing-complete programming languages available, which allow us to execute any logic, as simple or as rich as we can imagine.  Using only math massively reduces our options in implementing an algorithm. 
<br/>
<br/>For example I've seen several presentations in which authors demonstrate how they try to fine-tune moving average algorithms and try to get a robust base signal to check against but which is also not affected too much by previous outliers, which raise the moving average and might mask subsequent spikes).  
<br/>
<img src="http://dieter.plaetinck.be/files/fault-detection-moving-average.png">
from <a href="https://speakerdeck.com/astanway/a-deep-dive-into-monitoring-with-skyline">A Deep Dive into Monitoring with Skyline</a>
<br/>
<br/>
But you can't optimize both, because a mathematical formula can't make the distinction between past "good" vs "bad" data.
<br/>However: we wrap the output of any such algorithm with some code that decides what is a fault (or "anomaly" as labeled here) and alerts against it, so why would we hold ourselves back in feeding this useful information back into the algorithm?
<br/>I.e. <b>assist the math with logic</b> by writing some code to make it work better for us:  In this example, we could modify the code to just retain the old moving average from before the time-frame we consider to be problematic.  That way, when the anomaly passes, we resume "where we left off".  For timeseries that exhibit seasonality and a trend, we need to do a bit more, but the idea stays the same.
</p>
<p>
Another example: During his <a href="https://coderanger.net/talks/echo/">Monitorama talk</a>, Noah Kantrowitz made the interesting and thought provoking observation that Nagios flap detection is basically a low-pass filter.  A few people suggested re-implementing flap detection as a low-pass filter.  This seems backwards to me because reducing the problem to a pure mathematical formula loses information.  The code has the high-resolution view of above/below threshold and can visualize as such.  Why throw that away and limit your visibility?
</p>

<h2>Unsupervised machine learning... let's not get ahead of ourselves.</h2>
<a href="https://codeascraft.com/2013/06/11/introducing-kale/">Etsy's Kale</a> has ambitious goals: you configure a set of algorithms, and those algorithms get applied to <b>all</b> of your timeseries.  Out of that should come insights into what's going wrong.
<br/>I have quite a variety amongst my metrics.  Diskspace metrics exhibit a sawtooth pattern (due to constant growth and periodic cleanup),
crontabs cause (by definition) periodic spikes in activity, user activity causes a fairly smooth graph which is characterized by its daily pattern and often some seasonality and a long-term trend.
<br/>
<br/>
<img width="70%" src="http://dieter.plaetinck.be//files/anomaly-detection-cases.png">
<br/>
<br/>Because they look differently, anomalies and faults look different too.  In fact, within each category there are multiple problematic scenarios. (e.g. user activity should not suddenly drop, but also not be significantly lower than other days, even if the signal stays smooth)
<br/>
<br/>I have a hard time believing that running the same algorithms on all of that data, and doing minimal configuration on them, will produce meaningful results. At least I expect a very low signal/noise ratio.  Unfortunately, of the people who I've asked about their experiences with Kale/Skyline, the only cases where it's been useful is where skyline input has been restricted to a certain category of metrics.  But it's up to you do this filtering (via carbon-relay rules) and potentially running multiple instances to cover different scenarios.
(using something like bosun where you can visualize your series, play with algorithms and see the results in place on current and historical data, seems much more practical)
<br/>"Minimal configuration" is great but this doesn't seem to work.
<br/>
<br/>Some companies (all proprietary) take it a step further and pay tens of engineers to work on algorithms that inspect all of your series, classify them into categories, "learn" them and automatically configure algorithms that will do anomaly detection, so it can alert anytime something looks "different than usual" (not necessarily bad, as in fault detection).  This probably works fairly well, but has a high cost and still can't know everything there is to know about your timeseries.
<br/>
<br/>

I'm <b>suggesting we don't need to make it that fancy</b>:
<ul>
<li>using minimal work of classifying metrics via metric meta-data or rules that parse metric id's, we can automatically infer knowledge of how the series is supposed to behave. (e.g. assume that disk_mb_used looks like sawtooth, frontend_requests_per_s daily seasonal, etc).
<br/>Any sysadmin or programmer can do this, it's a bit of work but should make a hands-off automatic system such as Kale more accurate.
<br/>Of course, adopting <a href="http://metrics20.org/">metrics 2.0</a> will help with this as well. Another problem with machine learning is they would have to infer how metrics relate against each other, whereas with metric metadata this can easily be inferred (e.g.: what are the metrics for different machines in the same cluster, etc)</li>
<li>hooking into configuration management/the Paas brain/...: you probably already have some service, tool, or file that knows how your infrastructure looks like or what services run where.  We know where user-facing apps run, where crontabs run, where we store log files. We know in what ratios traffic is balanced across which nodes, and so on.  So alerting systems can leverage this information to apply better suited fault detection rules.  And you don't need a large machine learning infrastructure for it. (as an aside: I have a lot more ideas on cloud-monitoring integration)</li>
<li>More scientists are working on finding cause and affect when different series exhibit anomalies.  But again here, a simple model of the infrastructure (via CM or Paas) gives you service dependencies in a much easier way.</li>
<li>hook into your event tracking. If you have something like <a href="https://github.com/Dieterbe/anthracite/">anthracite</a> that lists upcoming press releases, then your monitoring system knows not to alert if suddenly traffic is a bit higher.  In fact, you might want to alert if your announcement did not create a sudden increase in traffic.  If you have a large scale infrastructure, you might go as far as tagging upcoming maintenance windows with metadata so the monitoring knows which services or hosts will be affected (and which shouldn't).
</ul>


<h2>The trap of complex event processing... no need to abandon familiar tools</h2>
On your quest into better alerting, you soon read and hear about real-time stream processing, and CEP (complex event processing) systems.
It's not hard to be convinced on their merits:  who wouldn't want real-time as-soon-as-the-data-arrives-you-can-execute-logic-and-fire-alerts?
<br/>They also come with a fairly extensive and flexible language that lets you program or compose monitoring rules using your domain knowledge.
I believe I've heard of <a href="https://storm.apache.org/">storm</a> for monitoring, but <a href="http://riemann.io/">Riemann</a> is the best known of these tools that focus on open source monitoring.
It is a nice, powerful tool and probably the easiest of the CEP tools to adopt.  It can also produce very useful dashboards.
However, these tools come with their own API or language, and programming against real-time streams is quite a paradigm shift which can be hard to justify.  And while their architecture works well for large scale situations, the performance difference is often not worth it for most shops I know:  it's a lot easier (albeit less efficient) to just query a datastore over and over and program in the language you're used to.  With a decent timeseries store (or one written to hold the most recent data in memory such as <a href="https://github.com/dgryski/carbonmem">carbonmem</a>) this is not an issue, and the difference in timeliness of alerts becomes negligible!


<h2>An example: finding spikes</h2>
Like many places, we were stuck with static thresholds, which don't cover some common failure scenarios.
So I started thinking about what categories of timeseries do we have, what kind of issues can arise, what does that look like in the data, and what's the simplest way I can detect them?
<br/>
Our most important data is the user-driven category from above.  And the most common problem (at least in my experience) is spikes in the data.  I.e. a sudden drop in requests/s or a sudden spike in response time.  As it turned out, this is much easier to detect than one might think.
<br/>
<img src="http://dieter.plaetinck.be/files/poor-mans-fault-detection.png">
<br/>
In this example I just track the standard deviation of a moving window of 10 points.  Any sudden spike bumps the standard deviation.   We can then simply set a threshold on the deviation.  Fairly trivial to set up, but has been highly effective for me.  The only thing is you need to manually declare what is an acceptable standard deviation value to be compared against.  But you typically don't have so many of these rules that this becomes a problem.
<a href="https://groups.google.com/forum/#!topic/it-telemetry/Zb2H4DP6qtk">more info @ it-telemetry thread</a>
<br/>Note that bosun provides band() and graphiteBand() functions which lets you query previous time windows (such as same timeframe last week, the week before, and 3 weeks ago), which means we can automatically compute an appropriate deviation threshold.  (in theory susceptible to previous outliers, in practice not so much, if you select several timeframes)
<br/>
<br/>
So don't worry about the fancy anomaly detection machine learning stuff, the advanced math, or real-time programming: you can go a long way by trying to come up with some code and setting up a few rules yourself.  (you can <a href="http://obfuscurity.com/2012/05/Polling-Graphite-with-Nagios">use a script to query Graphite from a Nagios check and do your logic</a> or use <a href="http://bosun.org/">bosun</a>).  I've been using <a href="http://vimeo.github.io/graph-explorer/">Graph-Explorer's</a> alerting feature and am now moving my non-trivial cases to bosun.  Graphite has a decent API which works well at aggregating and transforming one or more series into other series.  The bosun language is very complimentary and is great at reducing series to single numbers, boolean logic, and so on, which you need to declare alerting expressions.
<!--
divideSeries(stdev(avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)),10),avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)))

why keepLastValue?
* sumSeries -> none counts as 0, so you can experience big drops which would trigger anomaly detection or failover
* averageSeries -> effectively ignores none values, so your accuracy can drop a lot in light of none values.

of course this masks when your monitoring breaks, so you still need something else to detect anomalies in the "out-of-date-ness" of your points.
-->

<h2>Workflow is key: a closer look at bosun</h2>
Bosun is a one-of-a-kind open source alerting tool that focuses on the workflow.  It'll explain why this useful, and it'll also become apparent how this ties into various things I brought up before:
<ul>
<li>in each rule you can query any data you need from your datasources (currently graphite, openTSDB, and elasticsearch).  You can call various <a href="http://bosun.org/configuration.html">functions, boolean logic, and math</a>.  Although it doesn't expose you a full programming language, the bosun language as it stands is fairly complete, and can be extended to cover
new needs.  You choose your own alerting granularity (it can automatically instantiate alerts for every host/service/$your_dimension/... it finds, but you can also trivially aggregate across dimensions, or both).  This makes it easy to create advanced alerts that cover a lot of ground, making sure you don't get overloaded</li>
<li>you can define your own templates for alert emails, which can contain any html and you can even trivially plot graphs.  Clear, context-rich alerts!</li>
<li>as you iterate on alerting rules, you can execute them on historical data and see how they perform over time.  You can see previews of how the alert emails will be rendered</li>
</ul>

The <a href="http://bosun.org/examples.html">examples</a> section gives you an idea of the things you can do.


<h2>Conclusion</h2>
Many of us aren't ready for some of the new technologies, and the technology isn't ready for us. (and perhaps never will).
As an end-user investigating your options, it's easy to get lured in a direction that promotes overcomplication.
<br/>Taking a step back, it becomes apparent we <b>can</b> setup automated fault detection.  But instead of using machine learning, use metadata, instead of trying to come up with all-encompassing holy grail of math, use several rules of code that you prototype and iterate over, then share for similar cases.  Instead of requiring a paradigm shift, use a language you're familiar with.  Especially by polishing up the workflow, we can make many "manual" tasks much easier and quicker.  I believe we can keep polishing up the workflow, distilling common patterns into macros or functions that can be reused, leveraging metric metadata and other sources of truth to configure fault detection, and perhaps even introducing "metrics coverage", akin to "code coverage": Verify how much, and which of the metrics are adequately represented in alerting rules, so we can easily spot which metrics have yet to be included in alerting rules.  I think there's a lot low hanging fruit, but we have to look in the right direction.

<h2>PS: leveraging metrics 2.0 for anomaly detection</h2>
In my last <a href="https://www.usenix.org/conference/lisa14/conference-program/presentation/plaetinck">metrics 2.0 talk, at LISA14</a> I explored a few ideas on leveraging metrics 2.0 metadata for alerting and anomaly detection, such as automatically discovering error metrics across the stack, getting high level insights via tags, correlation, etc. If you're interested, it's in the video from 24:55 until 29:40
<br/>
<br/>
<center>
<img src="http://dieter.plaetinck.be/files/metrics20-alerting.png" width="50%">
</center>

<!--
It's not about alerts anyway.

alerts are an immensely crude approach to raising operator awareness.
They are basically boolean: either they interrupt your workflow or they don't.  There's no in between.
Yes, you can just check your alert emails "once in a while", but then realize that after an email or text is sent,
there is no way to update them with new information.  Which is really limiting once you start thinking about it.
Updates have to be provided via new "alerts", or they are available in the monitoring interface but there's no way to tell
by just glancing at your alert overview.  You might be looking at very out of date information.
-> only sent alerts for critical things.
-->

