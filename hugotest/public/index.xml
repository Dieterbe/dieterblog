<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dieter&#39;s blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Dieter&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 03 May 2015 23:01:07 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Sun, 03 May 2015 23:01:07 -0400</pubDate>
      
      <guid>http://localhost:1313/about/</guid>
      <description>&lt;br/&gt;

&lt;br/&gt;

&lt;p&gt;

Hi there, visitor.

My name is Dieter Plaetinck.

&lt;br/&gt;I enjoy backend programming, performance optimisation (in a very broad sense), music, drums, travel, structured data processing, sharing and learning.

&lt;br/&gt;There&#39;s a time and a place for everything in life, so I to be strategic about focusing on one or two things at a time.

&lt;br/&gt;Right now it&#39;s mainly backend programming in Go and python, telemetry (timeseries processing, analytics, visualisation) and that&#39;s what I blog about the most.  But you can find some other things on here too.



&lt;br/&gt;

&lt;br/&gt;

I had a blast working on &lt;a href=&#34;http://localhost:1313/tag/netlog&#34;&gt;netlog&lt;/a&gt;, a large social network in Europe, but we lost the battle against facebook.  Nowadays I work on the backend of &lt;a href=&#34;http://localhost:1313/tag/vimeo&#34;&gt;vimeo&lt;/a&gt;, the high quality video sharing website, where I focus on telemetry and big data.

Some people still know me from the &lt;a href=&#34;http://localhost:1313/tag/uzbl&#34;&gt;Uzbl&lt;/a&gt; minimalist web browser or my work on the &lt;a href=&#34;http://localhost:1313/tag/arch&#34;&gt;Arch Linux installer and releases&lt;/a&gt; or my &lt;a href=&#34;http://stopabusingsiprefixes.org/&#34;&gt;stop abusing SI prefixes&lt;/a&gt; website. (One of my pet peeves is correct application of terminology and concepts, I see this as a requirement for calling yourself an engineer or scientist)

&lt;br/&gt;

&lt;br/&gt;Nowadays my open source efforts revolve mostly around &lt;a href=&#34;http://localhost:1313/tag/monitoring&#34;&gt;monitoring (telemetry)&lt;/a&gt; projects, and in specific the &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics 2.0&lt;/a&gt; standardisation effort for timeseries metadata.

&lt;/p&gt;



&lt;p&gt;

You can email me on first-name at last-name dot be.  (i.e just like the domain)

&lt;/p&gt;



&lt;p&gt;

Here are some links you might find interesting.



&lt;ul&gt;

&lt;li&gt;&lt;a href=&#34;https://github.com/Dieterbe&#34;&gt;github.com/Dieterbe&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://twitter.com/Dieter_be&#34;&gt;twitter.com/Dieter_be&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/dieterplaetinck&#34;&gt;LinkedIn.com/in/dieterplaetinck&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://vimeo.com/dieterplaetinck&#34;&gt;vimeo.com/dieterplaetinck&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

&lt;/p&gt;



I use &lt;a href=&#34;http://pyblosxom.github.io/&#34;&gt;pyblosxom&lt;/a&gt; as blogging engine (though looking at &lt;a href=&#34;http://hugo.spf13.com/&#34;&gt;hugo&lt;/a&gt; as a successor).
</description>
    </item>
    
    <item>
      <title>Talks</title>
      <link>http://localhost:1313/talks/</link>
      <pubDate>Sun, 03 May 2015 23:01:07 -0400</pubDate>
      
      <guid>http://localhost:1313/talks/</guid>
      <description>&lt;h2&gt;Talks&lt;/h2&gt;

&lt;table&gt;

&lt;tbody&gt;

&lt;tr&gt;

    &lt;td&gt;Nov 12, 2014&lt;/td&gt;

    &lt;td&gt;Rethinking Metrics: Metrics 2.0&lt;/td&gt;

    &lt;td&gt;LISA 2014&lt;/td&gt;

    &lt;td&gt;Seattle, Washington&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/plaetinck&#34;&gt;slides, audio, video&lt;/a&gt;

    &lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;May 5, 2014&lt;/td&gt;

    &lt;td&gt;Metrics 2.0&lt;/td&gt;

    &lt;td&gt;Monitorama&lt;/td&gt;

    &lt;td&gt;Portland, Oregon&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;http://metrics20.org/media/&#34;&gt;slides, video&lt;/a&gt;,

        &lt;a href=&#34;http://dietertest.plaetinck.be/monitorama-pdx-metrics20.html&#34;&gt;blogpost&lt;/a&gt;

    &lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Apr 3, 2014&lt;/td&gt;

    &lt;td&gt;Metrics stack 2.0&lt;/td&gt;

    &lt;td&gt;nycdevops meetup&lt;/td&gt;

    &lt;td&gt;NYC&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://metrics20.org/media/&#34;&gt;slides&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Feb 18, 2014&lt;/td&gt;

    &lt;td&gt;Metrics 2.0 &amp; Graph-Explorer&lt;/td&gt;

    &lt;td&gt;FullStack engineering meetup&lt;/td&gt;

    &lt;td&gt;NYC&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://metrics20.org/media/&#34;&gt;slides, video&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Jan 18, 2013&lt;/td&gt;

    &lt;td&gt;Simple Black Box&lt;/td&gt;

    &lt;td&gt;Devopsdays&lt;/td&gt;

    &lt;td&gt;NYC&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;http://devopsdays.org/events/2012-newyork/proposals/SimpleBlackBox/&#34;&gt;devopsdays page&lt;/a&gt;, 

        &lt;a href=&#34;http://localhost:1313/profiling_and_behavior_testing_processes_daemons_devopsdays_nyc.html&#34;&gt;blog&lt;/a&gt;,

        &lt;a href=&#34;https://twitter.com/Dieter_be/status/293377294679027713&#34;&gt;slides&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Feb 6, 2011&lt;/td&gt;

    &lt;td&gt;Can we build a simple, cross-distribution installation framework?&lt;/td&gt;

    &lt;td&gt;Fosdem&lt;/td&gt;

    &lt;td&gt;Brussels, Belgium&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;https://archive.fosdem.org/2011/schedule/event/distro_crossinstall&#34;&gt;fosdem page&lt;/a&gt;, 

        &lt;a href=&#34;http://localhost:1313/can_we_build_a_simple_cross-distribution_installation_framework.html&#34;&gt;blog, slides, video&lt;/a&gt;

    &lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Jul 23, 2010&lt;/td&gt;

    &lt;td&gt;Uzbl - web interface tools which adhere to the unix philosophy&lt;/td&gt;

    &lt;td&gt;Archcon&lt;/td&gt;

    &lt;td&gt;Toronto, Canada&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://dieter.plaetinck.be/back_from_canada_archcon.html&#34;&gt;archcon blogpost, slides, video&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Jul 22, 2010&lt;/td&gt;

    &lt;td&gt;AIF: The Arch Installation Framework&lt;/td&gt;

    &lt;td&gt;Archcon&lt;/td&gt;

    &lt;td&gt;Toronto, Canada&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://dieter.plaetinck.be/back_from_canada_archcon.html&#34;&gt;archcon blogpost, slides, video&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Feb 6, 2010&lt;/td&gt;

    &lt;td&gt;Uzbl lightning talk&lt;/td&gt;

    &lt;td&gt;Fosdem&lt;/td&gt;

    &lt;td&gt;Brussels, Belgium&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;https://archive.fosdem.org/2010/schedule/events/uzbl&#34;&gt;fosdem page&lt;/a&gt;,

        &lt;a href=&#34;http://dieter.plaetinck.be/uzbl_monitoring_aif_talks.html&#34;&gt;blog post&lt;/a&gt;

    &lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;???, 2009&lt;/td&gt;

    &lt;td&gt;Overview of monitoring software&lt;/td&gt;

    &lt;td&gt;Kangaroot showcase&lt;/td&gt;

    &lt;td&gt;Vilvoorde, Belgium&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://dieter.plaetinck.be/uzbl_monitoring_aif_talks.html&#34;&gt;blog post&lt;/a&gt;. video/slides unavailable&lt;/td&gt;

&lt;/tr&gt;

&lt;/tbody&gt;

&lt;/table&gt;



&lt;h2&gt;Other fun public performances&lt;/h2&gt;

&lt;ul&gt;

&lt;li&gt;&lt;a href=&#34;http://localhost:1313/vimeo_holiday_special_and_other_great_videos.html&#34;&gt;Vimeo holiday special 2013&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://vimeo.com/59629623&#34;&gt;trailer&lt;/a&gt; and &lt;a href=&#34;https://vimeo.com/59740798&#34;&gt;full version&lt;/a&gt; of a live metal show with me on drums.  Summer of 2012&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://vimeo.com/89865779&#34;&gt;another metal music video&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;http://www.osnews.com/story/22692/Arch_Linux_Team&#34;&gt;Arch Linux team interview&lt;/a&gt;. Jan 11, 2010&lt;/li&gt;

&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Practical fault detection on timeseries part 2: first macros and templates</title>
      <link>http://localhost:1313/post/practical-fault-detection-on-timeseries-part-2/</link>
      <pubDate>Mon, 27 Apr 2015 09:05:02 -0400</pubDate>
      
      <guid>http://localhost:1313/post/practical-fault-detection-on-timeseries-part-2/</guid>
      <description>In the &lt;a href=&#34;http://localhost:1313/practical-fault-detection-alerting-dont-need-to-be-data-scientist.html&#34;&gt;previous fault detection article&lt;/a&gt;, we saw how we can cover a lot of ground in fault detection with simple methods and technology that is available today.

It had an example of a simple but effective approach to find sudden spikes (peaks and drops) within fluctuating time series.

This post explains the continuation of that work and provides you the means to implement this yourself with minimal effort.

I&#39;m sharing with you:

&lt;ul&gt;

&lt;li&gt;&lt;a href=&#34;http://bosun.org&#34;&gt;Bosun&lt;/a&gt; macros which detect our most common not-trivially-detectable symptoms of problems&lt;/li&gt;

&lt;li&gt;Bosun notification template which provides a decent amount of information&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;http://www.grafana.org&#34;&gt;Grafana&lt;/a&gt; and &lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer&lt;/a&gt; dashboards and integration for further troubleshooting&lt;/li&gt;

&lt;/ul&gt;

We reuse this stuff for a variety of cases where the data behaves similarly and I suspect that you will be able to apply this to a bunch of your monitoring targets as well.

&lt;!--more--&gt;

&lt;h2&gt;Target use case&lt;/h2&gt;

As in the previous article, we focus on the specific category of timeseries metrics driven by user activity.

Those series are expected to fluctuate in at least some kind of (usually daily) pattern, but is expected to have a certain smoothness to it. Think web requests per second or uploads per minute.   There are a few characteristics that are considered faulty or at least worth our attention:



&lt;table&gt;

&lt;tr&gt;

  &lt;td&gt;&lt;img src=&#34;http://localhost:1313/files/practical-alerting-timeseries-good.png&#34;/&gt;&lt;/td&gt;

  &lt;td&gt;&lt;img src=&#34;http://localhost:1313/files/practical-alerting-timeseries-bad-spikes.png&#34;/&gt;&lt;/td&gt;

  &lt;td&gt;&lt;img src=&#34;http://localhost:1313/files/practical-alerting-timeseries-bad-erratic.png&#34;/&gt;&lt;/td&gt;

  &lt;td&gt;&lt;img src=&#34;http://localhost:1313/files/practical-alerting-timeseries-bad-timeseries-median-drop.png&#34;/&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

  &lt;td&gt;&lt;b style=&#34;color: green;&#34;&gt;looks good&lt;/b&gt;&lt;br/&gt;consistent pattern&lt;br/&gt;consistent smoothness&lt;/td&gt;

  &lt;td&gt;&lt;b style=&#34;color: red;&#34;&gt;sudden deviation (spike)&lt;/b&gt;&lt;br/&gt;Almost always something broke or choked.&lt;br/&gt;could also be pointing up. ~ peaks and valleys&lt;/td&gt;

  &lt;td&gt;&lt;b style=&#34;color: red;&#34;&gt;increased erraticness&lt;/b&gt;&lt;br/&gt;Sometimes natural&lt;br/&gt;often result of performance issues&lt;/td&gt;

  &lt;td&gt;&lt;b style=&#34;color: red;&#34;&gt;lower values than usual&lt;/b&gt; (in the third cycle)&lt;br/&gt;Often caused by changes in code or config, sometimes innocent.  But best to alert operator in any case [*]&lt;/td&gt;

&lt;/tr&gt;

&lt;/table&gt;



&lt;br/&gt;

[*] Note that some regular patterns can look like this as well. For example weekend traffic lower than weekdays, etc.  We see this a lot.

&lt;br/&gt;The illustrations don&#39;t portray this for simplicity.   But the alerting logic below supports this just fine by comparing to same day last week instead of yesterday, etc.





&lt;h2&gt;Introducing the new approach&lt;/h2&gt;



The &lt;a href=&#34;http://localhost:1313/practical-fault-detection-alerting-dont-need-to-be-data-scientist.html&#34;&gt;previous article&lt;/a&gt; demonstrated using graphite to compute standard deviation.

This let us alert on the erraticness of the series in general and as a particularly interesting side-effect, on spikes up and down.

The new approach is more refined and concrete by leveraging some of bosun&#39;s and Grafana&#39;s strengths.  We can&#39;t always detect the last case above via erraticness checking (a lower amount may be introduced gradually, not via a sudden drop) so now we monitor for that as well, covering all cases above.



We use 

&lt;ul&gt;

&lt;li&gt;Bosun macros which encapsulate all the querying and processing&lt;/li&gt;

&lt;li&gt;Bosun template for notifications&lt;/li&gt;

&lt;li&gt;A generic Grafana dashboard which aids in troubleshooting&lt;/li&gt;

&lt;/ul&gt;

We can then leverage this for various use cases, as long as the expectations of the data are as outlined above.

We use this for web traffic, volume of log messages, uploads, telemetry traffic, etc.

For each case we simply define the graphite queries and some parameters and leverage the existing mentioned Bosun and Grafana configuration.

&lt;br/&gt;

&lt;p&gt;

The best way to introduce this is probably by showing how a notification looks like:

&lt;br/&gt;

&lt;center&gt;

&lt;a href=&#34;http://localhost:1313/files/practical-alerting-dm-notification.png&#34;&gt;&lt;img height=&#34;600px&#34; src=&#34;http://localhost:1313/files/practical-alerting-dm-notification.png&#34;/&gt;&lt;/a&gt;

&lt;br/&gt;

(image redacted to hide confidential information

&lt;br/&gt;the numbers are not accurate and for demonstration purposes only)

&lt;/center&gt;

&lt;p&gt;

As you can tell by the sections, we look at some global data (for example &#34;all web traffic&#34;, &#34;all log messages&#34;, etc), and also

by data segregated by a particular dimension (for example web traffic by country, log messages by key, etc)

&lt;br/&gt;

To cover all problematic cases outlined above, we do 3 different checks:

(note, everything is parametrized so you can tune it, see further down)

&lt;ul&gt;

&lt;li&gt;Global volume: comparing the median value of the last 60 minutes or so against the corresponding 60 minutes last week and expressing it as a &#34;strength ratio&#34;.  Anything below a given threshold such as 0.8 is alerted on&lt;/li&gt;

&lt;li&gt;Global erraticness. To find all forms of erraticness (increased deviation), we use a refined formula.  See details below.  A graph of the input data is included so you can visually verify the series&lt;/li&gt;

&lt;li&gt;On the segregated data: compare current (hour or so) median against median derived from the corresponding hours during the past few weeks, and only allow a certain amount of standard deviations difference&lt;/li&gt;

&lt;/ul&gt;



If any, or multiple of these conditions are in warning or critical state, we get 1 alert that gives us all the information we need.

&lt;br/&gt;

Note the various links to GE (Graph-Explorer) and Grafana for timeshifts.

The Graph-Explorer links are just standard GEQL queries, I usually use this if i want to be easily manage what I&#39;m viewing (compare against other countries, adjust time interval, etc) because that&#39;s what GE is really good at.

The timeshift view is a Grafana dashboard that takes in a Graphite expression as a template variable, and can hence be set via a GET parameter by using the url  &lt;pre&gt;http://grafana/#/dashboard/db/templatetimeshift?var-patt=expression&lt;/pre&gt;

It shows the current past week as red dots, and the past weeks before that as timeshifts in various shades of blue representing the age of the data. (darker is older).

&lt;br/&gt;

&lt;br/&gt;



&lt;a href=&#34;http://localhost:1313/files/practical-alerting-screenshot-template-timeshift.png&#34;&gt;&lt;img width=&#34;50%&#34; src=&#34;http://localhost:1313/files/practical-alerting-screenshot-template-timeshift.png&#34; /&gt;&lt;/a&gt;

&lt;br/&gt;

This allows us to easily spot when traffic becomes too low, overly erratic, etc as this example shows:

&lt;br/&gt;

&lt;br/&gt;



&lt;a href=&#34;http://localhost:1313/files/practical-alerting-timeshift-use.png&#34;&gt;&lt;img width=&#34;50%&#34; src=&#34;http://localhost:1313/files/practical-alerting-timeshift-use.png&#34; /&gt;&lt;/a&gt;

&lt;br/&gt;



&lt;h2&gt;Getting started&lt;/h2&gt;



Note: I Won&#39;t explain the details of the bosun configuration.  Familiarity with bosun is assumed.  The &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun documentation&lt;/a&gt; is pretty complete.

&lt;br/&gt;

&lt;br/&gt;

&lt;a href=&#34;https://gist.github.com/Dieterbe/d1892fa0b4454b892216&#34;&gt;Gist with bosun macro, template, example use, and Grafana dashboard definition&lt;/a&gt;.  Load the bosun stuff in your bosun.conf and import the dashboard in Grafana.

&lt;br/&gt;

&lt;br/&gt;

The pieces fit together like so:



&lt;ul&gt;

&lt;li&gt;The alert is where we define the graphite queries, the name of the dimension segregated by (used in template), how long the periods are, what the various thresholds are and the expressions to be fed into Grafana and Graph-Explorer.

&lt;br/&gt;

It also lets you set an importance which controls the sorting of the segregated entries in the notification (see screenshot).  By default it is based on the historical median of the values but you could override this.  For example for a particular alert we maintain a lookup table with custom importance values.&lt;/li&gt;

&lt;li&gt;The macros are split in two:

&lt;ol&gt;

&lt;li&gt;dm-load loads all the initial data based on your queries and computes a bunch of the numbers.&lt;/li&gt;

&lt;li&gt;dm-logic does some final computations and evaluates the warning and critical state expressions.&lt;/li&gt;

&lt;/ol&gt;

They are split so that your alerting rule can leverage the returned tags from the queries in dm-load to use a lookup table to set the importance variable or other thresholds, such as s_min_med_diff on a case-by-case basis, before calling dm-logic.

&lt;br/&gt;

We warn if one or more segregated items didn&#39;t meet their median requirements, and if erraticness exceeds its threshold (note that the latter can be disabled).

&lt;br&gt;Critical is when more than the specified number of segregated items didn&#39;t meet their median requirements, the global volume didn&#39;t meet the strength ratio, or if erraticness is enabled and above the critical threshold.

&lt;/li&gt;

&lt;li&gt;The template is evaluated and generates the notification like shown above&lt;/li&gt;

&lt;li&gt;Links to Grafana (timeshift) and GE are generated in the notification to make it easy to start troubleshooting&lt;/li&gt;

&lt;/ul&gt;



&lt;h2&gt;Erraticness formula refinements&lt;/h2&gt;

&lt;img style=&#34;float: right; margin: 45px;&#34; width=&#34;30%&#34; src=&#34;http://localhost:1313/files/practical-alerting-notes-cleaned-small.jpg&#34; /&gt;

You may notice that the formula has changed to

&lt;pre&gt;

(deviation-now * median-historical) / ( (deviation-historical * median-now) + 0.01)

&lt;/pre&gt;

&lt;ul&gt;

&lt;li&gt;Current deviation is compared to an automatically chosen historical deviation value (so no more need to manually set this)&lt;/li&gt;

&lt;li&gt;Accounts for difference in volume: for example if traffic at any point is much higher, we can also expect the deviation to be higher.  With the previous formula we would have cases where in the past the numbers were very low, and naturally the deviation then was low and not a reasonable standard to be held against when traffic is higher, resulting in trigger happy alerting with false positives.

&lt;br/&gt;Now we give a fair weight to the deviation ratio by making it inversely proportional to the median ratio&lt;/li&gt;

&lt;li&gt;The + 0.01 is to avoid division by zero&lt;/li&gt;

&lt;/ul&gt;



&lt;!--

streak covers cases where values are very low so that stdev is in the same order (like low volume logs) and we can&#39;t properly use the erraticness or x-deviations. for example logs with very little traffic, datapoints representing healthy traffic can look like (1, 2, 0, 3, 1, ..)

although i think the ratio of medians (median_now/median_then) should work just as well as streak



&lt;img src=&#34;http://localhost:1313/files/practical-alerting-low-values-zeroes.png&#34; /&gt;

&lt;img src=&#34;http://localhost:1313/files/practical-alerting-low-values-low.png&#34; /&gt;

--&gt;



&lt;h2&gt;Still far from perfect&lt;/h2&gt;

While this has been very helpful to us, I want to highlight a few things that could be improved.

&lt;ul&gt;

&lt;li&gt;With these alerts, you&#39;ll find yourself wanting to iteratively fine tune the various parameters and validate the result of your changes by comparing the status-over-time timeline before and after the change.  While Bosun already makes iterative development easier and lets you &lt;a href=&#34;http://bosun.org/public/ss_rule_timeline.png&#34;&gt;run test rules against old data and look at a the status over time&lt;/a&gt;, the interface could be improved by

&lt;ol&gt;

&lt;li&gt;&lt;a href=&#34;https://github.com/bosun-monitor/bosun/issues/636&#34;&gt;showing timeseries (with event markers where relevant) alongside the status visualization&lt;/a&gt;, so you have context to interpret the status timeline&lt;/li&gt;

&lt;li&gt;routinely building &lt;a href=&#34;https://github.com/grafana/grafana/pull/1569&#34;&gt;a knowledge base of time ranges annotated with a given state for a given alerting concern, which would help in validating the generated status timeline, both visually and in code.  We could compute percentage of issues found, missed, etc&lt;/a&gt;. &#34;unit tests for alerting&#34; my boss called it.&lt;/li&gt;

&lt;/ol&gt;

&lt;/li&gt;

&lt;li&gt;Template could be prettier.  In particular the plots often don&#39;t render very well.  We&#39;re looking into closer Grafana-Bosun integration so I think that will be resolved at some point.&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://github.com/bosun-monitor/bosun/issues/719&#34;&gt;Current logic doesn&#39;t take past outages into account. &#34;just taking enough periods in graphiteBand()&#34; helps alleviate it mostly, but it&#39;s not very robust&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;See that drop in the screenshot a bit higher up? That one was preceded by a code deploy event in anthracite which made some changes where a drop in traffic was actually expected.  Would love to be able to mark stuff like this in deploys (like putting in the commit message something like &#34;expect 20-50 drop&#34; and have the monitoring system leverage that.&lt;/li&gt;

&lt;/ul&gt;



&lt;h2&gt;In conclusion&lt;/h2&gt;

I know many people are struggling with poor alerting rules (static thresholds?)

&lt;br/&gt;As I explained in the previous article I fondly believe that the commonly cited solutions (anomaly detection via machine learning) are a very difficult endeavor and results can be achieved much quicker and more simpler.

&lt;br/&gt;While this only focuses on one class of timeseries (it won&#39;t work on diskspace metrics for example) I found this class to be in the most dire need of better fault detection.  Hopefully this is useful to you. Good luck and let me know how it goes!


</description>
    </item>
    
    <item>
      <title>Practical fault detection &amp; alerting.  You don&#39;t need to be a data scientist</title>
      <link>http://localhost:1313/post/practical-fault-detection-alerting-dont-need-to-be-data-scientist/</link>
      <pubDate>Thu, 29 Jan 2015 09:08:02 -0400</pubDate>
      
      <guid>http://localhost:1313/post/practical-fault-detection-alerting-dont-need-to-be-data-scientist/</guid>
      <description>&lt;br/&gt;

As we try to retain visibility into our increasingly complicated applications and infrastructure, we&#39;re building out more advanced monitoring systems.

Specifically, a lot of work is being done on alerting via fault and anomaly detection.

This post covers some common notions around these new approaches, debunks some of the myths that ask for over-complicated solutions, and provides some practical pointers that any programmer or sysadmin can implement that don&#39;t require becoming a data scientist.

&lt;!--more--&gt;

&lt;br/&gt;

&lt;br/&gt;



&lt;h2&gt;It&#39;s not all about math&lt;/h2&gt;

&lt;p&gt;

I&#39;ve seen smart people who are good programmers decide to tackle anomaly detection on their timeseries metrics.

(anomaly detection is about building algorithms which spot &#34;unusual&#34; values in data, via statistical frameworks).  This is a good reason to brush up on statistics, so you can apply some of those concepts.

But ironically, in doing so, they often seem to think that they are now only allowed to implement algebraic mathematical formulas. No more if/else, only standard deviations of numbers.  No more for loops, only moving averages. And so on.

&lt;br/&gt;When going from thresholds to something (&lt;i&gt;anything&lt;/i&gt;) more advanced, suddenly people only want to work with mathematical formula&#39;s.  Meanwhile we have entire Turing-complete programming languages available, which allow us to execute any logic, as simple or as rich as we can imagine.  Using only math massively reduces our options in implementing an algorithm. 

&lt;br/&gt;

&lt;br/&gt;For example I&#39;ve seen several presentations in which authors demonstrate how they try to fine-tune moving average algorithms and try to get a robust base signal to check against but which is also not affected too much by previous outliers, which raise the moving average and might mask subsequent spikes).  

&lt;br/&gt;

&lt;img src=&#34;http://localhost:1313/files/fault-detection-moving-average.png&#34;&gt;

from &lt;a href=&#34;https://speakerdeck.com/astanway/a-deep-dive-into-monitoring-with-skyline&#34;&gt;A Deep Dive into Monitoring with Skyline&lt;/a&gt;

&lt;br/&gt;

&lt;br/&gt;

But you can&#39;t optimize both, because a mathematical formula at any given point can&#39;t make the distinction between past data that represents &#34;good times&#34; versus &#34;faulty times&#34;.

&lt;br/&gt;However: we wrap the output of any such algorithm with some code that decides what is a fault (or &#34;anomaly&#34; as labeled here) and alerts against it, so why would we hold ourselves back in feeding this useful information back into the algorithm?

&lt;br/&gt;I.e. &lt;b&gt;assist the math with logic&lt;/b&gt; by writing some code to make it work better for us:  In this example, we could modify the code to just retain the old moving average from before the time-frame we consider to be faulty.  That way, when the anomaly passes, we resume &#34;where we left off&#34;.  For timeseries that exhibit seasonality and a trend, we need to do a bit more, but the idea stays the same.   Restricting ourselves to only math and statistics cripples our ability to detect actual &lt;b&gt;faults&lt;/b&gt; (problems).

&lt;/p&gt;

&lt;p&gt;

Another example: During his &lt;a href=&#34;https://coderanger.net/talks/echo/&#34;&gt;Monitorama talk&lt;/a&gt;, Noah Kantrowitz made the interesting and thought provoking observation that Nagios flap detection is basically a low-pass filter.  A few people suggested re-implementing flap detection as a low-pass filter.  This seems backwards to me because reducing the problem to a pure mathematical formula loses information.  The current code has the high-resolution view of above/below threshold and can visualize as such.  Why throw that away and limit your visibility?

&lt;/p&gt;



&lt;h2&gt;Unsupervised machine learning... let&#39;s not get ahead of ourselves.&lt;/h2&gt;

&lt;a href=&#34;https://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;Etsy&#39;s Kale&lt;/a&gt; has ambitious goals: you configure a set of algorithms, and those algorithms get applied to &lt;b&gt;all&lt;/b&gt; of your timeseries.  Out of that should come insights into what&#39;s going wrong.  The premise is that the found anomalies are relevant and indicative of faults that require our attention.

&lt;br/&gt;I have quite a variety amongst my metrics.  For example diskspace metrics exhibit a sawtooth pattern (due to constant growth and periodic cleanup),

crontabs cause (by definition) periodic spikes in activity, user activity causes a fairly smooth graph which is characterized by its daily pattern and often some seasonality and a long-term trend.

&lt;br/&gt;

&lt;br/&gt;

&lt;img width=&#34;70%&#34; src=&#34;http://localhost:1313/files/anomaly-detection-cases.png&#34;&gt;

&lt;br/&gt;

&lt;br/&gt;Because they look differently, anomalies and faults look different too.  In fact, within each category there are multiple problematic scenarios. (e.g. user activity based timeseries should not suddenly drop, but also not be significantly lower than other days, even if the signal stays smooth and follows the daily rhythm)

&lt;br/&gt;

&lt;br/&gt;I have a hard time believing that running the same algorithms on all of that data, and doing minimal configuration on them, will produce meaningful results. At least I expect a very low signal/noise ratio.  Unfortunately, of the people who I&#39;ve asked about their experiences with Kale/Skyline, the only cases where it&#39;s been useful is where skyline input has been restricted to a certain category of metrics - it&#39;s up to you do this filtering (perhaps via carbon-relay rules), potentially running multiple skyline instances - and sufficient time is required hand-selecting the appropriate algorithms to match the data.  This reduces the utility.

&lt;br/&gt;&#34;Minimal configuration&#34; sounds great but this doesn&#39;t seem to work.

&lt;br/&gt;

Instead, something like &lt;a href=&#34;http://bosun.org/&#34;&gt;Bosun&lt;/a&gt; (see further down) where you can visualize your series, experiment with algorithms and see the results in place on current and historical data, to manage alerting rules seems more practical.

&lt;br/&gt;

&lt;br/&gt;Some companies (all proprietary) take it a step further and pay tens of engineers to work on algorithms that inspect all of your series, classify them into categories, &#34;learn&#34; them and automatically configure algorithms that will do anomaly detection, so it can alert anytime something looks unusual (though not necessarily faulty). 

This probably works fairly well, but has a high cost, still can&#39;t know everything there is to know about your timeseries, is of no help of your timeseries is behaving faulty from the start and still alerts on anomalous, but irrelevant outliers.

&lt;br/&gt;

&lt;br/&gt;



I&#39;m &lt;b&gt;suggesting we don&#39;t need to make it that fancy&lt;/b&gt; and we can do much better by &lt;b&gt;injecting some domain knowledge&lt;/b&gt; into our monitoring system:

&lt;ul&gt;

&lt;li&gt;using minimal work of classifying metrics via metric meta-data or rules that parse metric id&#39;s, we can automatically infer knowledge of how the series is supposed to behave (e.g. assume that disk_mb_used looks like sawtooth, frontend_requests_per_s daily seasonal, etc) and apply fitting processing accordingly.

&lt;br/&gt;Any sysadmin or programmer can do this, it&#39;s a bit of work but should make a hands-off automatic system such as Kale more accurate.

&lt;br/&gt;Of course, adopting &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics 2.0&lt;/a&gt; will help with this as well. Another problem with machine learning is they would have to infer how metrics relate against each other, whereas with metric metadata this can easily be inferred (e.g.: what are the metrics for different machines in the same cluster, etc)&lt;/li&gt;

&lt;li&gt;hooking into service/configuration management: you probably already have a service, tool, or file that knows how your infrastructure looks like and which services run where.  We know where user-facing apps run, where crontabs run, where we store log files, where and when we run cleanup jobs.  We know in what ratios traffic is balanced across which nodes, and so on.  Alerting systems can leverage this information to apply better suited fault detection rules.  And you don&#39;t need a large machine learning infrastructure for it. (as an aside: I have a lot more ideas on cloud-monitoring integration)&lt;/li&gt;

&lt;li&gt;Many scientists are working on algorithms that find cause and effect when different series exhibit anomalies, so they can send more useful alerts.  But again here, a simple model of the infrastructure gives you service dependencies in a much easier way.&lt;/li&gt;

&lt;li&gt;hook into your event tracking. If you have something like &lt;a href=&#34;https://github.com/Dieterbe/anthracite/&#34;&gt;anthracite&lt;/a&gt; that lists upcoming press releases, then your monitoring system knows not to alert if suddenly traffic is a bit higher.  In fact, you might want to alert if your announcement did not create a sudden increase in traffic.  If you have a large scale infrastructure, you might go as far as tagging upcoming maintenance windows with metadata so the monitoring knows which services or hosts will be affected (and which shouldn&#39;t).

&lt;/ul&gt;

&lt;br/&gt;

Anomaly detection is useful if you don&#39;t know what you&#39;re looking for, or providing an extra watching eye on your log data.  Which is why it&#39;s commonly used for detecting fraud in security logs and such.

For operational metrics of which admins know what they mean, should and should not look like, and how they relate to each other, we can build more simple and more effective solutions.





&lt;h2&gt;The trap of complex event processing... no need to abandon familiar tools&lt;/h2&gt;

On your quest into better alerting, you soon read and hear about real-time stream processing, and CEP (complex event processing) systems.

It&#39;s not hard to be convinced on their merits:  who wouldn&#39;t want real-time as-soon-as-the-data-arrives-you-can-execute-logic-and-fire-alerts?

&lt;br/&gt;They also come with a fairly extensive and flexible language that lets you program or compose monitoring rules using your domain knowledge.

I believe I&#39;ve heard of &lt;a href=&#34;https://storm.apache.org/&#34;&gt;storm&lt;/a&gt; for monitoring, but &lt;a href=&#34;http://riemann.io/&#34;&gt;Riemann&lt;/a&gt; is the best known of these tools that focus on open source monitoring.

It is a nice, powerful tool and probably the easiest of the CEP tools to adopt.  It can also produce very useful dashboards.

However, these tools come with their own API or language, and programming against real-time streams is quite a paradigm shift which can be hard to justify.  And while their architecture and domain specificity works well for large scale situations, these benefits aren&#39;t worth it for most (medium and small) shops I know:  it&#39;s a lot easier (albeit less efficient) to just query a datastore over and over and program in the language you&#39;re used to.  With a decent timeseries store (or one written to hold the most recent data in memory such as &lt;a href=&#34;https://github.com/dgryski/carbonmem&#34;&gt;carbonmem&lt;/a&gt;) this is not an issue, and the difference in timeliness of alerts becomes negligible!





&lt;h2&gt;An example: finding spikes&lt;/h2&gt;

Like many places, we were stuck with static thresholds, which don&#39;t cover some common failure scenarios.

So I started asking myself some questions:

&lt;br&gt;

&lt;br&gt;

&lt;center&gt;

    &lt;i&gt;which behavioral categories of timeseries do we have, what kind of issues can arise in each category,

        &lt;br/&gt;how does that look like in the data, and what&#39;s the simplest way I can detect each scenario?&lt;/i&gt;

&lt;/center&gt;

&lt;br/&gt;

Our most important data falls within the user-driven category from above where various timeseries from across the stack are driven by, and reflect user activity.  And within this category, the most common problem (at least in my experience) is spikes in the data.  I.e. a sudden drop in requests/s or a sudden spike in response time.  As it turned out, this is much easier to detect than one might think:

&lt;br/&gt;

&lt;img style=&#34;float:left; margin:15px;&#34; src=&#34;http://localhost:1313/files/poor-mans-fault-detection.png&#34;&gt;

&lt;br/&gt;

In this example I just track the standard deviation of a moving window of 10 points.  &lt;a href=&#34;http://en.wikipedia.org/wiki/Standard_deviation&#34;&gt;Standard deviation&lt;/a&gt; is simply a measure of how much numerical values differ from each other.  Any sudden spike bumps the standard deviation.   We can then simply set a threshold on the deviation.  Fairly trivial to set up, but has been highly effective for us.

&lt;br/&gt;

&lt;br/&gt;You do need to manually declare what is an acceptable standard deviation value to be compared against, which you will typically deduce from previous data.  This can be annoying until you build an interface to speed up, or a tool to automate this step.

&lt;br/&gt;In fact, it&#39;s trivial to collect previous deviation data (e.g. from the same time of the day, yesterday, or the same time of the week, last week) and automatically use that to guide a threshold.  (&lt;a href=&#34;http://bosun.org/&#34;&gt;Bosun&lt;/a&gt; - see the following section - has &#34;band&#34; and &#34;graphiteBand&#34; functions to assist with this).  This is susceptible to previous outliers, but you can easily select multiple previous timeframes to minimize this issue in practice.

&lt;br/&gt;

&lt;a href=&#34;https://groups.google.com/forum/#!topic/it-telemetry/Zb2H4DP6qtk&#34;&gt;it-telemetry thread&lt;/a&gt;

&lt;br&gt;

&lt;br&gt;

So without requiring fancy anomaly detection, machine learning, advanced math, or event processing, we are able to reliably detect faults using simple, familiar tools.  Some basic statistical concepts (standard deviation, moving average, etc) are a must, but nothing that requires getting a PhD.  In this case I&#39;ve been using &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.10/functions.html#graphite.render.functions.stdev&#34;&gt;Graphite&#39;s stdev function&lt;/a&gt; and &lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer&#39;s&lt;/a&gt; alerting feature to manage these kinds of rules, but it doesn&#39;t allow for a very practical iterative workflow, so the non-trivial rules will be going into &lt;a href=&#34;http://bosun.org/&#34;&gt;Bosun&lt;/a&gt;.

&lt;br/&gt;BTW, you can also &lt;a href=&#34;http://obfuscurity.com/2012/05/Polling-Graphite-with-Nagios&#34;&gt;use a script to query Graphite from a Nagios check and do your logic&lt;/a&gt;

&lt;br/&gt;

&lt;br/&gt;

&lt;br/&gt;

  

&lt;!--

divideSeries(stdev(avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)),10),avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)))



why keepLastValue?

* sumSeries -&gt; none counts as 0, so you can experience big drops which would trigger anomaly detection or failover

* averageSeries -&gt; effectively ignores none values, so your accuracy can drop a lot in light of none values.



of course this masks when your monitoring breaks, so you still need something else to detect anomalies in the &#34;out-of-date-ness&#34; of your points.

--&gt;



&lt;h2&gt;Workflow is key.  A closer look at bosun&lt;/h2&gt;

One of the reasons we&#39;ve been chasing self-learning algorithms is that we have lost faith in the feasibility of a more direct approach.  We can no longer imagine building and maintaining alerting rules because we have no system that provides powerful alerting, helps us keep oversight and streamlines the process of maintaining and iteratively developing alerting.

&lt;br/&gt;I recently discovered &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun&lt;/a&gt;, an alerting frontend (&#34;IDE&#34;) by Stack Exchange, &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/brandt&#34;&gt;presented at Lisa14&lt;/a&gt;.  I highly recommend watching the video.  They have identified various issues that made alerting a pain, and built a solution that makes human-controlled alerting much more doable.  We&#39;ve been using it for a month now with good results (I also gave it support for Graphite).



I&#39;ll explain its merits, and it&#39;ll also become apparent how this ties into some of the things I brought up above:

&lt;img style=&#34;float:left; margin:35px;&#34; src=&#34;http://bosun.org/public/ss_rule_timeline.png&#34; width=&#34;15%&#34;&gt;

&lt;ul&gt;

&lt;li&gt;in each rule you can query any data you need from any of your datasources (currently graphite, openTSDB, and elasticsearch).  You can call various &lt;a href=&#34;http://bosun.org/configuration.html&#34;&gt;functions, boolean logic, and math&lt;/a&gt;.  Although it doesn&#39;t expose you a full programming language, the bosun language as it stands is fairly complete, and can be extended to cover

new needs.  You choose your own alerting granularity (it can automatically instantiate alerts for every host/service/$your_dimension/... it finds within your metrics, but you can also trivially aggregate across dimensions, or both).  This makes it easy to create advanced alerts that cover a lot of ground, making sure you don&#39;t get overloaded by multiple smaller alerts.  And you can incorporate data of other entities within the system, to simply make better alerting decisions.&lt;/li&gt;

&lt;li&gt;you can define your own templates for alert emails, which can contain any html code.  You can trivially plot graphs, build tables, use colors and so on.  Clear, context-rich alerts which contain all information you need!&lt;/li&gt;

&lt;li&gt;As alluded to, the bosun authors spent a lot of time &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/brandt&#34;&gt;thinking about, and solving&lt;/a&gt; the workflow of alerting.  As you work on advanced fault detection and alerting rules you need to be able to see the value of all data (including intermediate computations) and visualize it.  Over time, you will iteratively adjust the rules to become better and more precise.  Bosun supports all of this.  You can execute your rules on historical data and see exactly how the rule performs over time, by displaying the status in a timeline view and providing intermediate values.  And finally, you can see how the alert emails will be rendered &lt;i&gt;as you work on the rule and the templates&lt;/i&gt;&lt;/li&gt;

&lt;/ul&gt;



The &lt;a href=&#34;http://bosun.org/examples.html&#34;&gt;examples&lt;/a&gt; section gives you an idea of the things you can do.

&lt;br/&gt;I haven&#39;t seen anything solve a pragmatic alerting workflow like bosun (hence their name &#34;alerting IDE&#34;), and its ability to not hold you back as you work on your alerts is refreshing. Furthermore, the built-in processing functions are very &lt;b&gt;complimentary to graphite&lt;/b&gt;:

Graphite has a decent API which works well at aggregating and transforming one or more series into one new series; the bosun language is great at reducing series to single numbers, providing boolean logic, and so on, which you need to declare alerting expressions.  This makes them a great combination.

&lt;br/&gt;Of course Bosun isn&#39;t perfect either.  Plenty of things can be done to make it (and alerting in general) better.  But it does exemplify many of my points, and it&#39;s a nice leap forward in our monitoring toolkit.



&lt;h2&gt;Conclusion&lt;/h2&gt;

Many of us aren&#39;t ready for some of the new technologies, and some of the technology isn&#39;t - and perhaps never will be - ready for us.

As an end-user investigating your options, it&#39;s easy to get lured in a direction that promotes over-complication and stimulates your inner scientist but just isn&#39;t realistic.

&lt;br/&gt;Taking a step back, it becomes apparent we &lt;b&gt;can&lt;/b&gt; setup automated fault detection.  But instead of using machine learning, use metadata, instead of trying to come up with all-encompassing holy grail of math, use several rules of code that you prototype and iterate over, then reuse for similar cases.  Instead of requiring a paradigm shift, use a language you&#39;re familiar with.  Especially by polishing up the workflow, we can make many &#34;manual&#34; tasks much easier and quicker.  I believe we can keep polishing up the workflow, distilling common patterns into macros or functions that can be reused, leveraging metric metadata and other sources of truth to configure fault detection, and perhaps even introducing &#34;metrics coverage&#34;, akin to &#34;code coverage&#34;: verify how much, and which of the metrics are adequately represented in alerting rules, so we can easily spot which metrics have yet to be included in alerting rules.  I think there&#39;s a lot of things we can do to make fault detection work better for us, but we have to look in the right direction.



&lt;h2&gt;PS: leveraging metrics 2.0 for anomaly detection&lt;/h2&gt;

In my last &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/plaetinck&#34;&gt;metrics 2.0 talk, at LISA14&lt;/a&gt; I explored a few ideas on leveraging metrics 2.0 metadata for alerting and fault detection, such as automatically discovering error metrics across the stack, getting high level insights via tags, correlation, etc. If you&#39;re interested, it&#39;s in the video from 24:55 until 29:40

&lt;br/&gt;

&lt;br/&gt;

&lt;center&gt;

&lt;img src=&#34;http://localhost:1313/files/metrics20-alerting.png&#34; width=&#34;50%&#34;&gt;

&lt;/center&gt;



&lt;!--

It&#39;s not about alerts anyway.



alerts are an immensely crude approach to raising operator awareness.

They are basically boolean: either they interrupt your workflow or they don&#39;t.  There&#39;s no in between.

Yes, you can just check your alert emails &#34;once in a while&#34;, but then realize that after an email or text is sent,

there is no way to update them with new information.  Which is really limiting once you start thinking about it.

Updates have to be provided via new &#34;alerts&#34;, or they are available in the monitoring interface but there&#39;s no way to tell

by just glancing at your alert overview.  You might be looking at very out of date information.

-&gt; only sent alerts for critical things.

--&gt;
</description>
    </item>
    
    <item>
      <title>IT-Telemetry Google group.  Trying to foster more collaboration around operational insights.</title>
      <link>http://localhost:1313/post/it-telemetry-google-group-collaboration-operational-insights/</link>
      <pubDate>Sat, 06 Dec 2014 16:01:02 -0400</pubDate>
      
      <guid>http://localhost:1313/post/it-telemetry-google-group-collaboration-operational-insights/</guid>
      <description>The discipline of collecting infrastructure &amp; application performance metrics, aggregation, storage, visualizations and alerting has many terms associated with it...  Telemetry. Insights engineering.  Operational visibility.

I&#39;ve seen a bunch of people present their work in advancing the state of the art in this domain:  

&lt;br/&gt;from &lt;a href=&#34;http://mabrek.github.io/&#34;&gt;Anton Lebedevich&#39;s statistics for monitoring series&lt;/a&gt;, &lt;a href=&#34;https://vimeo.com/95069158&#34;&gt;Toufic Boubez&#39; talks on anomaly detection&lt;/a&gt; and Twitter&#39;s work on &lt;a href=&#34;https://blog.twitter.com/2014/breakout-detection-in-the-wild&#34;&gt;detecting mean shifts&lt;/a&gt; to projects such as &lt;a href=&#34;http://flapjack.io/&#34;&gt;flapjack&lt;/a&gt; (which aims to offload the alerting responsibility from your monitoring apps), the &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics 2.0 standardization effort&lt;/a&gt; or &lt;a href=&#34;https://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;Etsy&#39;s Kale stack&lt;/a&gt; which tries to bring interesting changes in timeseries to your attention with minimal configuration.

&lt;br/&gt;

&lt;br/&gt;



Much of this work is being shared via conference talks and blog posts, especially around anomaly and fault detection, and I couldn&#39;t find a location for collaboration, quicker feedback and discussions on more abstract (algorithmic/mathematical) topics or those that cross project boundaries.  So I created the &lt;a href=&#34;https://groups.google.com/forum/#!forum/it-telemetry&#34;&gt;IT-telemetry&lt;/a&gt; Google group.  If I missed something existing, let me know.  I can shut this down and point to whatever already exists.  Either way I hope this kind of avenue proves useful to people working on these kinds of problems.
</description>
    </item>
    
    <item>
      <title>A real whisper-to-InfluxDB program.</title>
      <link>http://localhost:1313/post/a-real-whisper-to-influxdb-program/</link>
      <pubDate>Tue, 30 Sep 2014 08:37:48 -0400</pubDate>
      
      <guid>http://localhost:1313/post/a-real-whisper-to-influxdb-program/</guid>
      <description>The &lt;a href=&#34;http://localhost:1313/graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay.html&#34;&gt;whisper-to-influxdb migration script&lt;/a&gt; I posted earlier is pretty bad.  A shell script, without concurrency, and an undiagnosed performance issue.

I hinted that one could write a Go program using the unofficial &lt;a href=&#34;https://github.com/kisielk/whisper-go&#34;&gt;whisper-go&lt;/a&gt; bindings and the &lt;a href=&#34;https://github.com/influxdb/influxdb/tree/master/client&#34;&gt;influxdb Go client library&lt;/a&gt;.

That&#39;s what I did now, it&#39;s at &lt;a href=&#34;https://github.com/vimeo/whisper-to-influxdb&#34;&gt;github.com/vimeo/whisper-to-influxdb&lt;/a&gt;.

It uses configurable amounts of workers for both whisper fetches and InfluxDB commits,

but it&#39;s still a bit naive in the sense that it commits to InfluxDB one serie at a time, irrespective of how many records are in it.

My series, and hence my commits have at most 60k records, and presumably InfluxDB could handle a lot more per commit, so we might leverage better batching later.  Either way, this way I can consistently commit about 100k series every 2.5 hours (or 10/s), where each serie has a few thousand points on average, with peaks up to 60k points. I usually play with 1 to 30 InfluxDB workers. 

Even though I&#39;ve hit a few &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/985&#34;&gt;InfluxDB&lt;/a&gt; &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/970&#34;&gt;issues&lt;/a&gt;, this tool has enabled me to fill in gaps after outages and to do a restore from whisper after a complete database wipe.


</description>
    </item>
    
    <item>
      <title>InfluxDB as a graphite backend, part 2</title>
      <link>http://localhost:1313/post/influxdb-as-graphite-backend-part2/</link>
      <pubDate>Wed, 24 Sep 2014 07:56:01 -0400</pubDate>
      
      <guid>http://localhost:1313/post/influxdb-as-graphite-backend-part2/</guid>
      <description>&lt;br&gt;

&lt;br/&gt;Updated oct 1, 2014 with a new &lt;i&gt;Disk space efficiency&lt;/i&gt; section which fixes some mistakes and adds more clarity.

&lt;br/&gt;



&lt;p&gt;

The &lt;i&gt;Graphite + InfluxDB&lt;/i&gt; series continues.

&lt;ul&gt;

&lt;li&gt;In part 1, &lt;a href=&#34;http://localhost:1313/on-graphite-whisper-and-influxdb.html&#34;&gt;&#34;On Graphite, Whisper and InfluxDB&#34;&lt;/a&gt; I described the problems of Graphite&#39;s whisper and ceres, why I disagree with common graphite clustering advice as being the right path forward, what a great timeseries storage system would mean to me, why InfluxDB - despite being the youngest project - is my main interest right now, and introduced my approach for combining both and leveraging their respective strengths: InfluxDB as an ingestion and storage backend (and at some point, realtime processing and pub-sub) and graphite for its renown data processing-on-retrieval functionality.

Furthermore, I introduced some tooling: &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt; to easily route streams of carbon data (metrics datapoints) to storage backends, allowing me to send production data to Carbon+whisper as well as InfluxDB in parallel, &lt;a href=&#34;https://github.com/brutasse/graphite-api&#34;&gt;graphite-api&lt;/a&gt;, the simpler Graphite API server, with &lt;a href=&#34;https://github.com/vimeo/graphite-influxdb&#34;&gt;graphite-influxdb&lt;/a&gt; to fetch data from InfluxDB.

&lt;/li&gt;

&lt;li&gt;Not Graphite related, but I wrote &lt;a href=&#34;https://github.com/Dieterbe/influx-cli&#34;&gt;influx-cli&lt;/a&gt; which I introduced &lt;a href=&#34;http://localhost:1313/influx-cli_a_commandline_interface_to_influxdb.html&#34;&gt;here&lt;/a&gt;.  It allows to easily interface with InfluxDB and measure the duration of operations, which will become useful for this article.&lt;/li&gt;

&lt;li&gt;In the &lt;a href=&#34;graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay.html&#34;&gt;Graphite &amp;amp; Influxdb intermezzo&lt;/a&gt; I shared a script to import whisper data into InfluxDB and noted some write performance issues I was seeing, but the better part of the article described the various improvements done to &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt;, which is becoming an increasingly versatile and useful tool.&lt;/li&gt;

&lt;li&gt;In &lt;a href=&#34;http://localhost:1313/using-influxdb-as-graphite-backend-part2.html&#34;&gt;part 2&lt;/a&gt;, which you are reading now, I&#39;m going to describe recent progress, share more info about my setup, testing results, state of affairs, and ideas for future work&lt;/li&gt;

&lt;/ul&gt;

&lt;!--more--&gt;



&lt;h4&gt;Progress made&lt;/h4&gt;

&lt;ul&gt;

&lt;li&gt;InfluxDB saw two major releases:

&lt;ul&gt;

&lt;li&gt;0.7 (and followups), which was mostly about some needed features and bug fixes&lt;/li&gt;

&lt;li&gt;0.8 was all about bringing some major refactorings in the hands of early adopters/testers: support for multiple storage engines, configurable shard spaces, rollups and retention schemes. There was some other useful stuff like speed and robustness improvements for the graphite input plugin (by yours truly) and various things like regex filtering for &#39;list series&#39;.  Note that a bunch of older bugs remained open throughout this release (most notably the broken &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/334&#34;&gt;derivative aggregator&lt;/a&gt;), and a bunch of new ones appeared. Maybe this is why the release was mostly in the dark.  In this context, it&#39;s not so bad, because we let graphite-api do all the processing, but if you want to query InfluxDB directly you might hit some roadblocks.&lt;/li&gt;

&lt;li&gt;An older fix, but worth mentioning: series names can now also contain any character, which means you can easily use &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics2.0&lt;/a&gt; identifiers.  This is a welcome relief after having struggled with Graphite&#39;s restrictions on metric keys.&lt;/li&gt;

&lt;/ul&gt;

&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;http://graphite-api.readthedocs.org&#34;&gt;graphite-api&lt;/a&gt; received various bug fixes and support for templating, statsd instrumentation and caching.

&lt;br/&gt;Much of this was driven by graphite-influxdb: the caching allows us to cache metadata and the statsd integration gives us insights into the performance of the steps it goes through of building a graph (getting metadata from InfluxDB, querying InfluxDB, interacting with cache, post processing data, etc).&lt;/li&gt;

&lt;li&gt;the progress on InfluxDB and graphite-api in turn enabled &lt;a href=&#34;https://github.com/vimeo/graphite-influxdb&#34;&gt;graphite-influxdb&lt;/a&gt; to become faster and simpler (note: graphite-influxdb requires InfluxDB 0.8).  Furthermore you can now configure series resolutions (but different retentions per serie is on the roadmap, see &lt;i&gt;State of affairs and what&#39;s coming&lt;/i&gt;), and of course it also got a bunch of bugfixes.&lt;/li&gt;

&lt;/ul&gt;

Because of all these improvements, all involved components are now ready for serious use.



&lt;h4&gt;Putting it all together, with docker&lt;/h4&gt;

&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; probably needs no introduction, it&#39;s a nifty tool to build an environment with given software installed, and allows to easily deploy it and run it in isolation.

&lt;a href=&#34;https://github.com/vimeo/graphite-api-influxdb-docker&#34;&gt;graphite-api-influxdb-docker&lt;/a&gt; is a very creatively named project that generates the - also very creatively named - docker image &lt;a href=&#34;https://registry.hub.docker.com/u/vimeo/graphite-api-influxdb/&#34;&gt;graphite-api-influxdb&lt;/a&gt;, which contains graphite-api and graphite-influxdb, making it easy to hook in a customized configuration and get it up and running quickly.  This is the recommended way to set this up, and this is what we run in production.



&lt;h4&gt;The setup&lt;/h4&gt;

&lt;ul&gt;

&lt;li&gt;a server running InfluxDB and graphite-api with graphite-influxdb via the docker approach described above:

&lt;pre&gt;

dell PowerEdge R610

24 x Intel(R) Xeon(R) X5660  @ 2.80GHz

96GB RAM

perc raid h700

6x600GB seagate 10k rpm drives in raid10 = 1.6 TB, Adaptive Read Ahead, Write Back, 64 kB blocks, no read caching

no sharding/shard spaces, compiled from git just before 0.8, using LevelDB (not rocksdb, which is now the default)

LevelDB max-open-files = 10000 (lsof shows about 30k open files total for the InfluxDB process), LRU 4096m, everything else is default I think.

&lt;/pre&gt;

&lt;/li&gt;

&lt;li&gt;a server running graphite-web, carbon, and whisper:

&lt;pre&gt;

dell PowerEdge R710

16 x Intel(R) Xeon(R) E5640  @ 2.67GHz

96GB RAM

perc raid h700

8x150GB seagate 15k rm in raid5 = 952 GB, Read Ahead, Write Back, 64 kB blocks, no read caching

MAX_UPDATES_PER_SECOND = 1000  # to sequentialize writes

&lt;/pre&gt;

&lt;/li&gt;

&lt;li&gt;a relay server running carbon-relay-ng that sends the same production load into both.  (about 2500 metrics/s, or 150k minutely)&lt;/li&gt;

&lt;/ul&gt;

As you can tell, on both machines RAM is vastly over provisioned, and they have lots of cpu available (the difference in cores should be negligible), but the difference in RAID level is important to note: RAID 5 comes with a write penalty. Even though the whisper machine has more, and faster disks, it probably has a disadvantage for writes.  Maybe.  Haven&#39;t done raid stuff in a long time, and I haven&#39;t it measured it out.

&lt;br/&gt;&lt;b&gt;Clearly you&#39;ll need to take the results with a grain of salt, as unfortunately I do not have 2 systems available with the same configuration and their baseline (raw) performance is unknown.&lt;/b&gt;.

&lt;br/&gt;Note: no InfluxDB clustering, see &lt;i&gt;State of affairs and what&#39;s coming&lt;/i&gt;.



&lt;h4&gt;The empirical validation &amp;amp; migration&lt;/h4&gt;

Once everything was setup and I could confidently send 100% of traffic to InfluxDB via carbon-relay-ng, it was trivial to run our dashboards with a flag deciding which server to go to.

This way I have literally been running our graphite dashboards next to each other, allowing us to compare both stacks on:

&lt;ul&gt;

&lt;li&gt;visual differences: after a bunch of work and bug fixing, we got to a point where both dashboards looked almost exactly the same.  (note that graphite-api&#39;s implementation of certain functions can behave slightly different, see for example this &lt;a href=&#34;https://github.com/brutasse/graphite-api/issues/66&#34;&gt;divideSeries bug&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;speed differences by simply refreshing both pages and watching the PNGs load, with some assistance from firebug&#39;s network requests profiler.  The difference here was big: graphs served up by graphite-api + InfluxDB loaded considerably faster.  A page with 40 graphs or so would load in a few seconds instead of 20-30 seconds (on both first, as well as subsequent hits).  This is for our default, 6-hour timeframe views.  When cranking the timeframes up to a couple of weeks, graphite-api + InfluxDB was still faster.&lt;/li&gt;

&lt;/ul&gt;

Soon enough my colleagues started asking to make graphite-api + InfluxDB the default, as it was much faster in all common cases.  I flipped the switch and everybody has been happy.

&lt;br/&gt;

&lt;br/&gt;

When loading a page with many dashboards, the InfluxDB machine will occasionally spike up to 500% cpu, though I rarely get to see any iowait (!), even after syncing the block cache (i just realized it&#39;ll probably still use the cache for reads after sync?)

&lt;br/&gt;The carbon/whisper machine, on the other hand, is always fighting iowait, which could be caused by the raid 5 write amplification but the random io due to the whisper format probably has more to do with it.  Via the MAX_UPDATES_PER_SECOND I&#39;ve tried to linearize writes, with mixed success.  But I&#39;ve never gone to deep into it.  So basically &lt;b&gt;comparing write performance would be unfair in these circumstances, I am only comparing reads in these tests&lt;/b&gt;.  Despite the different storage setups, the Linux block cache should make things fair for reads.   Whisper&#39;s iowait will handicap the reads, but I always did successive runs with fully loaded PNGs to make sure the block cache was warm for reads.



&lt;h4&gt;A &#34;slightly more professional&#34; benchmark&lt;/h4&gt;

I could have stopped here, but the validation above was not very scientific.  I wanted to do a somewhat more formal benchmark, to measure read speeds (though I did not have much time so it had to be quick and easy).

&lt;br/&gt;I wanted to compare InfluxDB vs whisper, and specifically how performance scales as you play with parameters such as number of series, points per series, and time range fetched (i.e. amount of points).  I &lt;a href=&#34;https://groups.google.com/forum/#!topic/influxdb/0VeUQCqzgVg&#34;&gt;posted the benchmark on the InfluxDB mailing list&lt;/a&gt;.  Look there for all information. I just want to reiterate the conclusion here:  I was surprised.  Because of the results above, I had assumed that InfluxDB would perform reads noticeably quicker than whisper but this is not the case.  (maybe because whisper reads are nicely sequential - it&#39;s mostly writes that suffer from the whisper format)

&lt;br/&gt;This very much contrasts my earlier findings where the graphite-api+InfluxDB powered dashboards clearly take the lead.  I have yet to figure out why this is.  Maybe something to do with the performance of graphite-web vs graphite-api itself, gunicorn vs apache, worker configuration, or maybe InfluxDB only starts outperforming whisper as concurrency increases.  Some more investigation is definitely needed!



&lt;h4&gt;Future benchmarks&lt;/h4&gt;

The simple benchmark above was very simple to execute, as it only requires influx-cli and whisper-fetch (so you can easily check for yourself), but clearly there is a need to test more realistic scenarios with concurrent reads, and doing some write benchmarks would be nice too.

&lt;br/&gt;We should also look into cpu and memory usage.  I have had the luxury of being able to completely ignore memory usage, but others seem to notice excessive InfluxDB memory usage.

&lt;br/&gt;conclusion: many tests and benchmarks should happen, but I don&#39;t really have time to conduct them.  Hopefully other people in the community will take this on.



&lt;h4&gt;Disk space efficiency&lt;/h4&gt;

Last time I checked, using LevelDB I was pretty close to 24B per record (which makes sense because time, seq_no and value are all 64bit values, and each record has those 3 fields).  (this was with snappy compression enabled, so it didn&#39;t seem to give much benefit).

&lt;br/&gt;Whisper seems to consume 12 Bytes per record - a 32bit timestamp and a 64bit float value - making it considerably more storage efficient than InfluxDB/levelDB for now.

&lt;br/&gt;Some notes on this though:

&lt;ul&gt;

&lt;li&gt;whisper explicitly encodes None values, with InfluxDB those are implied (and require no space).  We have some clusters of metrics that have very sparse data, so whisper gives us a lot of overhead here, but this is different for everyone.  (note: Ceres should also be better at handling sparse data)&lt;/li&gt;

&lt;li&gt;Whisper and Influxdb both explictly encode the timestamp for every record.  Influxdb uses 64bit so you can do very high resolution (up to microseconds), whisper is limited to per-second data.  Ceres AFAIK doesn&#39;t explicitly encode the timestamp at every record, which should also give it a space advantage.&lt;/li&gt;

&lt;li&gt;I&#39;ve been using a data format in InfluxDB where every record is timestamp-sequence_number-value.  It currently works best overall, and so that&#39;s how the graphite ingestion plugin stores it and the graphite-influxdb plugin queries for it.  But it exacerbates the overhead of the timestamp and sequence number.

&lt;br/&gt;We could technically use a row format where we use more variables as part of the record, storing them as columns instead of separate series, which would improve this dynamic (but currently comes with a big tradeoff in performance characteristics - see the &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/582&#34;&gt;column indexes&lt;/a&gt; ticket).

&lt;br/&gt;Another thing is that we could technically come up with a storage format for InfluxDB that is optimized for even-spaced metrics, it wouldn&#39;t need sequence numbers, and timestamps could be implicit instead of explicit, saving a lot of space.  We could even go further and introduce types (int, etc) for values which would consume even less space.

&lt;/ul&gt;

&lt;br/&gt;

It would be great if somebody with more Ceres experience could chip in here, as - in the context of space efficiency - it looks like a neat little format.

Also, I&#39;m probably not making proper use of the compression features that InfluxDB&#39;s storage engines support.  This also requires some more looking into.





&lt;h4&gt;State of affairs and what&#39;s coming&lt;/h4&gt;

&lt;ul&gt;

&lt;li&gt;InfluxDB typically performs pretty well, but not in all cases.  More validation is needed. It wouldn&#39;t surprise me at this point if tools like hbase/Cassandra/riak clearly outperform InfluxDB, as long as we keep in mind that InfluxDB is a young project.  A year, or two, from now, it&#39;ll probably perform much better. (and then again, it&#39;s not all about raw performance.  InfluxDB&#39;s has other strengths)&lt;/li&gt;

&lt;li&gt;A long time goal which is now a reality:  &lt;b&gt;You can use any Graphite dashboard on top of InfluxDB, as long as the data is stored in a graphite-compatible format.&lt;/b&gt;.  Again, the easiest to get running is via &lt;a href=&#34;https://github.com/vimeo/graphite-api-influxdb-docker&#34;&gt;graphite-api-influxdb-docker&lt;/a&gt;.  There are two issues to be mentioned, though:

&lt;ul&gt;

&lt;li&gt;graphite-influxdb needs to query InfluxDB for metadata, and this &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/884&#34;&gt;can be slow&lt;/a&gt;.  If you have millions of metrics, it can take tens of seconds before querying for the data even starts.  I am trying to work with the InfluxDB people on a solution.&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://github.com/brutasse/graphite-api/issues/57&#34;&gt;graphite-api doesn&#39;t work with metric id&#39;s that have equals signs in them&lt;/a&gt;.&lt;/li&gt;

&lt;/ul&gt;

&lt;li&gt;With the 0.8 release out the door, the shard spaces/rollups/retention intervals feature will start stabilizing, so we can start supporting multiple retention intervals per metric&lt;/li&gt;

&lt;li&gt;Because InfluxDB clustering is &lt;a href=&#34;https://github.com/influxdb/influxdb/pull/903&#34;&gt;undergoing major changes&lt;/a&gt;, and because clustering is not a high priority for me, I haven&#39;t needed to worry about this.  I&#39;ll probably only start looking at clustering somewhere in 2015 because I have more pressing issues.&lt;/li&gt;

&lt;li&gt;Once the new clustering system and the storage subsystem have matured (sounds like a v1.0 ~ v1.2 to me) we&#39;ll get more speed improvements and robustness.  Most of the integration work is done, it&#39;s just a matter of doing smaller improvements, bug fixes and waiting for InfluxDB to become better.  Maintaining this stack aside, I personally will start focusing more on:

    &lt;ul&gt;

    &lt;li&gt;per-second resolution in our data feeds, and potentially storage&lt;/li&gt;

    &lt;li&gt;realtime (but basic) anomaly detection, realtime graphs for some key timeseries.  Adrian Cockcroft had an inspirational piece in his &lt;a href=&#34;https://vimeo.com/95064249&#34;&gt;Monitorama keynote&lt;/a&gt; about how alerts from timeseries should trigger within seconds.&lt;/li&gt;

    &lt;li&gt;Mozilla&#39;s awesome &lt;a href=&#34;http://hekad.readthedocs.org&#34;&gt;heka&lt;/a&gt; project (this &lt;a href=&#34;https://vimeo.com/98689689&#34;&gt;heka video&lt;/a&gt; is great), which should help a lot with the above.  Also looking at &lt;a href=&#34;http://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;Etsy&#39;s kale stack&lt;/a&gt; for anomaly detection&lt;/li&gt;

    &lt;li&gt;metrics 2.0 and making sure metrics 2.0 works well with InfluxDB.  Up to now I find the series / columns as a data model too limiting and arbitrary, it could be so much more powerful, ditto for the query language.&lt;/li&gt;

&lt;/ul&gt;

&lt;/li&gt;

&lt;li&gt;Can we do anything else to make InfluxDB (+graphite) faster? Yes!

&lt;ul&gt;

&lt;li&gt;Long term, of course, InfluxDB should have powerful enough processing functions and query syntax, so that we don&#39;t even need a graphite layer anymore.&lt;/li&gt;

&lt;li&gt;A storage engine optimized for fixed intervals would probably help, timestamps and sequence numbers currently consume 2/3 of the record... and there&#39;s no reason to explicitly store either one in this use case.  I&#39;ve even rarely seen people make use of the sequence number in any other InfluxDB use case.  See all the remarks in the &lt;i&gt;Disk space efficiency&lt;/i&gt; section above.  Finally we could have InfluxDB have fill in None values without it doing &#34;group by&#34; (timeframe consolidation), which would shave off runtime overhead.&lt;/li&gt;

&lt;li&gt;Then of course, there are projects to replace graphite-web/graphite-api with a Go codebase: &lt;a href=&#34;https://github.com/graphite-ng/graphite-ng&#34;&gt;graphite-ng&lt;/a&gt; and &lt;a href=&#34;https://github.com/dgryski/carbonapi&#34;&gt;carbonapi&lt;/a&gt;.  the latter is more production ready, but depends on some custom tooling and io using protobufs.  But it performs an order of magnitude better than the python api server!  I haven&#39;t touched graphite-ng in a while, but hopefully at some point I can take it up again&lt;/li&gt;

&lt;/ul&gt;

&lt;li&gt;Another thing to keep in mind when switching to graphite-api + InfluxDB: you loose the graphite composer.  I have a few people relying on this, so I can either patch it to talk to graphite-api (meh), separate it out (meh) or replace it with a nicer dashboard like tessera, grafana or descartes.  (or Graph-Explorer, but it can be a bit too much of a paradigm shift).&lt;/li&gt;

&lt;li&gt;some more InfluxDB stuff I&#39;m looking forward to:

&lt;ul&gt;

&lt;li&gt;binary protocol and result streaming (faster communication and responses!) (the latter might not get implemented though)&lt;/li&gt;

&lt;li&gt;&#34;list series&#34; speed improvements (if metadata querying gets fast enough, we won&#39;t need ES anymore for metrics2.0 index)&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://github.com/influxdb/influxdb/pull/635&#34;&gt;InfluxDB instrumentation&lt;/a&gt; so we actually start getting an idea of what&#39;s going on in the system, a lot of the testing and troubleshooting is still in the dark.&lt;/li&gt;

&lt;/ul&gt;

&lt;/li&gt;

&lt;li&gt;Tracking exceptions in graphite-api is &lt;a href=&#34;https://github.com/brutasse/graphite-api/search?q=exception&amp;type=Issues&amp;utf8=%E2%9C%93&#34;&gt;much harder than it should be&lt;/a&gt;.  Currently there&#39;s no way to display exceptions to the user (in the http response) or to even log them.  So sometimes you&#39;ll get http 500 responses and don&#39;t know why.  You can use the &lt;a href=&#34;http://graphite-api.readthedocs.org/en/latest/configuration.html#extra-sections&#34;&gt;sentry integration&lt;/a&gt; which works all right, but is clunky.  Hopefully this will be addressed soon.&lt;/li&gt;

&lt;/ul&gt;



&lt;h4&gt;Conclusion&lt;/h4&gt;

The graphite-influxdb stack works and is ready for general consumption.  It&#39;s easy to install and operate, and performs well.

It is expected that InfluxDB will over time mature and ultimately meet all my &lt;a href=&#34;http://localhost:1313/on-graphite-whisper-and-influxdb.html&#34;&gt;requirements of the ideal backend&lt;/a&gt;.  It definitely has a long way to go.  More benchmarks and tests are needed.  Keep in mind that we&#39;re not doing large volumes of metrics. For small/medium shops this solution should work well, but on larger scales you will definitely run into issues.  You might conclude that InfluxDB is not for you (yet) (there are alternative projects, after all).

&lt;br/&gt;

&lt;br/&gt;

Finally, a closing thought:

&lt;br/&gt;&lt;i&gt;Having graphs and dashboards that look nice and load fast is a good thing to have, but keep in mind that graphs and dashboards should be a last resort.  It&#39;s a solution if all else fails.  The fewer graphs you need, the better you&#39;re doing.

&lt;br/&gt;How can you avoid needing graphs?  Automatic alerting on your data.

&lt;br/&gt;

&lt;br/&gt;I see graphs as a temporary measure: they provide headroom while you develop an understanding of the operational behavior of your infrastructure, conceive a model of it, and implement the alerting you need to do troubleshooting and capacity planning.  Of course, this process consumes more resources (time and otherwise), and these expenses are not always justifiable, but I think this is the ideal case we should be working towards.&lt;/i&gt;



&lt;br/&gt;

&lt;br/&gt;

Either way, good luck and have fun!
</description>
    </item>
    
    <item>
      <title>Graphite &amp; Influxdb intermezzo: migrating old data and a more powerful carbon relay</title>
      <link>http://localhost:1313/post/graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay/</link>
      <pubDate>Sat, 20 Sep 2014 15:18:32 -0400</pubDate>
      
      <guid>http://localhost:1313/post/graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay/</guid>
      <description>&lt;!--more--&gt;



&lt;h4&gt;Migrating data from whisper into InfluxDB&lt;/h4&gt;



&lt;i&gt;&#34;How do i migrate whisper data to influxdb&#34;&lt;/i&gt; is a question that comes up regularly, and I&#39;ve always replied it should be easy to write a tool

to do this.  I personally had no need for this, until a recent small influxdb outage where I wanted to sync data from our backup server (running graphite + whisper) to influxdb, so I wrote a script:



&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&amp;lt;!&lt;span style=&#34;color: #666666&#34;&gt;[&lt;/span&gt;CDATA&lt;span style=&#34;color: #666666&#34;&gt;[&lt;/span&gt;

&lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;#!/bin/bash&lt;/span&gt;

&lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;# whisper dir without trailing slash.&lt;/span&gt;

&lt;span style=&#34;color: #19177C&#34;&gt;wsp_dir&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;/opt/graphite/storage/whisper

&lt;span style=&#34;color: #19177C&#34;&gt;start&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;$(&lt;/span&gt;date -d &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;sep 17 6am&amp;#39;&lt;/span&gt; +%s&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;)&lt;/span&gt;

&lt;span style=&#34;color: #19177C&#34;&gt;end&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;$(&lt;/span&gt;date -d &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;sep 17 12pm&amp;#39;&lt;/span&gt; +%s&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;)&lt;/span&gt;

&lt;span style=&#34;color: #19177C&#34;&gt;db&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;graphite

&lt;span style=&#34;color: #19177C&#34;&gt;pipe_path&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;$(&lt;/span&gt;mktemp -u&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;)&lt;/span&gt;

mkfifo &lt;span style=&#34;color: #19177C&#34;&gt;$pipe_path&lt;/span&gt;

&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;function&lt;/span&gt; influx_updater&lt;span style=&#34;color: #666666&#34;&gt;()&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;{&lt;/span&gt;

    influx-cli -db &lt;span style=&#34;color: #19177C&#34;&gt;$db&lt;/span&gt; -async &amp;lt; &lt;span style=&#34;color: #19177C&#34;&gt;$pipe_path&lt;/span&gt;

&lt;span style=&#34;color: #666666&#34;&gt;}&lt;/span&gt;

influx_updater &amp;amp;

&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color: #008000&#34;&gt;read &lt;/span&gt;wsp; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;do&lt;/span&gt;

  &lt;span style=&#34;color: #19177C&#34;&gt;series&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;$(&lt;/span&gt;basename &lt;span style=&#34;color: #BB6688; font-weight: bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;wsp&lt;/span&gt;//&lt;span style=&#34;color: #BB6622; font-weight: bold&#34;&gt;\/&lt;/span&gt;/.&lt;span style=&#34;color: #BB6688; font-weight: bold&#34;&gt;}&lt;/span&gt; .wsp&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;)&lt;/span&gt;

  &lt;span style=&#34;color: #008000&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;updating &lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$series&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt; ...&amp;quot;&lt;/span&gt;

  whisper-fetch.py --from&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$start&lt;/span&gt; --until&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$end&lt;/span&gt; &lt;span style=&#34;color: #19177C&#34;&gt;$wsp_dir&lt;/span&gt;/&lt;span style=&#34;color: #19177C&#34;&gt;$wsp&lt;/span&gt;.wsp | grep -v &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;None$&amp;#39;&lt;/span&gt; | awk &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;{print &amp;quot;insert into \&amp;quot;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$series&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;\&amp;quot; values (&amp;quot;$1&amp;quot;000,1,&amp;quot;$2&amp;quot;)&amp;quot;}&amp;#39;&lt;/span&gt; &amp;gt; &lt;span style=&#34;color: #19177C&#34;&gt;$pipe_path&lt;/span&gt;

&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;done&lt;/span&gt; &amp;lt; &amp;lt;&lt;span style=&#34;color: #666666&#34;&gt;(&lt;/span&gt;find &lt;span style=&#34;color: #19177C&#34;&gt;$wsp_dir&lt;/span&gt; -name &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;*.wsp&amp;#39;&lt;/span&gt; | sed -e &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;s#&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$wsp_dir&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;/##&amp;quot;&lt;/span&gt; -e &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;s/.wsp&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$/&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;/&amp;quot;&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;)&lt;/span&gt;

&lt;span style=&#34;color: #666666&#34;&gt;]]&lt;/span&gt;&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;





It relies on the recently introduced asynchronous inserts feature of &lt;a href=&#34;https://github.com/Dieterbe/influx-cli&#34;&gt;influx-cli&lt;/a&gt; - which commits inserts in batches to improve the speed - and the whisper-fetch tool.

&lt;br/&gt;

You could probably also write a Go program using the unofficial &lt;a href=&#34;https://github.com/kisielk/whisper-go&#34;&gt;whisper-go&lt;/a&gt; bindings and the &lt;a href=&#34;https://github.com/influxdb/influxdb/tree/master/client&#34;&gt;influxdb Go client library&lt;/a&gt;.  But I wanted to keep it simple.  Especially when I found out that whisper-fetch is not a bottleneck: starting whisper-fetch, and reading out - in my case - 360 datapoints of a file always takes about 50ms, whereas InfluxDB at first only needed a few ms to flush hundreds of records, but that soon increased to seconds.

&lt;br/&gt;Maybe it&#39;s a bug in my code, I didn&#39;t test this much, because I didn&#39;t need to; but people keep asking for a tool so here you go.  Try it out and maybe you can fix a bug somewhere.  Something about the write performance here must be wrong.



&lt;h4&gt;A more powerful carbon-relay-ng&lt;/h4&gt;

&lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt; received a bunch of love and has been a great help in my graphite+influxdb experiments.

&lt;p&gt;

&lt;a href=&#34;http://localhost:1313/files/carbon-relay-web-ui.png&#34;&gt;&lt;img width=&#34;441&#34; src=&#34;http://localhost:1313/files/carbon-relay-web-ui.png&#34; /&gt;&lt;/a&gt;

&lt;/p&gt;

Here&#39;s what changed:

&lt;ul&gt;

&lt;li&gt;First I made it so that you can adjust routes at runtime while data is flowing through, via a telnet interface.&lt;/li&gt;

&lt;li&gt;Then &lt;a href=&#34;https://github.com/pauloconnor&#34;&gt;Paul O&#39;Connor&lt;/a&gt; built an embedded web interface to manage your routes in an easier and prettier way (pictured above)&lt;/li&gt;

&lt;li&gt;The relay now also emits performance metrics via statsd (I want to make this better by using &lt;a href=&#34;https://github.com/rcrowley/go-metrics&#34;&gt;go-metrics&lt;/a&gt; which will hopefully get &lt;a href=&#34;https://github.com/rcrowley/go-metrics/issues/68&#34;&gt;expvar support&lt;/a&gt; at some point - any takers?).&lt;/li&gt;

&lt;li&gt;Last but not least, I borrowed &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng/tree/master/nsqd&#34;&gt;the diskqueue&lt;/a&gt; code from &lt;a href=&#34;http://nsq.io/&#34;&gt;NSQ&lt;/a&gt; so now we can also spool to disk to bridge downtime of endpoints and re-fill them when they come back up&lt;/li&gt;

&lt;/ul&gt;

Beside our metrics storage, I also plan to put our anomaly detection (currently playing with &lt;a href=&#34;http://hekad.readthedocs.org/en/v0.7.1/&#34;&gt;heka&lt;/a&gt; and &lt;a href=&#34;http://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;kale&lt;/a&gt;) and &lt;a href=&#34;https://github.com/vimeo/carbon-tagger&#34;&gt;carbon-tagger&lt;/a&gt; behind the relay, centralizing all routing logic, making things more robust, and simplifying our system design.  The spooling should also help to deploy to our metrics gateways at other datacenters, to bridge outages of datacenter interconnects.

&lt;br/&gt;

&lt;br/&gt;

I used to think of carbon-relay-ng as the python carbon-relay but on steroids,

now it reminds me more of something like nsqd but with an ability to make packet routing decisions by introspecting the carbon protocol,

&lt;br/&gt;or perhaps Kafka but much simpler, single-node (no HA), and optimized for the domain of carbon streams.

&lt;br/&gt;I&#39;d like the HA stuff though, which is why I spend some of my spare time figuring out the intricacies of the increasingly popular &lt;a href=&#34;http://raftconsensus.github.io/&#34;&gt;raft&lt;/a&gt; consensus algorithm.   It seems opportune to have a simpler Kafka-like thing, in Go, using raft, for carbon streams.

(note: InfluxDB &lt;a href=&#34;https://github.com/influxdb/influxdb/pull/859&#34;&gt;might introduce such a component&lt;/a&gt;, so I&#39;m also a bit waiting to see what they come up with)

&lt;br/&gt;

&lt;br/&gt;

Reminder: notably missing from carbon-relay-ng is round robin and sharding.  I believe sharding/round robin/etc should be part of a broader HA design of the storage system, as I explained in &lt;a href=&#34;http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html&#34;&gt;On Graphite, Whisper and InfluxDB&lt;/a&gt;.  That said, both should be fairly easy to implement in carbon-relay-ng, and I&#39;m willing to assist those who want to contribute it.
</description>
    </item>
    
    <item>
      <title>Influx-cli: a commandline interface to Influxdb.</title>
      <link>http://localhost:1313/post/influx-cli_a_commandline_interface_to_influxdb/</link>
      <pubDate>Mon, 08 Sep 2014 08:36:36 -0400</pubDate>
      
      <guid>http://localhost:1313/post/influx-cli_a_commandline_interface_to_influxdb/</guid>
      <description>&lt;p&gt;

Time for another side project:

&lt;a href=&#34;https://github.com/Dieterbe/influx-cli&#34;&gt;influx-cli&lt;/a&gt;,

a commandline interface to influxdb.

&lt;br/&gt;

Nothing groundbreaking, and it behaves pretty much as you would expect if you&#39;ve ever used

the mysql, pgsql, vsql, etc tools before.

&lt;br/&gt;But I did want to highlight a few interesting features.

&lt;/p&gt;

&lt;!--more--&gt;

&lt;br/&gt;



&lt;p&gt;

&lt;b&gt;You can do things like user management via SQL,

even though influxdb doesn&#39;t have an SQL interface for this.&lt;/b&gt;

&lt;br/&gt;This is much easier than doing curl http requests!

&lt;pre&gt;&lt;![CDATA[

influx&gt; create admin test test

influx&gt; list admin

## 0

                     name root

## 1

                     name test

]]&gt;&lt;/pre&gt;

&lt;/p&gt;



&lt;p&gt;

&lt;b&gt;You can change parameters and re-bind with the new values&lt;/b&gt;

&lt;pre&gt;&lt;![CDATA[

influx&gt; \user test

influx&gt; \pass test

influx&gt; \db graphite

influx&gt; bind

]]&gt;&lt;/pre&gt;

&lt;/p&gt;



&lt;p&gt;

&lt;b&gt;Write your variables (user, pass, host, db, ...) to ~/.influxrc&lt;/b&gt;

&lt;pre&gt;&lt;![CDATA[

influx&gt; writerc

]]&gt;&lt;/pre&gt;

&lt;/p&gt;



&lt;p&gt;

&lt;b&gt;You can even do inserts via SQL, instead of http posts&lt;/b&gt;

&lt;br&gt;I use this often.  This is very useful to script test cases for bug reports etc.

&lt;pre&gt;&lt;![CDATA[

influx&gt; create db issue-1234

influx&gt; \db issue-1234

influx&gt; bind

influx&gt; insert into demo (time, value, tag) values (120000, 10, &#34;hi&#34;)

influx&gt; insert into demo (time, value, tag) values (180000, 20, &#34;hi again&#34;)

influx&gt; select * from demo

## demo

                time sequence_number               value                 tag

       120000.000000      70001.000000                  10                &#34;hi&#34;

       180000.000000      80001.000000                  20          &#34;hi again&#34;

influx&gt; delete db issue-1234

]]&gt;&lt;/pre&gt;

&lt;/p&gt;



&lt;p&gt;

&lt;b&gt;You can send queries on standard input, which is useful in shell commands and scripts.&lt;/b&gt;

&lt;pre&gt;&lt;![CDATA[

$ echo &#39;list series&#39; | influx-cli | wc -l

194722

$ influx-cli &lt;&lt;&lt; &#39;list series&#39; | wc -l

194722

]]&gt;&lt;/pre&gt;

&lt;/p&gt;



&lt;p&gt;

But even better, &lt;b&gt;from inside an influx-cli session, you can send output from any query into any other command.&lt;/b&gt;

&lt;br&gt;In fact you can also &lt;b&gt;write output of queries into external files.&lt;/b&gt;

All this via familiar shell constructs

&lt;pre&gt;&lt;![CDATA[

$ influx-cli

influx&gt; list series | wc -l

194721

influx&gt; list series &gt; list-series.txt

]]&gt;&lt;/pre&gt;



(note: the discrepancy of one line is due to &lt;a href=&#34;https://github.com/shavac/readline/issues/2&#34;&gt;the Go readline library echoing the query&lt;/a&gt;.

&lt;/p&gt;



&lt;p&gt;

&lt;b&gt;You can also toggle options, such as compression or display of timings.&lt;/b&gt;

&lt;br/&gt;This can be very useful to easily get insights of performance of different operations.

&lt;pre&gt;&lt;![CDATA[

influx&gt; \t

timing is now true

influx&gt; select * from foo | wc -l

64637

timing&gt;

query+network: 1.288792048s

displaying   : 457.091811ms

influx&gt; \comp

compression is now disabled

influx&gt; select * from foo | wc -l

64637

timing&gt;

query+network: 969.322374ms

displaying   : 670.736018ms

influx&gt; list series &gt;/dev/null

timing&gt;

query+network: 3.109178142s

displaying   : 65.712027ms

]]&gt;&lt;/pre&gt;

&lt;br/&gt;This has enabled me to pinpoint slow operations and provide evidence when &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/884&#34;&gt;when creating tickets&lt;/a&gt;.

&lt;/p&gt;



&lt;p&gt;

&lt;b&gt;Executing queries and debugging their result data format, works too&lt;/b&gt;

&lt;br/&gt;This is useful when you want to understand the api better or if the database gets support for new queries with a different output format that influx-cli doesn&#39;t understand yet.

&lt;pre&gt;&lt;![CDATA[

influx&gt; raw select * from foo limit 1

([]*client.Series) (len=1 cap=4) {

 (*client.Series)(0xc20b4f0480)({

  Name: (string) (len=51) &#34;foo&#34;,

  Columns: ([]string) (len=3 cap=4) {

   (string) (len=4) &#34;time&#34;,

   (string) (len=15) &#34;sequence_number&#34;,

   (string) (len=5) &#34;value&#34;

  },

  Points: ([][]interface {}) (len=1 cap=4) {

   ([]interface {}) (len=3 cap=4) {

    (float64) 1.410148588e+12,

    (float64) 1,

    (float64) 95.549995

   }

  }

 })

}

]]&gt;&lt;/pre&gt;

&lt;/p&gt;



&lt;p&gt;

And that&#39;s about it.

I&#39;ve found this to be a much easier way to interface with InfluxDB then using the web interface and curl, but YMMV.

&lt;br/&gt;If you were wondering, this is of course built on top of the &lt;a href=&#34;https://github.com/influxdb/influxdb/tree/master/client&#34;&gt;influxdb go client library&lt;/a&gt;, which was overall pretty pleasant to work with.

&lt;br/&gt;Some ideas for future work:

&lt;ul&gt;

&lt;li&gt;&lt;a href=&#34;https://github.com/Dieterbe/influx-cli/issues/2&#34;&gt;bulk insert performance could be better&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;once &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/263&#34;&gt;influxdb can report query execution time&lt;/a&gt; and hopefully also serialization time, the timing output can be more useful.  Right now we can only measure query execution+serialization+network transfer time combined&lt;/li&gt;

&lt;li&gt;my gut feeling says that using something like msgpack instead of json, and/or even streaming the resultset as it is being generated (instead of first building the entire result, then serializing it, then sending it over, then having the client deserialize the entire thing) could really help performance, not just here, but basically anywhere you interface with influxdb.  Though I don&#39;t have hard numbers on this yet.&lt;/li&gt;

&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Darktable: a magnificent photo manager and editor</title>
      <link>http://localhost:1313/post/darktable_magnificent_photo_manager_editor/</link>
      <pubDate>Tue, 12 Aug 2014 08:36:36 -0400</pubDate>
      
      <guid>http://localhost:1313/post/darktable_magnificent_photo_manager_editor/</guid>
      <description>A post about the magnificent &lt;a href=&#34;http://darktable.org/&#34;&gt;darktable&lt;/a&gt; photo manager/editor and why I&#39;m abandoning &lt;a href=&#34;http://localhost:1313/pixie.html&#34;&gt;pixie&lt;/a&gt;



&lt;!--more--&gt;

&lt;br/&gt;

&lt;br/&gt;



&lt;p&gt;

When I wrote &lt;a href=&#34;http://localhost:1313/pixie.html&#34;&gt;pixie&lt;/a&gt;, I was aware of &lt;a href=&#34;http://darktable.org/&#34;&gt;darktable&lt;/a&gt;.

It looked like a neat application with potential to be pretty much what I was looking for, although it

also looked complicated, mainly due to terminology like &#34;darkroom&#34; and &#34;lighttable&#34;, which was a bit off-putting to me

and made me feel like the application was meant for photo professionals and probably wouldn&#39;t work well with

the ideals of a techie with some purist views on how to manage files and keep my filesystems clean.

&lt;/p&gt; 

&lt;p&gt; 

Basically I didn&#39;t want to give the application a proper chance and then rationalized the decision after I made it.

I&#39;m sure psychologists have a term for this behavior.  I try to be aware of these cases and not to fall in the trap,

but this time I was very aware of it and still proceeded, but I think I had a reasonable excuse.

I wanted an app that behaves exactly how I like, I wanted to play with angularjs, it seemed like a fun learning exercise to implement a full-stack program backed by a Go api server and an angularjs interface, with some keybind features and vim-like navigation sprinkled on top.

&lt;/p&gt;

&lt;p&gt;

Pixie ended up working, but I got fed up with some angularjs issues, slow js performance and a list of to-do&#39;s i would need to address before i

would consider pixie feature complete, so only as of a few days ago I started giving darktable the chance it had deserved from the beginning.

&lt;br/&gt;

As it turns out, darktable is actually a fantastic application, and despite some imperfections, the difference is clear enough for me to abandon pixie.&lt;/p&gt;

&lt;center&gt;

&lt;a href=&#34;http://localhost:1313/files/darktable-lighttable.png&#34;&gt;

&lt;img src=&#34;http://localhost:1313/files/darktable-lighttable.png&#34; width=&#34;30%&#34;/&gt;

&lt;/a&gt;

&lt;/center&gt;

&lt;p&gt;

Here&#39;s why I like it:

&lt;ol&gt;

&lt;li&gt;

It stays true to my ideals:  It doesn&#39;t modify your files at all, this is a must for easily synchronizing photo archives with each other and with devices.

You can tag, assign metadata, create edits, etc. and re-size on export.  It stores metadata in a simple sqlite database, and also in xmp files which it puts along with the original files, but luckily you can easily ignore those while syncing.  (I have yet to verify whether you can adjust dates or set GPS info without modifying the actual files, but I had no solution for that either)

&lt;/li&gt;

&lt;li&gt;basically, it&#39;s just well thought out and works well.  the terminology thing is a non-issue.  Just realize that lighttable means the set of pictures in your collection you want to work with, darkroom is the editor where you edit the image, and film roll is a directory with imported images.   Everything else is intuitive&lt;/li&gt;

&lt;li&gt;It has decent tag editing features, and a powerful mechanism to build a selection of images using a variety of criteria using exif data, tags, GPS info, labels, etc. You can make duplicates of an image and make different edits, and treat them as images of their own&lt;/li&gt;

&lt;li&gt;It has &lt;a href=&#34;http://www.darktable.org/usermanual/ch06s03.html.php&#34;&gt;pretty extensive key binding options&lt;/a&gt;, and even provides a &lt;a href=&#34;http://darktable.org/redmine/projects/darktable/wiki/LuaUsage&#34;&gt;lua api&lt;/a&gt; so you can hook in your own plugins.  People are working on a bunch of &lt;a href=&#34;http://darktable.org/redmine/projects/darktable/wiki/LuaScripts&#34;&gt;scripts&lt;/a&gt; already.&lt;/li&gt;

&lt;li&gt;It&#39;s fast. Navigating a 33k file archive, adjusting thumbnail sizes on the fly, iterating fast, works well&lt;/li&gt;

&lt;li&gt;It has good support for non-destructive editing.  It has a variety of editing possibilities, as if it was commercial software&lt;/li&gt;

&lt;li&gt;It has &lt;a href=&#34;http://www.darktable.org/usermanual/index.html.php&#34;&gt;complete documentation&lt;/a&gt;, a &lt;a href=&#34;http://www.darktable.org/category/blog/&#34;&gt;great blog&lt;/a&gt; with plenty of &lt;a href=&#34;http://www.darktable.org/tag/tutorial/&#34;&gt;tutorial articles&lt;/a&gt;, and &lt;a href=&#34;https://www.youtube.com/playlist?list=PLmvlUro_Up1NBX7VK8UUuyWo1B468zEA0&#34;&gt;tutorial videos&lt;/a&gt;&lt;/li&gt;

&lt;/ol&gt;

&lt;/p&gt;

&lt;center&gt;

&lt;a href=&#34;http://localhost:1313/files/darktable-darkroom.png&#34;&gt;

&lt;img src=&#34;http://localhost:1313/files/darktable-darkroom.png&#34; width=&#34;30%&#34;/&gt;

&lt;/a&gt;

&lt;/center&gt;

&lt;p&gt;

I did notice some bugs (including a few crashes), but there&#39;s always a few developers and community members active, on IRC and the bug tracker, so it&#39;s pretty active project and I&#39;m confident/hopeful my issues will be resolved soon.

&lt;!--

Finally I&#39;d love to declare all my exports in a config file (collect files with these tags, export re-sized versions to directory X, etc) rather then this being manual, explicit actions. --&gt;

&lt;br/&gt;I also have a few more ideas for features that would make it closer to my ideals, but as it stands, darktable is already a great application and I&#39;m happy I can deprecate pixie at this point.

I even &lt;a href=&#34;https://github.com/Dieterbe/tmsu-to-darktable&#34;&gt;wrote a script&lt;/a&gt; that automatically does all tag assignments in darktable based on the pixie information in tmsu, to make the transition friction free.

&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Beautiful Go patterns for concurrent access to shared resources and coordinating responses</title>
      <link>http://localhost:1313/post/beautiful_go_patterns_for_concurrent_access_to_shared_resources_and_coordinating_responses/</link>
      <pubDate>Sat, 26 Jul 2014 13:22:32 -0400</pubDate>
      
      <guid>http://localhost:1313/post/beautiful_go_patterns_for_concurrent_access_to_shared_resources_and_coordinating_responses/</guid>
      <description>&lt;p&gt;

It&#39;s a pretty common thing in backend go programs to have multiple coroutines concurrently needing to modify a shared resource,

and needing a response that tells them whether the operation succeeded and/or other auxiliary information.

Something centralized manages the shared state, the changes to it and the responses.

&lt;/p&gt;

&lt;!--more--&gt;



&lt;p&gt;

This is effectively two things.

&lt;/p&gt;



&lt;h2&gt;Pattern one: making access to thread-unsafe data structures thread safe&lt;/h2&gt;

&lt;p&gt;

Making modifications to thread-unsafe data (remember, maps for example are not thread safe in go) in a thread safe way, you can use a select loop that reads

from various channels and enforces that all operations are executed serially, because only one select case can happen at the same time.

I saw this first in &lt;a href=&#34;https://github.com/bitly/statsdaemon/blob/master/statsdaemon.go#L90&#34;&gt;bitly&#39;s statsdaemon&lt;/a&gt; and have since used this in various places, including &lt;a href=&#34;https://github.com/vimeo/statsdaemon&#34;&gt;vimeo/statsdaemon&lt;/a&gt; and &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt;, for example to route metrics (which needs read access to the routes map) while allowing changes to the routes (coming from the telnet admin interface), by having those as two cases in a select statement.  This was my first &#34;aha!&#34; moment.

&lt;/p&gt;



&lt;h2&gt;pattern two: coordinating flow of responses&lt;/h2&gt;

&lt;p&gt;

For the second, after (potentially time consuming) work, returning a response to the invoker, (let&#39;s say in the carbon-relay-ng case where you want to notify whether the route change succeeded) I have so far just passed on references to the admin interface session along with the request, and after completion of the work it would spawn a new goroutine that resumes the session with the given response.  Not the most elegant, but it works.

&lt;/p&gt;



&lt;p&gt;

The other day though, I saw a very interesting pattern for this case. I don&#39;t remember where (probably one of the gophercon presentations)

or what it&#39;s called. 

But the idea is you can simply use one shared channel for all requests, and one shared channel for all responses.

As long as the requesters write a request to the requests channel and then read a response from the other channel, and the coordinator first reads a request and then writes the response, no further synchronization is needed.  Here&#39;s a demo program:

&lt;/p&gt;



&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&amp;lt;![CDATA[

&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;package&lt;/span&gt; main



&lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;// demo the fact that we can just use one shared req and one resp channel.&lt;/span&gt;

&lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;// as long as they are unbuffered, the synchronization works just fine.&lt;/span&gt;



&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;

&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;sync&amp;quot;&lt;/span&gt;

&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;math/rand&amp;quot;&lt;/span&gt;

&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;time&amp;quot;&lt;/span&gt;



&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;var&lt;/span&gt; requests = &lt;span style=&#34;color: #008000&#34;&gt;make&lt;/span&gt;(&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;chan&lt;/span&gt; &lt;span style=&#34;color: #B00040&#34;&gt;int&lt;/span&gt;)

&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;var&lt;/span&gt; responses = &lt;span style=&#34;color: #008000&#34;&gt;make&lt;/span&gt;(&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;chan&lt;/span&gt; &lt;span style=&#34;color: #B00040&#34;&gt;string&lt;/span&gt;)



&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;func&lt;/span&gt; routine(num &lt;span style=&#34;color: #B00040&#34;&gt;int&lt;/span&gt;, wg &lt;span style=&#34;color: #666666&#34;&gt;*&lt;/span&gt;sync.WaitGroup) {

    &lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;// pretend this is a routine that&amp;#39;s doing something, like serving a user session&lt;/span&gt;

    &lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;// but then we need to modify some shared state&lt;/span&gt;

    time.Sleep(time.Duration(rand.Intn(&lt;span style=&#34;color: #666666&#34;&gt;100&lt;/span&gt;)) &lt;span style=&#34;color: #666666&#34;&gt;*&lt;/span&gt; time.Millisecond)

    requests &lt;span style=&#34;color: #666666&#34;&gt;&amp;lt;-&lt;/span&gt; num

    resp &lt;span style=&#34;color: #666666&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;&amp;lt;-&lt;/span&gt;responses

    fmt.Printf(&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;routine %d gets response: %s\n&amp;quot;&lt;/span&gt;, num, resp)

    wg.Done()

}



&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;func&lt;/span&gt; coordinator() {

    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;for&lt;/span&gt; {

        req &lt;span style=&#34;color: #666666&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;&amp;lt;-&lt;/span&gt;requests

        &lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;// in here, you can do whatever modifications to shared state you need.&lt;/span&gt;

        time.Sleep(time.Duration(rand.Intn(&lt;span style=&#34;color: #666666&#34;&gt;100&lt;/span&gt;)) &lt;span style=&#34;color: #666666&#34;&gt;*&lt;/span&gt; time.Millisecond) &lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;// simulate some heavy lifting&lt;/span&gt;

        responses &lt;span style=&#34;color: #666666&#34;&gt;&amp;lt;-&lt;/span&gt; fmt.Sprintf(&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;this return value is meant for routine %d&amp;quot;&lt;/span&gt;, req)

    }

}



&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;func&lt;/span&gt; main() {

    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;go&lt;/span&gt; coordinator()



    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;var&lt;/span&gt; wg sync.WaitGroup

    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color: #666666&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span style=&#34;color: #666666&#34;&gt;10&lt;/span&gt;; i&lt;span style=&#34;color: #666666&#34;&gt;++&lt;/span&gt; {

        wg.Add(&lt;span style=&#34;color: #666666&#34;&gt;1&lt;/span&gt;)

        &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;go&lt;/span&gt; routine(i, &lt;span style=&#34;color: #666666&#34;&gt;&amp;amp;&lt;/span&gt;wg)

    }

    wg.Wait()

    &lt;span style=&#34;color: #008000&#34;&gt;close&lt;/span&gt;(requests)

}

]]&amp;gt;
&lt;/pre&gt;&lt;/div&gt;




&lt;a href=&#34;http://play.golang.org/p/32BSXT0xhN&#34;&gt;code on Go playground&lt;/a&gt;



&lt;p&gt;

At first glance, it looked as if the seemingly arbitrary reading and writing from/to channels without explicit synchronization would introduce race conditions, with routines getting

the response meant for other routines.  But after some reasoning, it becomes apparent that 

the &#34;channel operation as synchronization&#34; keeps everything under control, in a pretty elegant way.

&lt;b&gt;There is nothing explicit to assure the routines get their response, and not the response meant for another routine.

Instead, it just flows naturally and implicitly from the ordering of the blocked channel operations.&lt;/b&gt;.

Another &#34;aha!&#34; moment for me.  I&#39;ve heard &#34;use channel operations for synchronization&#34; often enough, and this is the most

beautiful example of it I&#39;ve come across so far.  The routines are blocked on channel reads and writes, but when a channel operation occurs, that&#39;s where the respective goroutines unblock, and everything just works the way it&#39;s supposed to.  How elegant!

&lt;/p&gt;



&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;

Maybe these patterns are obvious to you, maybe they are widely known patterns.

But I think as you evolve from go rookie to experienced developer (and often need to wrap your head around new concepts and approaches)

you will encounter some interesting patterns and also have your &#34;aha!&#34; moments, so I hope this will help someone.

&lt;/p&gt;



&lt;p&gt;

I&#39;ve been using the first pattern in a few places, I haven&#39;t used the second one yet, but I know some places where I can apply it and simplify some code.

Take for example this &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng/pull/7&#34;&gt;pull request to add a web UI to carbon-relay-ng&lt;/a&gt;, now the metrics-router, the admin telnet interface, &lt;b&gt;and&lt;/b&gt; the new http interface will all need access to the routes map.  I&#39;m looking forward to implement the second pattern, simplifying the code while making it more generic at the same time.

&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monitorama PDX &amp; my metrics 2.0 presentation</title>
      <link>http://localhost:1313/post/monitorama-pdx-metrics20/</link>
      <pubDate>Thu, 29 May 2014 10:39:32 -0400</pubDate>
      
      <guid>http://localhost:1313/post/monitorama-pdx-metrics20/</guid>
      <description>&lt;p&gt;

Earlier this month we had another iteration of the &lt;a href=&#34;http://monitorama.com/&#34;&gt;Monitorama&lt;/a&gt; conference, this time in Portland, Oregon.

&lt;/p&gt;

&lt;p&gt;

&lt;img src=&#34;http://localhost:1313/files/blog/monitorama-audience.jpg&#34; width=&#34;800&#34; /&gt;

&lt;br/&gt;(photo by &lt;a href=&#34;https://www.flickr.com/photos/78527903@N00/sets/72157644593947233/&#34;&gt;obfuscurity&lt;/a&gt;)

&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;

I think the conference was better than the first one in Boston, much more to learn.  Also this one was quite focused on telemetry (timeseries metrics processing), lots of talks on timeseries analytics, not so much about things like sensu or nagios.  

&lt;a href=&#34;https://vimeo.com/95064249&#34;&gt;Adrian Cockroft&#39;s keynote&lt;/a&gt; brought some interesting ideas to the table, like building a feedback loop into the telemetry to drive infrastructure changes (something we do at Vimeo, I briefly give an example in the intro of my talk) or shortening the time from fault to alert (which I&#39;m excited to start working on soon)

&lt;br/&gt;My other favorite was &lt;a href=&#34;https://vimeo.com/95227467&#34;&gt;Noah Kantrowitz&#39;s talk about applying audio DSP techniques to timeseries&lt;/a&gt;,

I always loved audio processing and production.  Combining these two interests hadn&#39;t occurred to me so now I&#39;m very excited about the applications.

&lt;br/&gt;The opposite, but just as interesting of an idea - conveying information about system state as an audio stream - came up in &lt;a href=&#34;http://puppetlabs.com/podcasts/podcast-insights-monitorama-conference&#34;&gt;puppetlab&#39;s monitorama recap&lt;/a&gt; and that seems to make a lot of sense as well. There&#39;s &lt;b&gt;a lot&lt;/b&gt; of information in a stream of sound, it is much denser than text, icons and perhaps even graph plots.  Listening to an audio stream that&#39;s crafted to represent various information might be a better way to get insights into your system.

&lt;/p&gt;

&lt;br/&gt;

&lt;i&gt;

&lt;p&gt;I&#39;m happy to see the idea reinforced that telemetry is a key part of modern monitoring.

For me personally, telemetry (the tech and the process) is &lt;b&gt;the most fascinating part of modern technical operations&lt;/b&gt;, and I&#39;m glad to be part of the movement pushing this forward.  There&#39;s also a bunch of startups in the space (many stealthy ones), validating the market.  I&#39;m curious to see how this will play out.

&lt;/p&gt;

&lt;/i&gt;&lt;br/&gt;



&lt;p&gt;I had the privilege to present &lt;a href=&#34;http://metrics20.org&#34;&gt;metrics 2.0&lt;/a&gt; and

&lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer&lt;/a&gt;.

As usual the &lt;a href=&#34;http://www.slideshare.net/Dieterbe/metrics20-34319840&#34;&gt;slides are on slideshare&lt;/a&gt; and the &lt;a href=&#34;https://vimeo.com/95076197&#34;&gt;footage on the better video sharing platform&lt;/a&gt; ;-) .

&lt;br/&gt;I&#39;m happy with all the positive feedback, although I&#39;m not aware yet of other tools and applications adopting metrics 2.0, and I&#39;m looking forward to see some more of that, because ultimately that&#39;s what will show if my ideas are any good.

&lt;/p&gt;

&lt;p&gt;

&lt;iframe src=&#34;http://www.slideshare.net/slideshow/embed_code/34319840&#34; width=&#34;427&#34; height=&#34;356&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; allowfullscreen&gt; &lt;/iframe&gt;



&lt;iframe allowfullscreen=&#34;allowfullscreen&#34; frameborder=&#34;0&#34; height=&#34;356&#34; mozallowfullscreen=&#34;mozallowfullscreen&#34; src=&#34;//player.vimeo.com/video/95076197?title=0&amp;amp;byline=0&amp;amp;portrait=0&amp;amp;color=33a352&#34; webkitallowfullscreen=&#34;webkitallowfullscreen&#34; width=&#34;633&#34;&gt;&lt;/iframe&gt;

&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On Graphite, Whisper and InfluxDB</title>
      <link>http://localhost:1313/post/on-graphite-whisper-and-influxdb/</link>
      <pubDate>Sun, 18 May 2014 13:22:32 -0400</pubDate>
      
      <guid>http://localhost:1313/post/on-graphite-whisper-and-influxdb/</guid>
      <description>&lt;h4&gt;Graphite, and the storage Achilles heel&lt;/h4&gt;



Graphite is a neat timeseries metrics storage system that comes with a powerful querying api, mainly due to the whole bunch of &lt;a href=&#34;http://graphite.readthedocs.org/en/latest/functions.html&#34;&gt;available processing functions&lt;/a&gt;.

&lt;br/&gt;For medium to large setups, the storage aspect quickly becomes a pain point.  Whisper, the default graphite storage format, is a simple storage format, using one file per metric (timeseries).

&lt;!--more--&gt;

&lt;ul&gt;

&lt;li&gt;It can&#39;t keep all file descriptors in memory so there&#39;s a lot of overhead in constantly opening, seeking, and closing files, especially since usually one write comes in for all metrics at the same time.

&lt;/li&gt;

&lt;li&gt;Using the rollups feature (different data resolutions based on age) causes a lot of extra IO.&lt;/li&gt;

&lt;li&gt;The format is also simply not optimized for writes.  Carbon, the storage agent that sits in front of whisper has a feature to batch up writes to files to make them more sequential but this doesn&#39;t seem to help much.&lt;/li&gt;

&lt;li&gt;Worse, due to various &lt;a href=&#34;https://github.com/pcn/carbon/blob/new-sending-mechanism/Why_Spooling.md#what-problems-have-we-had&#34;&gt;implementation details&lt;/a&gt; the carbon agent is surprisingly inefficient and even cpu-bound.  People often run into cpu limitations before they hit the io bottleneck.  Once the writeback queue hits a certain size, carbon will blow up.&lt;/li&gt;

&lt;/ul&gt;

Common recommendations are to &lt;a href=&#34;http://bitprophet.org/blog/2013/03/07/graphite/&#34;&gt;run multiple carbon agents&lt;/a&gt; and

&lt;a href=&#34;http://obfuscurity.com/2012/04/Unhelpful-Graphite-Tip-5&#34;&gt;running graphite on SSD drives&lt;/a&gt;.

&lt;br/&gt;If you want to scale out across multiple systems, you can get carbon to shard metrics across multiple nodes, but the complexity can get out of hand and manually maintaining a cluster where nodes get added, fail, get phased out, need recovery, etc involves a lot of manual labor even though &lt;a href=&#34;https://github.com/jssjr/carbonate/&#34;&gt;carbonate&lt;/a&gt; makes this easier.  This is a path I simply don&#39;t want to go down.

&lt;br/&gt;

&lt;br/&gt;

&lt;p&gt;

&lt;i&gt;These might be reasonable solutions based on the circumstances (often based on short-term local gains), but I believe as a community we should solve the problem at its root, so that everyone can reap the long term benefits.

&lt;/i&gt;

&lt;/p&gt;

&lt;br/&gt;



In particular, &lt;a href=&#34;http://blog.sweetiq.com/2013/01/using-ceres-as-the-back-end-database-to-graphite/#axzz324uQtk3d&#34;&gt;running Ceres instead of whisper&lt;/a&gt;, is only a slight improvement, that suffers from most of the same problems.  I don&#39;t see any good reason to keep working on Ceres, other than perhaps that it&#39;s a fun exercise.   This probably explains the slow pace of development.

&lt;br/&gt;However, many mistakenly believe Ceres is &#34;the future&#34;.

&lt;br/&gt;&lt;a href=&#34;http://www.inmobi.com/blog/2014/01/24/extending-graphites-mileage&#34;&gt;Switching to LevelDB&lt;/a&gt; seems much more sensible but IMHO still doesn&#39;t cut it as a general purpose, scalable solution.



&lt;h4&gt;The ideal backend&lt;/h4&gt;

I believe we can build a backend for graphite that

&lt;ul&gt;

&lt;li&gt;can easily scale from a few metrics on my laptop in power-save mode to millions of metrics on a highly loaded cluster&lt;/li&gt;

&lt;li&gt;supports nodes joining and leaving at runtime and automatically balancing the load across them&lt;/li&gt;

&lt;li&gt;assures high availability and heals itself in case of disk or node failures&lt;/li&gt;

&lt;li&gt;is simple to deploy.  think: just run an executable that knows which directories it can use for storage, elasticsearch-style automatic clustering, etc.&lt;/li&gt;

&lt;li&gt;has the right read/write optimizations.  I&#39;ve never seen a graphite system that is not write-focused, so something like &lt;a href=&#34;http://en.wikipedia.org/wiki/Log-structured_merge-tree&#34;&gt;LSM trees&lt;/a&gt; seems to make a lot of sense.&lt;/li&gt;

&lt;li&gt;can leverage cpu resources (e.g. for compression)&lt;/li&gt;

&lt;li&gt;provides a more natural model for phasing out data.  Optional, runtime-changeable rollups.  And an age limit (possibly, but not necessarily round robin)

&lt;/ul&gt;



While we&#39;re at it. pub-sub for realtime analytics would be nice too.  Especially when it allows to use the same functions as the query api.

&lt;br/&gt;And getting rid of the metric name restrictions such as inability to use dots or slashes.  Efficient sparse series support would be nice too.



&lt;h4&gt;InfluxDB&lt;/h4&gt;



There&#39;s a lot of databases that you could hook up to graphite.

riak, hdfs based (opentsdb), Cassandra based (kairosdb, blueflood, cyanite), etc.



Some of these are solid and production ready, and would make sense depending on what you already have and have experience with.

I&#39;m personally very interested in playing with Riak, but decided to choose InfluxDB as my first victim.

&lt;br/&gt;

&lt;br/&gt;

InfluxDB is a young project that will need time to build maturity, but is on track to meet all my goals very well.

In particular, installing it is a breeze (no dependencies), it&#39;s specifically built for timeseries (not based on a general purpose database),

which allows them to do a bunch of simplifications and optimizations, is write-optimized, and should meet my goals for scalability, performance, and availability well.

And they&#39;re in NYC so meeting up for lunch has proven to be pretty fruitful for both parties.  I&#39;m pretty confident that these guys can pull off something big.

&lt;br/&gt;

&lt;br/&gt;

Technically, InfluxDB is a &#34;timeseries, metrics, and analytics&#34; databases with use cases well beyond graphite and even technical operations.

Like the alternative databases, graphite-like behaviors such as rollups management and automatically picking the series in the most appropriate resolutions, is something to be implemented on top of it.  Although you never know, it might end up being natively supported.





&lt;h4&gt;Graphite + InfluxDB&lt;/h4&gt;



InfluxDB developers plan to implement a whole bunch of processing functions (akin to graphite, except they can do locality optimizations) and add a dashboard that talks to InfluxDB natively (or use &lt;a href=&#34;http://grafana.org/&#34;&gt;Grafana&lt;/a&gt;), which means at some point you could completely swap graphite for InfluxDB.



However, I think for quite a while, the ability to use the Graphite api, combine backends, and use various graphite dashboards is still very useful.

So here&#39;s how my setup currently works:



&lt;ul&gt;

&lt;li&gt;

&lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt; is a carbon relay in Go.  

It&#39;s a pretty nifty program to partition and manage carbon metrics streams.  I use it in front of our traditional graphite system, and have it stream - in realtime - a copy of a subset of our metrics into InfluxDB.  This way I basically have our unaltered Graphite system, and in parallel to it, InfluxDB, containing a subset of the same data.

&lt;br/&gt;With a bit more work it will be a high performance alternative to the python carbon relay, allowing you to manage your streams on the fly.

It doesn&#39;t support consistent hashing, because CH should be part of a strategy of a highly available storage system (see requirements above), using CH in the relay still results in a poor storage system, so there&#39;s no need for it.

&lt;/li&gt;

&lt;li&gt;I contributed the code to InfluxDB to make it listen on the carbon protocol.  So basically, for the purpose of ingestion, InfluxDB can look and act just like a graphite server.  Anything that can write to graphite, can now write to InfluxDB.  (assuming the plain-text protocol, it doesn&#39;t support the pickle protocol, which I think is a thing to avoid anyway because almost nothing supports it and you can&#39;t debug what&#39;s going on)&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://github.com/brutasse/graphite-api&#34;&gt;graphite-api&lt;/a&gt; is a fork/clone of graphite-web, stripped of needless dependencies, stripped of the composer.  It&#39;s conceived for many of the same reasons behind &lt;a href=&#34;http://dieter.plaetinck.be/graphite-ng_a-next-gen-graphite-server-in-go.html&#34;&gt;graphite-ng&lt;/a&gt; (graphite technical debt, slow development pace, etc) though it doesn&#39;t go to such extreme lengths and for now focuses on being a robust alternative for the graphite server, api-compatible, trivial to install and with a faster pace of development.

&lt;/li&gt;

&lt;li&gt;

That&#39;s where &lt;a href=&#34;https://github.com/vimeo/graphite-influxdb&#34;&gt;graphite-influxdb&lt;/a&gt; comes in.  It hooks InfluxDB into graphite-api, so that you can query the graphite api, but using data in InfluxDB.

It should also work with the regular graphite, though I&#39;ve never tried.  (I have no incentive to bother with that, because I don&#39;t use the composer.  And I think it makes more sense to move the composer into a separate project anyway).

&lt;/li&gt;

&lt;/ul&gt;



With all these parts in place, I can run our dashboards next to each other - one running on graphite with whisper, one on graphite-api with InfluxDB - and simply look whether the returned data matches up, and which dashboards loads graphs faster.

Later i might do more extensive benchmarking and acceptance testing.

&lt;br/&gt;

&lt;br/&gt;

If all goes well, I can make carbon-relay-ng fully mirror all data, make graphite-api/InfluxDB the primary, and turn our old graphite box into a live &#34;backup&#34;.

We&#39;ll need to come up with something for rollups and deletions of old data (although it looks like by itself influx is already more storage efficient than whisper too), and I&#39;m really looking forward to the InfluxDB team building out the function api, having the same function api available for historical querying as well as realtime pub-sub.  (my goal used to be implementing this in graphite-ng and/or carbon-relay-ng, but if they do this well, I might just abandon graphite-ng)



&lt;br/&gt;

&lt;br/&gt;To be continued..


</description>
    </item>
    
    <item>
      <title>Metrics 2.0 now has its own website!</title>
      <link>http://localhost:1313/post/metrics-2-0-own-website/</link>
      <pubDate>Wed, 23 Apr 2014 09:10:32 -0400</pubDate>
      
      <guid>http://localhost:1313/post/metrics-2-0-own-website/</guid>
      <description>Metrics 2.0 started as a &lt;a href=&#34;http://dieter.plaetinck.be/metrics_2_a_proposal.html&#34;&gt;half-formal proposal&lt;/a&gt; and an implementation via &lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;graph-explorer&lt;/a&gt;, but is broad enough in scope that it deserves its own website, its own spec, its own community.  That&#39;s why I launched &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics20.org&lt;/a&gt; and a &lt;a href=&#34;https://groups.google.com/forum/#!forum/metrics20&#34;&gt;discussion group&lt;/a&gt;.

&lt;!--more--&gt;



&lt;p&gt;

from the website:

&lt;/p&gt;

      &lt;p&gt;

      We have pretty good storage of timeseries data, collection agents, and dashboards.  But the idea of giving timeseries a &#34;name&#34; or a

      &#34;key&#34; is profoundly limiting us.  Especially when they&#39;re not standardized and missing information.

      &lt;br/&gt;Metrics 2.0 aims for &lt;b&gt;self-describing&lt;/b&gt;, &lt;b&gt;standardized&lt;/b&gt; metrics using &lt;b&gt;orthogonal tags&lt;/b&gt; for every dimension.

      &#34;metrics&#34; being the pieces of information that point to, and describe timeseries of data.

      &lt;/p&gt;By adopting metrics 2.0 you can:

      &lt;ul&gt;

        &lt;li&gt;increase compatibility between tools&lt;/li&gt;

        &lt;li&gt;get immediate understanding of metrics&lt;/li&gt;

        &lt;li&gt;build graphs, plots, dashboards and alerting expressions with minimal hassle&lt;/li&gt;

      &lt;/ul&gt;

Read more on &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics20.org&lt;/a&gt;.
</description>
    </item>
    
    <item>
      <title>Introduction talk to metrics 2.0 and Graph-Explorer</title>
      <link>http://localhost:1313/post/introduction_talk_to_metrics2-0_and_graph_explorer/</link>
      <pubDate>Sun, 23 Feb 2014 16:20:32 -0400</pubDate>
      
      <guid>http://localhost:1313/post/introduction_talk_to_metrics2-0_and_graph_explorer/</guid>
      <description>This week I had the opportunity to present &lt;a href=&#34;http://dieter.plaetinck.be/metrics_2_a_proposal.html&#34;&gt;metrics 2.0&lt;/a&gt; and

&lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer&lt;/a&gt; at the

&lt;a href=&#34;http://www.meetup.com/Full-Stack-Engineering-Meetup/&#34;&gt;Full-stack engineering meetup&lt;/a&gt;.

&lt;!--more--&gt;

&lt;br/&gt;

I could easily talk for hours about this stuff but the talk had to be about 20 minutes so I paraphrased it to be only

about the basic concepts and ideas and some practical use cases and features.  I think it serves as a pretty good introduction

and a showcase of the most commonly used features (graph composition, aggregations and unit conversion), and some new stuff such as alerting and dashboards.



&lt;p&gt;

The talk also briefly covers native metrics 2.0 through your metrics pipeline using &lt;a href=&#34;https://github.com/vimeo/statsdaemon&#34;&gt;statsdaemon&lt;/a&gt; and &lt;a href=&#34;https://github.com/vimeo/carbon-tagger&#34;&gt;carbon-tagger&lt;/a&gt;.  I&#39;m psyched that by formatting metrics at the source a little better and having an aggregation daemon that expresses the performed operations by updating the metric tags, all the foundations are in place for some truly next-gen UI&#39;s and applications (one of them already being implemented: graph-explorer can pretty much generate all graphs I need by just phrasing an information need as a proper query)

&lt;/p&gt;

&lt;p&gt;

The &lt;a href=&#34;http://vimeo.com/87194301&#34;&gt;video&lt;/a&gt; and &lt;a href=&#34;https://www.slideshare.net/Dieterbe/metrics2-0graphexplorer20140218&#34; title=&#34;Metrics 2.0 &amp;amp; Graph-Explorer&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt; are available and also embedded below.

&lt;/p&gt;



&lt;iframe src=&#34;//player.vimeo.com/video/87194301?title=0&amp;amp;byline=0&amp;amp;portrait=0&amp;amp;color=33a352&#34;

width=&#34;500&#34; height=&#34;281&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;

&lt;iframe src=&#34;http://www.slideshare.net/slideshow/embed_code/31440297&#34; width=&#34;427&#34; height=&#34;356&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;



&lt;p&gt;

I would love to do another talk that allows me to dive into more of the underlying ideas, the benefits of metrics2.0 for things like metric storage systems, graph renderers, anomaly detection, dashboards, etc.

&lt;br/&gt;

&lt;br/&gt;

Hope you like it!

&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>