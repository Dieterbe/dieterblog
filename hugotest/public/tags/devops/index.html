<!DOCTYPE html>

<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width">
	<title>Dieter Plaetinck&#39;s blog</title>
	<link rel="profile" href="http://gmpg.org/xfn/11">
	<!--[if lt IE 9]>
	<script src="http://localhost:1313//js/html5.js"></script>
	<![endif]-->
    
    <link href="http://localhost:1313//index.xml" rel="alternate" type="application/rss+xml" title="Dieter Plaetinck&#39;s blog" />

    <link rel='stylesheet' id='twentyfourteen-lato-css'  href='//fonts.googleapis.com/css?family=Lato%3A300%2C400%2C700%2C900%2C300italic%2C400italic%2C700italic&#038;subset=latin%2Clatin-ext' type='text/css' media='all' />

    <link rel='stylesheet' id='genericons-css' href='http://localhost:1313//genericons/genericons.css' type='text/css' media='all' />
	<link rel='stylesheet' id='twentyfourteen-style-css' href='http://localhost:1313//css/style.css' type='text/css' media='all' />
	
	<script type='text/javascript' src='http://localhost:1313//js/jquery/jquery.js'></script>
	<script type='text/javascript' src='http://localhost:1313//js/jquery/jquery-migrate.min.js'></script>
	<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>
</head>

<body class="home blog masthead-fixed list-view full-width grid">
<div id="page" class="hfeed site">
	<header id="masthead" class="site-header" role="banner">
		<div class="header-main">
			<h1 class="site-title"><a href="http://localhost:1313//index.html" rel="home">Dieter Plaetinck&#39;s blog</a></h1>

			<div class="search-toggle">
				<a href="#search-container" class="screen-reader-text">Search</a>
			</div>

			<nav id="primary-navigation" class="site-navigation primary-navigation" role="navigation">
				<button class="menu-toggle">Primary Menu</button>
				<a class="screen-reader-text skip-link" href="#content">Skip to content</a>
				<div class="nav-menu">
					<ul>
						
						<li class="page_item"> 
							<a href="/">blog</a>
						</li>
						
						<li class="page_item"> 
							<a href="/about/">about</a>
						</li>
						
						<li class="page_item"> 
							<a href="/talks/">talks</a>
						</li>
						
						<li class="page_item"> 
							<a href="https://twitter.com/Dieter_be">tweets</a>
						</li>
						
						<li class="page_item"> 
							<a href="https://github.com/Dieterbe">Github</a>
						</li>
						
					</ul>
				</div>
			</nav>
		</div>

		<div id="search-container" class="search-box-wrapper hide">
			<div class="search-box">
                <script type="text/javascript">
    function site_search(obj) {
    	var host = window.location.host;
        obj.q.value = "site:" + host + " " + obj.ss_q.value;
    }
</script>

<aside id="search-3" class="widget widget_search">
	<form role="search" class="search-form" action="//www.google.com/search" method="get" onSubmit="site_search(this)">

	<input name="q" type="hidden" />
	    <label>
	        <span class="screen-reader-text">Search for:</span>
	        <input name="ss_q" type="text" placeholder="Search ..." class="search-field" />
	    </label>
	    <input type="submit" value="Search" class="search-submit" />
	</form>
</aside>
			</div>
		</div>
	</header>

	<div id="main" class="site-main">


<div id="main-content" class="main-content">

	<div id="primary" class="content-area">
		<div id="content" class="site-content" role="main">

			<header class="archive-header">
				<h1 class="archive-title">Content tagged "Devops"</h1>
			</header>

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/practical-fault-detection-on-timeseries-part-2/">Practical fault detection on timeseries part 2: first macros and templates</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/practical-fault-detection-on-timeseries-part-2/" rel="bookmark">
					<time class="entry-date" datetime="2015-04-27 09:05:02 -0400 EDT">
						April 27, 2015
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>It had an example of a simple but effective approach to find sudden spikes (peaks and drops) within fluctuating time series.</p>

<p>This post explains the continuation of that work and provides you the means to implement this yourself with minimal effort.</p>

<p>I&rsquo;m sharing with you:</p>

<ul>

<li><a href="http://bosun.org">Bosun</a> macros which detect our most common not-trivially-detectable symptoms of problems</li>

<li>Bosun notification template which provides a decent amount of information</li>

<li><a href="http://www.grafana.org">Grafana</a> and <a href="http://vimeo.github.io/graph-explorer/">Graph-Explorer</a> dashboards and integration for further troubleshooting</li>

</ul>

<p>We reuse this stuff for a variety of cases where the data behaves similarly and I suspect that you will be able to apply this to a bunch of your monitoring targets as well.</p>

<h2>Target use case</h2>

<p>As in the previous article, we focus on the specific category of timeseries metrics driven by user activity.</p>

<p>Those series are expected to fluctuate in at least some kind of (usually daily) pattern, but is expected to have a certain smoothness to it. Think web requests per second or uploads per minute.   There are a few characteristics that are considered faulty or at least worth our attention:</p>

<table>

<tr>

  <td><img src="/files/practical-alerting-timeseries-good.png"/></td>

  <td><img src="/files/practical-alerting-timeseries-bad-spikes.png"/></td>

  <td><img src="/files/practical-alerting-timeseries-bad-erratic.png"/></td>

  <td><img src="/files/practical-alerting-timeseries-bad-timeseries-median-drop.png"/></td>

</tr>

<tr>

  <td><b style="color: green;">looks good</b><br/>consistent pattern<br/>consistent smoothness</td>

  <td><b style="color: red;">sudden deviation (spike)</b><br/>Almost always something broke or choked.<br/>could also be pointing up. ~ peaks and valleys</td>

  <td><b style="color: red;">increased erraticness</b><br/>Sometimes natural<br/>often result of performance issues</td>

  <td><b style="color: red;">lower values than usual</b> (in the third cycle)<br/>Often caused by changes in code or config, sometimes innocent.  But best to alert operator in any case [*]</td>

</tr>

</table>

<p><br/></p>

<p>[*] Note that some regular patterns can look like this as well. For example weekend traffic lower than weekdays, etc.  We see this a lot.</p>

<p><br/>The illustrations don&rsquo;t portray this for simplicity.   But the alerting logic below supports this just fine by comparing to same day last week instead of yesterday, etc.</p>

<h2>Introducing the new approach</h2>

<p>The <a href="/practical-fault-detection-alerting-dont-need-to-be-data-scientist.html">previous article</a> demonstrated using graphite to compute standard deviation.</p>

<p>This let us alert on the erraticness of the series in general and as a particularly interesting side-effect, on spikes up and down.</p>

<p>The new approach is more refined and concrete by leveraging some of bosun&rsquo;s and Grafana&rsquo;s strengths.  We can&rsquo;t always detect the last case above via erraticness checking (a lower amount may be introduced gradually, not via a sudden drop) so now we monitor for that as well, covering all cases above.</p>

<p>We use</p>

<ul>

<li>Bosun macros which encapsulate all the querying and processing</li>

<li>Bosun template for notifications</li>

<li>A generic Grafana dashboard which aids in troubleshooting</li>

</ul>

<p>We can then leverage this for various use cases, as long as the expectations of the data are as outlined above.</p>

<p>We use this for web traffic, volume of log messages, uploads, telemetry traffic, etc.</p>

<p>For each case we simply define the graphite queries and some parameters and leverage the existing mentioned Bosun and Grafana configuration.</p>

<p><br/></p>

<p><p></p>

<p>The best way to introduce this is probably by showing how a notification looks like:</p>

<p><br/></p>

<p><center></p>

<p><a href="/files/practical-alerting-dm-notification.png"><img height="600px" src="/files/practical-alerting-dm-notification.png"/></a></p>

<p><br/></p>

<p>(image redacted to hide confidential information</p>

<p><br/>the numbers are not accurate and for demonstration purposes only)</p>

<p></center></p>

<p><p></p>

<p>As you can tell by the sections, we look at some global data (for example &ldquo;all web traffic&rdquo;, &ldquo;all log messages&rdquo;, etc), and also</p>

<p>by data segregated by a particular dimension (for example web traffic by country, log messages by key, etc)</p>

<p><br/></p>

<p>To cover all problematic cases outlined above, we do 3 different checks:</p>

<p>(note, everything is parametrized so you can tune it, see further down)</p>

<ul>

<li>Global volume: comparing the median value of the last 60 minutes or so against the corresponding 60 minutes last week and expressing it as a "strength ratio".  Anything below a given threshold such as 0.8 is alerted on</li>

<li>Global erraticness. To find all forms of erraticness (increased deviation), we use a refined formula.  See details below.  A graph of the input data is included so you can visually verify the series</li>

<li>On the segregated data: compare current (hour or so) median against median derived from the corresponding hours during the past few weeks, and only allow a certain amount of standard deviations difference</li>

</ul>

<p>If any, or multiple of these conditions are in warning or critical state, we get 1 alert that gives us all the information we need.</p>

<p><br/></p>

<p>Note the various links to GE (Graph-Explorer) and Grafana for timeshifts.</p>

<p>The Graph-Explorer links are just standard GEQL queries, I usually use this if i want to be easily manage what I&rsquo;m viewing (compare against other countries, adjust time interval, etc) because that&rsquo;s what GE is really good at.</p>

<p>The timeshift view is a Grafana dashboard that takes in a Graphite expression as a template variable, and can hence be set via a GET parameter by using the url  <pre><a href="http://grafana/#/dashboard/db/templatetimeshift?var-patt=expression">http://grafana/#/dashboard/db/templatetimeshift?var-patt=expression</a></pre></p>

<p>It shows the current past week as red dots, and the past weeks before that as timeshifts in various shades of blue representing the age of the data. (darker is older).</p>

<p><br/></p>

<p><br/></p>

<p><a href="/files/practical-alerting-screenshot-template-timeshift.png"><img width="50%" src="/files/practical-alerting-screenshot-template-timeshift.png" /></a></p>

<p><br/></p>

<p>This allows us to easily spot when traffic becomes too low, overly erratic, etc as this example shows:</p>

<p><br/></p>

<p><br/></p>

<p><a href="/files/practical-alerting-timeshift-use.png"><img width="50%" src="/files/practical-alerting-timeshift-use.png" /></a></p>

<p><br/></p>

<h2>Getting started</h2>

<p>Note: I Won&rsquo;t explain the details of the bosun configuration.  Familiarity with bosun is assumed.  The <a href="http://bosun.org/">bosun documentation</a> is pretty complete.</p>

<p><br/></p>

<p><br/></p>

<p><a href="https://gist.github.com/Dieterbe/d1892fa0b4454b892216">Gist with bosun macro, template, example use, and Grafana dashboard definition</a>.  Load the bosun stuff in your bosun.conf and import the dashboard in Grafana.</p>

<p><br/></p>

<p><br/></p>

<p>The pieces fit together like so:</p>

<ul>

<li>The alert is where we define the graphite queries, the name of the dimension segregated by (used in template), how long the periods are, what the various thresholds are and the expressions to be fed into Grafana and Graph-Explorer.

<br/>

It also lets you set an importance which controls the sorting of the segregated entries in the notification (see screenshot).  By default it is based on the historical median of the values but you could override this.  For example for a particular alert we maintain a lookup table with custom importance values.</li>

<li>The macros are split in two:

<ol>

<li>dm-load loads all the initial data based on your queries and computes a bunch of the numbers.</li>

<li>dm-logic does some final computations and evaluates the warning and critical state expressions.</li>

</ol>

They are split so that your alerting rule can leverage the returned tags from the queries in dm-load to use a lookup table to set the importance variable or other thresholds, such as s_min_med_diff on a case-by-case basis, before calling dm-logic.

<br/>

We warn if one or more segregated items didn't meet their median requirements, and if erraticness exceeds its threshold (note that the latter can be disabled).

<br>Critical is when more than the specified number of segregated items didn't meet their median requirements, the global volume didn't meet the strength ratio, or if erraticness is enabled and above the critical threshold.

</li>

<li>The template is evaluated and generates the notification like shown above</li>

<li>Links to Grafana (timeshift) and GE are generated in the notification to make it easy to start troubleshooting</li>

</ul>

<h2>Erraticness formula refinements</h2>

<p><img style="float: right; margin: 45px;" width="30%" src="/files/practical-alerting-notes-cleaned-small.jpg" /></p>

<p>You may notice that the formula has changed to</p>

<pre>

(deviation-now * median-historical) / ( (deviation-historical * median-now) + 0.01)

</pre>

<ul>

<li>Current deviation is compared to an automatically chosen historical deviation value (so no more need to manually set this)</li>

<li>Accounts for difference in volume: for example if traffic at any point is much higher, we can also expect the deviation to be higher.  With the previous formula we would have cases where in the past the numbers were very low, and naturally the deviation then was low and not a reasonable standard to be held against when traffic is higher, resulting in trigger happy alerting with false positives.

<br/>Now we give a fair weight to the deviation ratio by making it inversely proportional to the median ratio</li>

<li>The + 0.01 is to avoid division by zero</li>

</ul>

<!--

streak covers cases where values are very low so that stdev is in the same order (like low volume logs) and we can't properly use the erraticness or x-deviations. for example logs with very little traffic, datapoints representing healthy traffic can look like (1, 2, 0, 3, 1, ..)

although i think the ratio of medians (median_now/median_then) should work just as well as streak



<img src="/files/practical-alerting-low-values-zeroes.png" />

<img src="/files/practical-alerting-low-values-low.png" />

-->

<h2>Still far from perfect</h2>

<p>While this has been very helpful to us, I want to highlight a few things that could be improved.</p>

<ul>

<li>With these alerts, you'll find yourself wanting to iteratively fine tune the various parameters and validate the result of your changes by comparing the status-over-time timeline before and after the change.  While Bosun already makes iterative development easier and lets you <a href="http://bosun.org/public/ss_rule_timeline.png">run test rules against old data and look at a the status over time</a>, the interface could be improved by

<ol>

<li><a href="https://github.com/bosun-monitor/bosun/issues/636">showing timeseries (with event markers where relevant) alongside the status visualization</a>, so you have context to interpret the status timeline</li>

<li>routinely building <a href="https://github.com/grafana/grafana/pull/1569">a knowledge base of time ranges annotated with a given state for a given alerting concern, which would help in validating the generated status timeline, both visually and in code.  We could compute percentage of issues found, missed, etc</a>. "unit tests for alerting" my boss called it.</li>

</ol>

</li>

<li>Template could be prettier.  In particular the plots often don't render very well.  We're looking into closer Grafana-Bosun integration so I think that will be resolved at some point.</li>

<li><a href="https://github.com/bosun-monitor/bosun/issues/719">Current logic doesn't take past outages into account. "just taking enough periods in graphiteBand()" helps alleviate it mostly, but it's not very robust</a></li>

<li>See that drop in the screenshot a bit higher up? That one was preceded by a code deploy event in anthracite which made some changes where a drop in traffic was actually expected.  Would love to be able to mark stuff like this in deploys (like putting in the commit message something like "expect 20-50 drop" and have the monitoring system leverage that.</li>

</ul>

<h2>In conclusion</h2>

<p>I know many people are struggling with poor alerting rules (static thresholds?)</p>

<p><br/>As I explained in the previous article I fondly believe that the commonly cited solutions (anomaly detection via machine learning) are a very difficult endeavor and results can be achieved much quicker and more simpler.</p>

<p><br/>While this only focuses on one class of timeseries (it won&rsquo;t work on diskspace metrics for example) I found this class to be in the most dire need of better fault detection.  Hopefully this is useful to you. Good luck and let me know how it goes!</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/joining-raintank/">Focusing on open source monitoring.  Joining raintank.</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/joining-raintank/" rel="bookmark">
					<time class="entry-date" datetime="2015-04-26 09:08:02 -0400 EDT">
						April 26, 2015
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>

It's never been as hard saying goodbye to the people and the work environment as it is now.

<br/>

Vimeo was created by dedicated film creators and enthousiasts, just over 10 years ago, and today it still shows.

From the quirky, playful office culture, <a href="https://vimeo.com/staff">the staff created shortfilms</a>,

to the <a href="https://vimeo.com/categories">tremendous curation effort</a> and <a href="https://vimeo.com/channels/staffpicks/videos">staff picks</a> including <a href="http://websta.me/p/797128421285501516_12986477">monthly staff screenings</a> where we get to see the best of the best videos on the internet each month,

to the dedication towards building the best platform and community on the web to enjoy videos and the uncomprimising commitment to supporting movie creators and working in their best interest.

<br/>Engineering wise, there has been plenty of opportunity to make an impact and learn.

<br/>Nonetheless, I have to leave and I'll explain why.  First I want to mention a few more things.

</p>

<p>

In Belgium I used to hitchhike to/from work so that each day brought me opportunities to have a chat with a diverse, fantastic assortment of people.  I still fondly remember some of those memories. (and it was also usually faster than taking the bus!)

<br/>Here in NYC this isn't really feasible, so I tried the next best thing.  A mission to have lunch with every single person in the company, starting with those I don't typically interact with.  I didn't quite make it that far, but I still managed to have lunch with 95 people, get to know them a bit, find some gems of personalities and anectdotes, and have conversations on a tremendous variety of subjects.  It was fun and I hope to be able to keep doing such social experiments in my new environment.



</p>

<p>

Vimeo is also part of my life in an unusually personal way.  When I came to New York (my first ever visit to the US) in 2011 to interview, I also met a pretty fantastic woman in a random bar in Williamsburg. We ended up traveling together in Europe, I decided to move the US and we moved in together.  I've had the pleasure of being submerged in both American and Greek culture for the last few years, but the best part is that today we are engaged and I feel like the luckiest guy in the world.  While I try to keep work and personal life separate, Vimeo has made an undeniable ever lasting impact on my life that I'm very grateful for.

</p>

<p>

At Vimeo I found an area where a bunch of my interests converge: operational best practices, high performance systems, number crunching, statistics and open source software.  Specifically, timeseries metrics processing in the context of monitoring.  While I have enjoyed my opportunity <a href="/tag/monitoring/">to make contributions in this space</a> to help our teams and other companies who end up using my tools, I want to move out of the cost center of the company, I want to be in the department that creates the value.  If I want to focus on open source monitoring, I should align my incentives with those of my employer.  Both for my and their sake.

The time has come for me to join a company that is all about making open source monitoring better.

</p>

<h2>Hello Raintank!</h2>

<p>Over the past two years or so I&rsquo;ve talked to many people in the industry about monitoring, many of them trying to bring me into their team.</p>

<p>I would always find good reasons not to switch jobs but as we transitioned from 2014 into 2015, the starts seemingly aligned for me.</p>

<p>Here&rsquo;s why I&rsquo;m very excited to join the <a href="http://www.raintank.io/">Raintank</a> crew:</p>

<ul>

<li>I'm a strong believer in open source.  I believe fundamental infrastructure tooling should be modifyable.  It's partially a philosophical argument, but also what I believe will separate long lasting business from short term successes. 

<br/>SaaS is great, but not if it locks you in via technical debt.  In this day and age, I think you should lock in your customers by providing a great experience, not by having them build technical debt as they integrate into your stack.

<br/>That said, integrating with open source also incurs technical debt, and some closed source service providers are so good that the debt is worth it.  But I don't believe this lasts long term, especially given the required pricing models.  I think you can entice customers more by lowering the debt they build up (i.e. use standardized protocols, tooling and making it easy for them to leave) as they adopt your service.  I was really interested in datadog for example.  I liked the team, I liked the product they are/were building and I saw they would become successfull, but it's not the kind of business I want to build towards long term.

<br/>Raintank is committed to open source and I think came up with a good business model that combines the benefits of SaaS with that of open source.  At least it's a bet I want to take.  Because it's open source, we can also make a bigger impact</li>

<li>Something I value a lot in coworkers, besides "technical" abilities is emotional intelligence.  Part of me thinks it's a vastly underestimated quality in general, although perhaps I just happen to find it more important than average.  Either way, the new team seems solid on both fronts.  I'm mostly impressed by the founder/CEO Raj Dutt who has shown a very personal and graceful approach.  Looking forward to work with him.

<li>I (and my fiancee) want to see the world. The common conception that significant travel can't be combined with hard, or even normal amounts of work seems so backwards to me.  Today more than ever, we have globalisation, we are developing a sharing economy (airbnb, lyft, ...).  There's no reason we can't do great work while enjoying our personal time to the fullest.  Working remotely, and/or with remote colleagues requires more discipline but I simply only want to work with disciplined people.  Working on a fixed schedule, in a fixed location seems needlessly arcane, constrained and inefficient.  I get a lot more done working remote and I'm happy that Raintank not only agrees with the idea, but actually encourages travel.  This year we'll spend some time with our families in Belgium and Cyprus and take the opportunity to travel around the US and live in different places, all I will make sure I don't compromise my, or the teams productivity</li>

<li>I've always wanted to be in a company from the start and experience the "true" start up feel.  I have a lot of opinions on organisation, culture, and product and so I'm glad to have the opportunity to make that kind of impact as well</li>



</ul>

<!--

nyc skin issues

alternative funding? crowd etc





https://vimeo.com/blog/post:702

grown as backend engineer, carbon-relay-ng most fun





my various projects

contributor to graphite, influxdb, bosun, diamond, statsd, graphite-api,

https://github.com/brutasse/graphite-api

https://github.com/Dieterbe/timeserieswidget

https://github.com/Dieterbe/profile-process

http://vimeo.github.io/graph-explorer/

https://github.com/vimeo/graphite-influxdb

https://github.com/vimeo/carbon-tagger

https://github.com/vimeo/statsdaemon

https://github.com/vimeo/graphite-api-influxdb-docker

https://github.com/vimeo/whisper-to-influxdb

https://github.com/vimeo/smoketcp

https://github.com/vimeo/timeserieswidget

https://github.com/vimeo/simple-black-box

https://groups.google.com/forum/#!forum/it-telemetry



https://github.com/python-diamond

https://github.com/bosun-monitor



https://github.com/Dieterbe/anthracite

https://github.com/Dieterbe/influx-cli



https://github.com/graphite-ng/graphite-ng

https://github.com/graphite-ng/carbon-relay-ng

http://metrics20.org/

humbled by big players who saw my work and invited me to work with them

basically 2 big camps.





it seems like every week a monitoring startup launches somewhere.  There is, and will be, more and more competition.  It's almost ludicrous to start or join a new one.

-->

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
                <a href="http://localhost:1313//tags/life/index.html" rel="tag">life</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/practical-fault-detection-alerting-dont-need-to-be-data-scientist/">Practical fault detection &amp; alerting.  You don&#39;t need to be a data scientist</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/practical-fault-detection-alerting-dont-need-to-be-data-scientist/" rel="bookmark">
					<time class="entry-date" datetime="2015-01-29 09:08:02 -0400 -0400">
						January 29, 2015
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>As we try to retain visibility into our increasingly complicated applications and infrastructure, we&rsquo;re building out more advanced monitoring systems.</p>

<p>Specifically, a lot of work is being done on alerting via fault and anomaly detection.</p>

<p>This post covers some common notions around these new approaches, debunks some of the myths that ask for over-complicated solutions, and provides some practical pointers that any programmer or sysadmin can implement that don&rsquo;t require becoming a data scientist.</p>

<p><br/></p>

<p><br/></p>

<h2>It's not all about math</h2>

<p>

I've seen smart people who are good programmers decide to tackle anomaly detection on their timeseries metrics.

(anomaly detection is about building algorithms which spot "unusual" values in data, via statistical frameworks).  This is a good reason to brush up on statistics, so you can apply some of those concepts.

But ironically, in doing so, they often seem to think that they are now only allowed to implement algebraic mathematical formulas. No more if/else, only standard deviations of numbers.  No more for loops, only moving averages. And so on.

<br/>When going from thresholds to something (<i>anything</i>) more advanced, suddenly people only want to work with mathematical formula's.  Meanwhile we have entire Turing-complete programming languages available, which allow us to execute any logic, as simple or as rich as we can imagine.  Using only math massively reduces our options in implementing an algorithm. 

<br/>

<br/>For example I've seen several presentations in which authors demonstrate how they try to fine-tune moving average algorithms and try to get a robust base signal to check against but which is also not affected too much by previous outliers, which raise the moving average and might mask subsequent spikes).  

<br/>

<img src="http://dieter.plaetinck.be/files/fault-detection-moving-average.png">

from <a href="https://speakerdeck.com/astanway/a-deep-dive-into-monitoring-with-skyline">A Deep Dive into Monitoring with Skyline</a>

<br/>

<br/>

But you can't optimize both, because a mathematical formula at any given point can't make the distinction between past data that represents "good times" versus "faulty times".

<br/>However: we wrap the output of any such algorithm with some code that decides what is a fault (or "anomaly" as labeled here) and alerts against it, so why would we hold ourselves back in feeding this useful information back into the algorithm?

<br/>I.e. <b>assist the math with logic</b> by writing some code to make it work better for us:  In this example, we could modify the code to just retain the old moving average from before the time-frame we consider to be faulty.  That way, when the anomaly passes, we resume "where we left off".  For timeseries that exhibit seasonality and a trend, we need to do a bit more, but the idea stays the same.   Restricting ourselves to only math and statistics cripples our ability to detect actual <b>faults</b> (problems).

</p>

<p>

Another example: During his <a href="https://coderanger.net/talks/echo/">Monitorama talk</a>, Noah Kantrowitz made the interesting and thought provoking observation that Nagios flap detection is basically a low-pass filter.  A few people suggested re-implementing flap detection as a low-pass filter.  This seems backwards to me because reducing the problem to a pure mathematical formula loses information.  The current code has the high-resolution view of above/below threshold and can visualize as such.  Why throw that away and limit your visibility?

</p>

<h2>Unsupervised machine learning... let's not get ahead of ourselves.</h2>

<p><a href="https://codeascraft.com/2013/06/11/introducing-kale/">Etsy&rsquo;s Kale</a> has ambitious goals: you configure a set of algorithms, and those algorithms get applied to <b>all</b> of your timeseries.  Out of that should come insights into what&rsquo;s going wrong.  The premise is that the found anomalies are relevant and indicative of faults that require our attention.</p>

<p><br/>I have quite a variety amongst my metrics.  For example diskspace metrics exhibit a sawtooth pattern (due to constant growth and periodic cleanup),</p>

<p>crontabs cause (by definition) periodic spikes in activity, user activity causes a fairly smooth graph which is characterized by its daily pattern and often some seasonality and a long-term trend.</p>

<p><br/></p>

<p><br/></p>

<p><img width="70%" src="http://dieter.plaetinck.be//files/anomaly-detection-cases.png"></p>

<p><br/></p>

<p><br/>Because they look differently, anomalies and faults look different too.  In fact, within each category there are multiple problematic scenarios. (e.g. user activity based timeseries should not suddenly drop, but also not be significantly lower than other days, even if the signal stays smooth and follows the daily rhythm)</p>

<p><br/></p>

<p><br/>I have a hard time believing that running the same algorithms on all of that data, and doing minimal configuration on them, will produce meaningful results. At least I expect a very low signal/noise ratio.  Unfortunately, of the people who I&rsquo;ve asked about their experiences with Kale/Skyline, the only cases where it&rsquo;s been useful is where skyline input has been restricted to a certain category of metrics - it&rsquo;s up to you do this filtering (perhaps via carbon-relay rules), potentially running multiple skyline instances - and sufficient time is required hand-selecting the appropriate algorithms to match the data.  This reduces the utility.</p>

<p><br/>&ldquo;Minimal configuration&rdquo; sounds great but this doesn&rsquo;t seem to work.</p>

<p><br/></p>

<p>Instead, something like <a href="http://bosun.org/">Bosun</a> (see further down) where you can visualize your series, experiment with algorithms and see the results in place on current and historical data, to manage alerting rules seems more practical.</p>

<p><br/></p>

<p><br/>Some companies (all proprietary) take it a step further and pay tens of engineers to work on algorithms that inspect all of your series, classify them into categories, &ldquo;learn&rdquo; them and automatically configure algorithms that will do anomaly detection, so it can alert anytime something looks unusual (though not necessarily faulty).</p>

<p>This probably works fairly well, but has a high cost, still can&rsquo;t know everything there is to know about your timeseries, is of no help of your timeseries is behaving faulty from the start and still alerts on anomalous, but irrelevant outliers.</p>

<p><br/></p>

<p><br/></p>

<p>I&rsquo;m <b>suggesting we don&rsquo;t need to make it that fancy</b> and we can do much better by <b>injecting some domain knowledge</b> into our monitoring system:</p>

<ul>

<li>using minimal work of classifying metrics via metric meta-data or rules that parse metric id's, we can automatically infer knowledge of how the series is supposed to behave (e.g. assume that disk_mb_used looks like sawtooth, frontend_requests_per_s daily seasonal, etc) and apply fitting processing accordingly.

<br/>Any sysadmin or programmer can do this, it's a bit of work but should make a hands-off automatic system such as Kale more accurate.

<br/>Of course, adopting <a href="http://metrics20.org/">metrics 2.0</a> will help with this as well. Another problem with machine learning is they would have to infer how metrics relate against each other, whereas with metric metadata this can easily be inferred (e.g.: what are the metrics for different machines in the same cluster, etc)</li>

<li>hooking into service/configuration management: you probably already have a service, tool, or file that knows how your infrastructure looks like and which services run where.  We know where user-facing apps run, where crontabs run, where we store log files, where and when we run cleanup jobs.  We know in what ratios traffic is balanced across which nodes, and so on.  Alerting systems can leverage this information to apply better suited fault detection rules.  And you don't need a large machine learning infrastructure for it. (as an aside: I have a lot more ideas on cloud-monitoring integration)</li>

<li>Many scientists are working on algorithms that find cause and effect when different series exhibit anomalies, so they can send more useful alerts.  But again here, a simple model of the infrastructure gives you service dependencies in a much easier way.</li>

<li>hook into your event tracking. If you have something like <a href="https://github.com/Dieterbe/anthracite/">anthracite</a> that lists upcoming press releases, then your monitoring system knows not to alert if suddenly traffic is a bit higher.  In fact, you might want to alert if your announcement did not create a sudden increase in traffic.  If you have a large scale infrastructure, you might go as far as tagging upcoming maintenance windows with metadata so the monitoring knows which services or hosts will be affected (and which shouldn't).

</ul>

<p><br/></p>

<p>Anomaly detection is useful if you don&rsquo;t know what you&rsquo;re looking for, or providing an extra watching eye on your log data.  Which is why it&rsquo;s commonly used for detecting fraud in security logs and such.</p>

<p>For operational metrics of which admins know what they mean, should and should not look like, and how they relate to each other, we can build more simple and more effective solutions.</p>

<h2>The trap of complex event processing... no need to abandon familiar tools</h2>

<p>On your quest into better alerting, you soon read and hear about real-time stream processing, and CEP (complex event processing) systems.</p>

<p>It&rsquo;s not hard to be convinced on their merits:  who wouldn&rsquo;t want real-time as-soon-as-the-data-arrives-you-can-execute-logic-and-fire-alerts?</p>

<p><br/>They also come with a fairly extensive and flexible language that lets you program or compose monitoring rules using your domain knowledge.</p>

<p>I believe I&rsquo;ve heard of <a href="https://storm.apache.org/">storm</a> for monitoring, but <a href="http://riemann.io/">Riemann</a> is the best known of these tools that focus on open source monitoring.</p>

<p>It is a nice, powerful tool and probably the easiest of the CEP tools to adopt.  It can also produce very useful dashboards.</p>

<p>However, these tools come with their own API or language, and programming against real-time streams is quite a paradigm shift which can be hard to justify.  And while their architecture and domain specificity works well for large scale situations, these benefits aren&rsquo;t worth it for most (medium and small) shops I know:  it&rsquo;s a lot easier (albeit less efficient) to just query a datastore over and over and program in the language you&rsquo;re used to.  With a decent timeseries store (or one written to hold the most recent data in memory such as <a href="https://github.com/dgryski/carbonmem">carbonmem</a>) this is not an issue, and the difference in timeliness of alerts becomes negligible!</p>

<h2>An example: finding spikes</h2>

<p>Like many places, we were stuck with static thresholds, which don&rsquo;t cover some common failure scenarios.</p>

<p>So I started asking myself some questions:</p>

<p><br></p>

<p><br></p>

<p><center></p>

<pre><code>&lt;i&gt;which behavioral categories of timeseries do we have, what kind of issues can arise in each category,

    &lt;br/&gt;how does that look like in the data, and what's the simplest way I can detect each scenario?&lt;/i&gt;
</code></pre>

<p></center></p>

<p><br/></p>

<p>Our most important data falls within the user-driven category from above where various timeseries from across the stack are driven by, and reflect user activity.  And within this category, the most common problem (at least in my experience) is spikes in the data.  I.e. a sudden drop in requests/s or a sudden spike in response time.  As it turned out, this is much easier to detect than one might think:</p>

<p><br/></p>

<p><img style="float:left; margin:15px;" src="http://dieter.plaetinck.be/files/poor-mans-fault-detection.png"></p>

<p><br/></p>

<p>In this example I just track the standard deviation of a moving window of 10 points.  <a href="http://en.wikipedia.org/wiki/Standard_deviation">Standard deviation</a> is simply a measure of how much numerical values differ from each other.  Any sudden spike bumps the standard deviation.   We can then simply set a threshold on the deviation.  Fairly trivial to set up, but has been highly effective for us.</p>

<p><br/></p>

<p><br/>You do need to manually declare what is an acceptable standard deviation value to be compared against, which you will typically deduce from previous data.  This can be annoying until you build an interface to speed up, or a tool to automate this step.</p>

<p><br/>In fact, it&rsquo;s trivial to collect previous deviation data (e.g. from the same time of the day, yesterday, or the same time of the week, last week) and automatically use that to guide a threshold.  (<a href="http://bosun.org/">Bosun</a> - see the following section - has &ldquo;band&rdquo; and &ldquo;graphiteBand&rdquo; functions to assist with this).  This is susceptible to previous outliers, but you can easily select multiple previous timeframes to minimize this issue in practice.</p>

<p><br/></p>

<p><a href="https://groups.google.com/forum/#!topic/it-telemetry/Zb2H4DP6qtk">it-telemetry thread</a></p>

<p><br></p>

<p><br></p>

<p>So without requiring fancy anomaly detection, machine learning, advanced math, or event processing, we are able to reliably detect faults using simple, familiar tools.  Some basic statistical concepts (standard deviation, moving average, etc) are a must, but nothing that requires getting a PhD.  In this case I&rsquo;ve been using <a href="http://graphite.readthedocs.org/en/0.9.10/functions.html#graphite.render.functions.stdev">Graphite&rsquo;s stdev function</a> and <a href="http://vimeo.github.io/graph-explorer/">Graph-Explorer&rsquo;s</a> alerting feature to manage these kinds of rules, but it doesn&rsquo;t allow for a very practical iterative workflow, so the non-trivial rules will be going into <a href="http://bosun.org/">Bosun</a>.</p>

<p><br/>BTW, you can also <a href="http://obfuscurity.com/2012/05/Polling-Graphite-with-Nagios">use a script to query Graphite from a Nagios check and do your logic</a></p>

<p><br/></p>

<p><br/></p>

<p><br/></p>

<!--

divideSeries(stdev(avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)),10),avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)))



why keepLastValue?

* sumSeries -> none counts as 0, so you can experience big drops which would trigger anomaly detection or failover

* averageSeries -> effectively ignores none values, so your accuracy can drop a lot in light of none values.



of course this masks when your monitoring breaks, so you still need something else to detect anomalies in the "out-of-date-ness" of your points.

-->

<h2>Workflow is key.  A closer look at bosun</h2>

<p>One of the reasons we&rsquo;ve been chasing self-learning algorithms is that we have lost faith in the feasibility of a more direct approach.  We can no longer imagine building and maintaining alerting rules because we have no system that provides powerful alerting, helps us keep oversight and streamlines the process of maintaining and iteratively developing alerting.</p>

<p><br/>I recently discovered <a href="http://bosun.org/">bosun</a>, an alerting frontend (&ldquo;IDE&rdquo;) by Stack Exchange, <a href="https://www.usenix.org/conference/lisa14/conference-program/presentation/brandt">presented at Lisa14</a>.  I highly recommend watching the video.  They have identified various issues that made alerting a pain, and built a solution that makes human-controlled alerting much more doable.  We&rsquo;ve been using it for a month now with good results (I also gave it support for Graphite).</p>

<p>I&rsquo;ll explain its merits, and it&rsquo;ll also become apparent how this ties into some of the things I brought up above:</p>

<p><img style="float:left; margin:35px;" src="http://bosun.org/public/ss_rule_timeline.png" width="15%"></p>

<ul>

<li>in each rule you can query any data you need from any of your datasources (currently graphite, openTSDB, and elasticsearch).  You can call various <a href="http://bosun.org/configuration.html">functions, boolean logic, and math</a>.  Although it doesn't expose you a full programming language, the bosun language as it stands is fairly complete, and can be extended to cover

new needs.  You choose your own alerting granularity (it can automatically instantiate alerts for every host/service/$your_dimension/... it finds within your metrics, but you can also trivially aggregate across dimensions, or both).  This makes it easy to create advanced alerts that cover a lot of ground, making sure you don't get overloaded by multiple smaller alerts.  And you can incorporate data of other entities within the system, to simply make better alerting decisions.</li>

<li>you can define your own templates for alert emails, which can contain any html code.  You can trivially plot graphs, build tables, use colors and so on.  Clear, context-rich alerts which contain all information you need!</li>

<li>As alluded to, the bosun authors spent a lot of time <a href="https://www.usenix.org/conference/lisa14/conference-program/presentation/brandt">thinking about, and solving</a> the workflow of alerting.  As you work on advanced fault detection and alerting rules you need to be able to see the value of all data (including intermediate computations) and visualize it.  Over time, you will iteratively adjust the rules to become better and more precise.  Bosun supports all of this.  You can execute your rules on historical data and see exactly how the rule performs over time, by displaying the status in a timeline view and providing intermediate values.  And finally, you can see how the alert emails will be rendered <i>as you work on the rule and the templates</i></li>

</ul>

<p>The <a href="http://bosun.org/examples.html">examples</a> section gives you an idea of the things you can do.</p>

<p><br/>I haven&rsquo;t seen anything solve a pragmatic alerting workflow like bosun (hence their name &ldquo;alerting IDE&rdquo;), and its ability to not hold you back as you work on your alerts is refreshing. Furthermore, the built-in processing functions are very <b>complimentary to graphite</b>:</p>

<p>Graphite has a decent API which works well at aggregating and transforming one or more series into one new series; the bosun language is great at reducing series to single numbers, providing boolean logic, and so on, which you need to declare alerting expressions.  This makes them a great combination.</p>

<p><br/>Of course Bosun isn&rsquo;t perfect either.  Plenty of things can be done to make it (and alerting in general) better.  But it does exemplify many of my points, and it&rsquo;s a nice leap forward in our monitoring toolkit.</p>

<h2>Conclusion</h2>

<p>Many of us aren&rsquo;t ready for some of the new technologies, and some of the technology isn&rsquo;t - and perhaps never will be - ready for us.</p>

<p>As an end-user investigating your options, it&rsquo;s easy to get lured in a direction that promotes over-complication and stimulates your inner scientist but just isn&rsquo;t realistic.</p>

<p><br/>Taking a step back, it becomes apparent we <b>can</b> setup automated fault detection.  But instead of using machine learning, use metadata, instead of trying to come up with all-encompassing holy grail of math, use several rules of code that you prototype and iterate over, then reuse for similar cases.  Instead of requiring a paradigm shift, use a language you&rsquo;re familiar with.  Especially by polishing up the workflow, we can make many &ldquo;manual&rdquo; tasks much easier and quicker.  I believe we can keep polishing up the workflow, distilling common patterns into macros or functions that can be reused, leveraging metric metadata and other sources of truth to configure fault detection, and perhaps even introducing &ldquo;metrics coverage&rdquo;, akin to &ldquo;code coverage&rdquo;: verify how much, and which of the metrics are adequately represented in alerting rules, so we can easily spot which metrics have yet to be included in alerting rules.  I think there&rsquo;s a lot of things we can do to make fault detection work better for us, but we have to look in the right direction.</p>

<h2>PS: leveraging metrics 2.0 for anomaly detection</h2>

<p>In my last <a href="https://www.usenix.org/conference/lisa14/conference-program/presentation/plaetinck">metrics 2.0 talk, at LISA14</a> I explored a few ideas on leveraging metrics 2.0 metadata for alerting and fault detection, such as automatically discovering error metrics across the stack, getting high level insights via tags, correlation, etc. If you&rsquo;re interested, it&rsquo;s in the video from 24:55 until 29:40</p>

<p><br/></p>

<p><br/></p>

<p><center></p>

<p><img src="http://dieter.plaetinck.be/files/metrics20-alerting.png" width="50%"></p>

<p></center></p>

<!--

It's not about alerts anyway.



alerts are an immensely crude approach to raising operator awareness.

They are basically boolean: either they interrupt your workflow or they don't.  There's no in between.

Yes, you can just check your alert emails "once in a while", but then realize that after an email or text is sent,

there is no way to update them with new information.  Which is really limiting once you start thinking about it.

Updates have to be provided via new "alerts", or they are available in the monitoring interface but there's no way to tell

by just glancing at your alert overview.  You might be looking at very out of date information.

-> only sent alerts for critical things.

-->

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/it-telemetry-google-group-collaboration-operational-insights/">IT-Telemetry Google group.  Trying to foster more collaboration around operational insights.</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/it-telemetry-google-group-collaboration-operational-insights/" rel="bookmark">
					<time class="entry-date" datetime="2014-12-06 16:01:02 -0400 -0400">
						December 6, 2014
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>I&rsquo;ve seen a bunch of people present their work in advancing the state of the art in this domain:</p>

<p><br/>from <a href="http://mabrek.github.io/">Anton Lebedevich&rsquo;s statistics for monitoring series</a>, <a href="https://vimeo.com/95069158">Toufic Boubez&rsquo; talks on anomaly detection</a> and Twitter&rsquo;s work on <a href="https://blog.twitter.com/2014/breakout-detection-in-the-wild">detecting mean shifts</a> to projects such as <a href="http://flapjack.io/">flapjack</a> (which aims to offload the alerting responsibility from your monitoring apps), the <a href="http://metrics20.org/">metrics 2.0 standardization effort</a> or <a href="https://codeascraft.com/2013/06/11/introducing-kale/">Etsy&rsquo;s Kale stack</a> which tries to bring interesting changes in timeseries to your attention with minimal configuration.</p>

<p><br/></p>

<p><br/></p>

<p>Much of this work is being shared via conference talks and blog posts, especially around anomaly and fault detection, and I couldn&rsquo;t find a location for collaboration, quicker feedback and discussions on more abstract (algorithmic/mathematical) topics or those that cross project boundaries.  So I created the <a href="https://groups.google.com/forum/#!forum/it-telemetry">IT-telemetry</a> Google group.  If I missed something existing, let me know.  I can shut this down and point to whatever already exists.  Either way I hope this kind of avenue proves useful to people working on these kinds of problems.</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/a-real-whisper-to-influxdb-program/">A real whisper-to-InfluxDB program.</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/a-real-whisper-to-influxdb-program/" rel="bookmark">
					<time class="entry-date" datetime="2014-09-30 08:37:48 -0400 EDT">
						September 30, 2014
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>I hinted that one could write a Go program using the unofficial <a href="https://github.com/kisielk/whisper-go">whisper-go</a> bindings and the <a href="https://github.com/influxdb/influxdb/tree/master/client">influxdb Go client library</a>.</p>

<p>That&rsquo;s what I did now, it&rsquo;s at <a href="https://github.com/vimeo/whisper-to-influxdb">github.com/vimeo/whisper-to-influxdb</a>.</p>

<p>It uses configurable amounts of workers for both whisper fetches and InfluxDB commits,</p>

<p>but it&rsquo;s still a bit naive in the sense that it commits to InfluxDB one serie at a time, irrespective of how many records are in it.</p>

<p>My series, and hence my commits have at most 60k records, and presumably InfluxDB could handle a lot more per commit, so we might leverage better batching later.  Either way, this way I can consistently commit about 100k series every 2.5 hours (or 10/s), where each serie has a few thousand points on average, with peaks up to 60k points. I usually play with 1 to 30 InfluxDB workers.</p>

<p>Even though I&rsquo;ve hit a few <a href="https://github.com/influxdb/influxdb/issues/985">InfluxDB</a> <a href="https://github.com/influxdb/influxdb/issues/970">issues</a>, this tool has enabled me to fill in gaps after outages and to do a restore from whisper after a complete database wipe.</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
                <a href="http://localhost:1313//tags/golang/index.html" rel="tag">golang</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/influxdb-as-graphite-backend-part2/">InfluxDB as a graphite backend, part 2</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/influxdb-as-graphite-backend-part2/" rel="bookmark">
					<time class="entry-date" datetime="2014-09-24 07:56:01 -0400 EDT">
						September 24, 2014
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p><br/>Updated oct 1, 2014 with a new <i>Disk space efficiency</i> section which fixes some mistakes and adds more clarity.</p>

<p><br/></p>

<p><p></p>

<p>The <i>Graphite + InfluxDB</i> series continues.</p>

<ul>

<li>In part 1, <a href="/on-graphite-whisper-and-influxdb.html">"On Graphite, Whisper and InfluxDB"</a> I described the problems of Graphite's whisper and ceres, why I disagree with common graphite clustering advice as being the right path forward, what a great timeseries storage system would mean to me, why InfluxDB - despite being the youngest project - is my main interest right now, and introduced my approach for combining both and leveraging their respective strengths: InfluxDB as an ingestion and storage backend (and at some point, realtime processing and pub-sub) and graphite for its renown data processing-on-retrieval functionality.

Furthermore, I introduced some tooling: <a href="https://github.com/graphite-ng/carbon-relay-ng">carbon-relay-ng</a> to easily route streams of carbon data (metrics datapoints) to storage backends, allowing me to send production data to Carbon+whisper as well as InfluxDB in parallel, <a href="https://github.com/brutasse/graphite-api">graphite-api</a>, the simpler Graphite API server, with <a href="https://github.com/vimeo/graphite-influxdb">graphite-influxdb</a> to fetch data from InfluxDB.

</li>

<li>Not Graphite related, but I wrote <a href="https://github.com/Dieterbe/influx-cli">influx-cli</a> which I introduced <a href="/influx-cli_a_commandline_interface_to_influxdb.html">here</a>.  It allows to easily interface with InfluxDB and measure the duration of operations, which will become useful for this article.</li>

<li>In the <a href="graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay.html">Graphite &amp; Influxdb intermezzo</a> I shared a script to import whisper data into InfluxDB and noted some write performance issues I was seeing, but the better part of the article described the various improvements done to <a href="https://github.com/graphite-ng/carbon-relay-ng">carbon-relay-ng</a>, which is becoming an increasingly versatile and useful tool.</li>

<li>In <a href="/using-influxdb-as-graphite-backend-part2.html">part 2</a>, which you are reading now, I'm going to describe recent progress, share more info about my setup, testing results, state of affairs, and ideas for future work</li>

</ul>

<h4>Progress made</h4>

<ul>

<li>InfluxDB saw two major releases:

<ul>

<li>0.7 (and followups), which was mostly about some needed features and bug fixes</li>

<li>0.8 was all about bringing some major refactorings in the hands of early adopters/testers: support for multiple storage engines, configurable shard spaces, rollups and retention schemes. There was some other useful stuff like speed and robustness improvements for the graphite input plugin (by yours truly) and various things like regex filtering for 'list series'.  Note that a bunch of older bugs remained open throughout this release (most notably the broken <a href="https://github.com/influxdb/influxdb/issues/334">derivative aggregator</a>), and a bunch of new ones appeared. Maybe this is why the release was mostly in the dark.  In this context, it's not so bad, because we let graphite-api do all the processing, but if you want to query InfluxDB directly you might hit some roadblocks.</li>

<li>An older fix, but worth mentioning: series names can now also contain any character, which means you can easily use <a href="http://metrics20.org/">metrics2.0</a> identifiers.  This is a welcome relief after having struggled with Graphite's restrictions on metric keys.</li>

</ul>

<p></li></p>

<p><li><a href="http://graphite-api.readthedocs.org">graphite-api</a> received various bug fixes and support for templating, statsd instrumentation and caching.</p>

<p><br/>Much of this was driven by graphite-influxdb: the caching allows us to cache metadata and the statsd integration gives us insights into the performance of the steps it goes through of building a graph (getting metadata from InfluxDB, querying InfluxDB, interacting with cache, post processing data, etc).</li></p>

<p><li>the progress on InfluxDB and graphite-api in turn enabled <a href="https://github.com/vimeo/graphite-influxdb">graphite-influxdb</a> to become faster and simpler (note: graphite-influxdb requires InfluxDB 0.8).  Furthermore you can now configure series resolutions (but different retentions per serie is on the roadmap, see <i>State of affairs and what&rsquo;s coming</i>), and of course it also got a bunch of bugfixes.</li></p>

<p></ul></p>

<p>Because of all these improvements, all involved components are now ready for serious use.</p>

<h4>Putting it all together, with docker</h4>

<p><a href="https://www.docker.com/">Docker</a> probably needs no introduction, it&rsquo;s a nifty tool to build an environment with given software installed, and allows to easily deploy it and run it in isolation.</p>

<p><a href="https://github.com/vimeo/graphite-api-influxdb-docker">graphite-api-influxdb-docker</a> is a very creatively named project that generates the - also very creatively named - docker image <a href="https://registry.hub.docker.com/u/vimeo/graphite-api-influxdb/">graphite-api-influxdb</a>, which contains graphite-api and graphite-influxdb, making it easy to hook in a customized configuration and get it up and running quickly.  This is the recommended way to set this up, and this is what we run in production.</p>

<h4>The setup</h4>

<ul>

<li>a server running InfluxDB and graphite-api with graphite-influxdb via the docker approach described above:

<pre>

dell PowerEdge R610

24 x Intel(R) Xeon(R) X5660  @ 2.80GHz

96GB RAM

perc raid h700

6x600GB seagate 10k rpm drives in raid10 = 1.6 TB, Adaptive Read Ahead, Write Back, 64 kB blocks, no read caching

no sharding/shard spaces, compiled from git just before 0.8, using LevelDB (not rocksdb, which is now the default)

LevelDB max-open-files = 10000 (lsof shows about 30k open files total for the InfluxDB process), LRU 4096m, everything else is default I think.

</pre>

</li>

<li>a server running graphite-web, carbon, and whisper:

<pre>

dell PowerEdge R710

16 x Intel(R) Xeon(R) E5640  @ 2.67GHz

96GB RAM

perc raid h700

8x150GB seagate 15k rm in raid5 = 952 GB, Read Ahead, Write Back, 64 kB blocks, no read caching

MAX_UPDATES_PER_SECOND = 1000  # to sequentialize writes

</pre>

</li>

<li>a relay server running carbon-relay-ng that sends the same production load into both.  (about 2500 metrics/s, or 150k minutely)</li>

</ul>

<p>As you can tell, on both machines RAM is vastly over provisioned, and they have lots of cpu available (the difference in cores should be negligible), but the difference in RAID level is important to note: RAID 5 comes with a write penalty. Even though the whisper machine has more, and faster disks, it probably has a disadvantage for writes.  Maybe.  Haven&rsquo;t done raid stuff in a long time, and I haven&rsquo;t it measured it out.</p>

<p><br/><b>Clearly you&rsquo;ll need to take the results with a grain of salt, as unfortunately I do not have 2 systems available with the same configuration and their baseline (raw) performance is unknown.</b>.</p>

<p><br/>Note: no InfluxDB clustering, see <i>State of affairs and what&rsquo;s coming</i>.</p>

<h4>The empirical validation &amp; migration</h4>

<p>Once everything was setup and I could confidently send 100% of traffic to InfluxDB via carbon-relay-ng, it was trivial to run our dashboards with a flag deciding which server to go to.</p>

<p>This way I have literally been running our graphite dashboards next to each other, allowing us to compare both stacks on:</p>

<ul>

<li>visual differences: after a bunch of work and bug fixing, we got to a point where both dashboards looked almost exactly the same.  (note that graphite-api's implementation of certain functions can behave slightly different, see for example this <a href="https://github.com/brutasse/graphite-api/issues/66">divideSeries bug</a>)</li>

<li>speed differences by simply refreshing both pages and watching the PNGs load, with some assistance from firebug's network requests profiler.  The difference here was big: graphs served up by graphite-api + InfluxDB loaded considerably faster.  A page with 40 graphs or so would load in a few seconds instead of 20-30 seconds (on both first, as well as subsequent hits).  This is for our default, 6-hour timeframe views.  When cranking the timeframes up to a couple of weeks, graphite-api + InfluxDB was still faster.</li>

</ul>

<p>Soon enough my colleagues started asking to make graphite-api + InfluxDB the default, as it was much faster in all common cases.  I flipped the switch and everybody has been happy.</p>

<p><br/></p>

<p><br/></p>

<p>When loading a page with many dashboards, the InfluxDB machine will occasionally spike up to 500% cpu, though I rarely get to see any iowait (!), even after syncing the block cache (i just realized it&rsquo;ll probably still use the cache for reads after sync?)</p>

<p><br/>The carbon/whisper machine, on the other hand, is always fighting iowait, which could be caused by the raid 5 write amplification but the random io due to the whisper format probably has more to do with it.  Via the MAX_UPDATES_PER_SECOND I&rsquo;ve tried to linearize writes, with mixed success.  But I&rsquo;ve never gone to deep into it.  So basically <b>comparing write performance would be unfair in these circumstances, I am only comparing reads in these tests</b>.  Despite the different storage setups, the Linux block cache should make things fair for reads.   Whisper&rsquo;s iowait will handicap the reads, but I always did successive runs with fully loaded PNGs to make sure the block cache was warm for reads.</p>

<h4>A "slightly more professional" benchmark</h4>

<p>I could have stopped here, but the validation above was not very scientific.  I wanted to do a somewhat more formal benchmark, to measure read speeds (though I did not have much time so it had to be quick and easy).</p>

<p><br/>I wanted to compare InfluxDB vs whisper, and specifically how performance scales as you play with parameters such as number of series, points per series, and time range fetched (i.e. amount of points).  I <a href="https://groups.google.com/forum/#!topic/influxdb/0VeUQCqzgVg">posted the benchmark on the InfluxDB mailing list</a>.  Look there for all information. I just want to reiterate the conclusion here:  I was surprised.  Because of the results above, I had assumed that InfluxDB would perform reads noticeably quicker than whisper but this is not the case.  (maybe because whisper reads are nicely sequential - it&rsquo;s mostly writes that suffer from the whisper format)</p>

<p><br/>This very much contrasts my earlier findings where the graphite-api+InfluxDB powered dashboards clearly take the lead.  I have yet to figure out why this is.  Maybe something to do with the performance of graphite-web vs graphite-api itself, gunicorn vs apache, worker configuration, or maybe InfluxDB only starts outperforming whisper as concurrency increases.  Some more investigation is definitely needed!</p>

<h4>Future benchmarks</h4>

<p>The simple benchmark above was very simple to execute, as it only requires influx-cli and whisper-fetch (so you can easily check for yourself), but clearly there is a need to test more realistic scenarios with concurrent reads, and doing some write benchmarks would be nice too.</p>

<p><br/>We should also look into cpu and memory usage.  I have had the luxury of being able to completely ignore memory usage, but others seem to notice excessive InfluxDB memory usage.</p>

<p><br/>conclusion: many tests and benchmarks should happen, but I don&rsquo;t really have time to conduct them.  Hopefully other people in the community will take this on.</p>

<h4>Disk space efficiency</h4>

<p>Last time I checked, using LevelDB I was pretty close to 24B per record (which makes sense because time, seq_no and value are all 64bit values, and each record has those 3 fields).  (this was with snappy compression enabled, so it didn&rsquo;t seem to give much benefit).</p>

<p><br/>Whisper seems to consume 12 Bytes per record - a 32bit timestamp and a 64bit float value - making it considerably more storage efficient than InfluxDB/levelDB for now.</p>

<p><br/>Some notes on this though:</p>

<ul>

<li>whisper explicitly encodes None values, with InfluxDB those are implied (and require no space).  We have some clusters of metrics that have very sparse data, so whisper gives us a lot of overhead here, but this is different for everyone.  (note: Ceres should also be better at handling sparse data)</li>

<li>Whisper and Influxdb both explictly encode the timestamp for every record.  Influxdb uses 64bit so you can do very high resolution (up to microseconds), whisper is limited to per-second data.  Ceres AFAIK doesn't explicitly encode the timestamp at every record, which should also give it a space advantage.</li>

<li>I've been using a data format in InfluxDB where every record is timestamp-sequence_number-value.  It currently works best overall, and so that's how the graphite ingestion plugin stores it and the graphite-influxdb plugin queries for it.  But it exacerbates the overhead of the timestamp and sequence number.

<br/>We could technically use a row format where we use more variables as part of the record, storing them as columns instead of separate series, which would improve this dynamic (but currently comes with a big tradeoff in performance characteristics - see the <a href="https://github.com/influxdb/influxdb/issues/582">column indexes</a> ticket).

<br/>Another thing is that we could technically come up with a storage format for InfluxDB that is optimized for even-spaced metrics, it wouldn't need sequence numbers, and timestamps could be implicit instead of explicit, saving a lot of space.  We could even go further and introduce types (int, etc) for values which would consume even less space.

</ul>

<p><br/></p>

<p>It would be great if somebody with more Ceres experience could chip in here, as - in the context of space efficiency - it looks like a neat little format.</p>

<p>Also, I&rsquo;m probably not making proper use of the compression features that InfluxDB&rsquo;s storage engines support.  This also requires some more looking into.</p>

<h4>State of affairs and what's coming</h4>

<ul>

<li>InfluxDB typically performs pretty well, but not in all cases.  More validation is needed. It wouldn't surprise me at this point if tools like hbase/Cassandra/riak clearly outperform InfluxDB, as long as we keep in mind that InfluxDB is a young project.  A year, or two, from now, it'll probably perform much better. (and then again, it's not all about raw performance.  InfluxDB's has other strengths)</li>

<li>A long time goal which is now a reality:  <b>You can use any Graphite dashboard on top of InfluxDB, as long as the data is stored in a graphite-compatible format.</b>.  Again, the easiest to get running is via <a href="https://github.com/vimeo/graphite-api-influxdb-docker">graphite-api-influxdb-docker</a>.  There are two issues to be mentioned, though:

<ul>

<li>graphite-influxdb needs to query InfluxDB for metadata, and this <a href="https://github.com/influxdb/influxdb/issues/884">can be slow</a>.  If you have millions of metrics, it can take tens of seconds before querying for the data even starts.  I am trying to work with the InfluxDB people on a solution.</li>

<li><a href="https://github.com/brutasse/graphite-api/issues/57">graphite-api doesn't work with metric id's that have equals signs in them</a>.</li>

</ul>

<p><li>With the 0.8 release out the door, the shard spaces/rollups/retention intervals feature will start stabilizing, so we can start supporting multiple retention intervals per metric</li></p>

<p><li>Because InfluxDB clustering is <a href="https://github.com/influxdb/influxdb/pull/903">undergoing major changes</a>, and because clustering is not a high priority for me, I haven&rsquo;t needed to worry about this.  I&rsquo;ll probably only start looking at clustering somewhere in 2015 because I have more pressing issues.</li></p>

<p><li>Once the new clustering system and the storage subsystem have matured (sounds like a v1.0 ~ v1.2 to me) we&rsquo;ll get more speed improvements and robustness.  Most of the integration work is done, it&rsquo;s just a matter of doing smaller improvements, bug fixes and waiting for InfluxDB to become better.  Maintaining this stack aside, I personally will start focusing more on:</p>

<pre><code>&lt;ul&gt;

&lt;li&gt;per-second resolution in our data feeds, and potentially storage&lt;/li&gt;

&lt;li&gt;realtime (but basic) anomaly detection, realtime graphs for some key timeseries.  Adrian Cockcroft had an inspirational piece in his &lt;a href=&quot;https://vimeo.com/95064249&quot;&gt;Monitorama keynote&lt;/a&gt; about how alerts from timeseries should trigger within seconds.&lt;/li&gt;

&lt;li&gt;Mozilla's awesome &lt;a href=&quot;http://hekad.readthedocs.org&quot;&gt;heka&lt;/a&gt; project (this &lt;a href=&quot;https://vimeo.com/98689689&quot;&gt;heka video&lt;/a&gt; is great), which should help a lot with the above.  Also looking at &lt;a href=&quot;http://codeascraft.com/2013/06/11/introducing-kale/&quot;&gt;Etsy's kale stack&lt;/a&gt; for anomaly detection&lt;/li&gt;

&lt;li&gt;metrics 2.0 and making sure metrics 2.0 works well with InfluxDB.  Up to now I find the series / columns as a data model too limiting and arbitrary, it could be so much more powerful, ditto for the query language.&lt;/li&gt;
</code></pre>

<p></ul></p>

<p></li></p>

<p><li>Can we do anything else to make InfluxDB (+graphite) faster? Yes!</p>

<ul>

<li>Long term, of course, InfluxDB should have powerful enough processing functions and query syntax, so that we don't even need a graphite layer anymore.</li>

<li>A storage engine optimized for fixed intervals would probably help, timestamps and sequence numbers currently consume 2/3 of the record... and there's no reason to explicitly store either one in this use case.  I've even rarely seen people make use of the sequence number in any other InfluxDB use case.  See all the remarks in the <i>Disk space efficiency</i> section above.  Finally we could have InfluxDB have fill in None values without it doing "group by" (timeframe consolidation), which would shave off runtime overhead.</li>

<li>Then of course, there are projects to replace graphite-web/graphite-api with a Go codebase: <a href="https://github.com/graphite-ng/graphite-ng">graphite-ng</a> and <a href="https://github.com/dgryski/carbonapi">carbonapi</a>.  the latter is more production ready, but depends on some custom tooling and io using protobufs.  But it performs an order of magnitude better than the python api server!  I haven't touched graphite-ng in a while, but hopefully at some point I can take it up again</li>

</ul>

<p><li>Another thing to keep in mind when switching to graphite-api + InfluxDB: you loose the graphite composer.  I have a few people relying on this, so I can either patch it to talk to graphite-api (meh), separate it out (meh) or replace it with a nicer dashboard like tessera, grafana or descartes.  (or Graph-Explorer, but it can be a bit too much of a paradigm shift).</li></p>

<p><li>some more InfluxDB stuff I&rsquo;m looking forward to:</p>

<ul>

<li>binary protocol and result streaming (faster communication and responses!) (the latter might not get implemented though)</li>

<li>"list series" speed improvements (if metadata querying gets fast enough, we won't need ES anymore for metrics2.0 index)</li>

<li><a href="https://github.com/influxdb/influxdb/pull/635">InfluxDB instrumentation</a> so we actually start getting an idea of what's going on in the system, a lot of the testing and troubleshooting is still in the dark.</li>

</ul>

<p></li></p>

<p><li>Tracking exceptions in graphite-api is <a href="https://github.com/brutasse/graphite-api/search?q=exception&type=Issues&utf8=%E2%9C%93">much harder than it should be</a>.  Currently there&rsquo;s no way to display exceptions to the user (in the http response) or to even log them.  So sometimes you&rsquo;ll get http 500 responses and don&rsquo;t know why.  You can use the <a href="http://graphite-api.readthedocs.org/en/latest/configuration.html#extra-sections">sentry integration</a> which works all right, but is clunky.  Hopefully this will be addressed soon.</li></p>

<p></ul></p>

<h4>Conclusion</h4>

<p>The graphite-influxdb stack works and is ready for general consumption.  It&rsquo;s easy to install and operate, and performs well.</p>

<p>It is expected that InfluxDB will over time mature and ultimately meet all my <a href="/on-graphite-whisper-and-influxdb.html">requirements of the ideal backend</a>.  It definitely has a long way to go.  More benchmarks and tests are needed.  Keep in mind that we&rsquo;re not doing large volumes of metrics. For small/medium shops this solution should work well, but on larger scales you will definitely run into issues.  You might conclude that InfluxDB is not for you (yet) (there are alternative projects, after all).</p>

<p><br/></p>

<p><br/></p>

<p>Finally, a closing thought:</p>

<p><br/><i>Having graphs and dashboards that look nice and load fast is a good thing to have, but keep in mind that graphs and dashboards should be a last resort.  It&rsquo;s a solution if all else fails.  The fewer graphs you need, the better you&rsquo;re doing.</p>

<p><br/>How can you avoid needing graphs?  Automatic alerting on your data.</p>

<p><br/></p>

<p><br/>I see graphs as a temporary measure: they provide headroom while you develop an understanding of the operational behavior of your infrastructure, conceive a model of it, and implement the alerting you need to do troubleshooting and capacity planning.  Of course, this process consumes more resources (time and otherwise), and these expenses are not always justifiable, but I think this is the ideal case we should be working towards.</i></p>

<p><br/></p>

<p><br/></p>

<p>Either way, good luck and have fun!</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
                <a href="http://localhost:1313//tags/golang/index.html" rel="tag">golang</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay/">Graphite &amp; Influxdb intermezzo: migrating old data and a more powerful carbon relay</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay/" rel="bookmark">
					<time class="entry-date" datetime="2014-09-20 15:18:32 -0400 EDT">
						September 20, 2014
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<h4>Migrating data from whisper into InfluxDB</h4>

<p><i>&ldquo;How do i migrate whisper data to influxdb&rdquo;</i> is a question that comes up regularly, and I&rsquo;ve always replied it should be easy to write a tool</p>

<p>to do this.  I personally had no need for this, until a recent small influxdb outage where I wanted to sync data from our backup server (running graphite + whisper) to influxdb, so I wrote a script:</p>

<p><div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%">&lt;!<span style="color: #666666">[</span>CDATA<span style="color: #666666">[</span>

<span style="color: #408080; font-style: italic">#!/bin/bash</span>

<span style="color: #408080; font-style: italic"># whisper dir without trailing slash.</span>

<span style="color: #19177C">wsp_dir</span><span style="color: #666666">=</span>/opt/graphite/storage/whisper

<span style="color: #19177C">start</span><span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">$(</span>date -d <span style="color: #BA2121">&#39;sep 17 6am&#39;</span> +%s<span style="color: #008000; font-weight: bold">)</span>

<span style="color: #19177C">end</span><span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">$(</span>date -d <span style="color: #BA2121">&#39;sep 17 12pm&#39;</span> +%s<span style="color: #008000; font-weight: bold">)</span>

<span style="color: #19177C">db</span><span style="color: #666666">=</span>graphite

<span style="color: #19177C">pipe_path</span><span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">$(</span>mktemp -u<span style="color: #008000; font-weight: bold">)</span>

mkfifo <span style="color: #19177C">$pipe_path</span>

<span style="color: #008000; font-weight: bold">function</span> influx_updater<span style="color: #666666">()</span> <span style="color: #666666">{</span>

    influx-cli -db <span style="color: #19177C">$db</span> -async &lt; <span style="color: #19177C">$pipe_path</span>

<span style="color: #666666">}</span>

influx_updater &amp;

<span style="color: #008000; font-weight: bold">while</span> <span style="color: #008000">read </span>wsp; <span style="color: #008000; font-weight: bold">do</span>

  <span style="color: #19177C">series</span><span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">$(</span>basename <span style="color: #BB6688; font-weight: bold">${</span><span style="color: #19177C">wsp</span>//<span style="color: #BB6622; font-weight: bold">\/</span>/.<span style="color: #BB6688; font-weight: bold">}</span> .wsp<span style="color: #008000; font-weight: bold">)</span>

  <span style="color: #008000">echo</span> <span style="color: #BA2121">&quot;updating </span><span style="color: #19177C">$series</span><span style="color: #BA2121"> ...&quot;</span>

  whisper-fetch.py --from<span style="color: #666666">=</span><span style="color: #19177C">$start</span> --until<span style="color: #666666">=</span><span style="color: #19177C">$end</span> <span style="color: #19177C">$wsp_dir</span>/<span style="color: #19177C">$wsp</span>.wsp | grep -v <span style="color: #BA2121">&#39;None$&#39;</span> | awk <span style="color: #BA2121">&#39;{print &quot;insert into \&quot;&#39;</span><span style="color: #19177C">$series</span><span style="color: #BA2121">&#39;\&quot; values (&quot;$1&quot;000,1,&quot;$2&quot;)&quot;}&#39;</span> &gt; <span style="color: #19177C">$pipe_path</span>

<span style="color: #008000; font-weight: bold">done</span> &lt; &lt;<span style="color: #666666">(</span>find <span style="color: #19177C">$wsp_dir</span> -name <span style="color: #BA2121">&#39;*.wsp&#39;</span> | sed -e <span style="color: #BA2121">&quot;s#</span><span style="color: #19177C">$wsp_dir</span><span style="color: #BA2121">/##&quot;</span> -e <span style="color: #BA2121">&quot;s/.wsp</span><span style="color: #19177C">$/</span><span style="color: #BA2121">/&quot;</span><span style="color: #666666">)</span>

<span style="color: #666666">]]</span>&gt;
</pre></div>
<p></p>

<p>It relies on the recently introduced asynchronous inserts feature of <a href="https://github.com/Dieterbe/influx-cli">influx-cli</a> - which commits inserts in batches to improve the speed - and the whisper-fetch tool.</p>

<p><br/></p>

<p>You could probably also write a Go program using the unofficial <a href="https://github.com/kisielk/whisper-go">whisper-go</a> bindings and the <a href="https://github.com/influxdb/influxdb/tree/master/client">influxdb Go client library</a>.  But I wanted to keep it simple.  Especially when I found out that whisper-fetch is not a bottleneck: starting whisper-fetch, and reading out - in my case - 360 datapoints of a file always takes about 50ms, whereas InfluxDB at first only needed a few ms to flush hundreds of records, but that soon increased to seconds.</p>

<p><br/>Maybe it&rsquo;s a bug in my code, I didn&rsquo;t test this much, because I didn&rsquo;t need to; but people keep asking for a tool so here you go.  Try it out and maybe you can fix a bug somewhere.  Something about the write performance here must be wrong.</p>

<h4>A more powerful carbon-relay-ng</h4>

<p><a href="https://github.com/graphite-ng/carbon-relay-ng">carbon-relay-ng</a> received a bunch of love and has been a great help in my graphite+influxdb experiments.</p>

<p>

<a href="/files/carbon-relay-web-ui.png"><img width="441" src="/files/carbon-relay-web-ui.png" /></a>

</p>

<p>Here&rsquo;s what changed:</p>

<ul>

<li>First I made it so that you can adjust routes at runtime while data is flowing through, via a telnet interface.</li>

<li>Then <a href="https://github.com/pauloconnor">Paul O'Connor</a> built an embedded web interface to manage your routes in an easier and prettier way (pictured above)</li>

<li>The relay now also emits performance metrics via statsd (I want to make this better by using <a href="https://github.com/rcrowley/go-metrics">go-metrics</a> which will hopefully get <a href="https://github.com/rcrowley/go-metrics/issues/68">expvar support</a> at some point - any takers?).</li>

<li>Last but not least, I borrowed <a href="https://github.com/graphite-ng/carbon-relay-ng/tree/master/nsqd">the diskqueue</a> code from <a href="http://nsq.io/">NSQ</a> so now we can also spool to disk to bridge downtime of endpoints and re-fill them when they come back up</li>

</ul>

<p>Beside our metrics storage, I also plan to put our anomaly detection (currently playing with <a href="http://hekad.readthedocs.org/en/v0.7.1/">heka</a> and <a href="http://codeascraft.com/2013/06/11/introducing-kale/">kale</a>) and <a href="https://github.com/vimeo/carbon-tagger">carbon-tagger</a> behind the relay, centralizing all routing logic, making things more robust, and simplifying our system design.  The spooling should also help to deploy to our metrics gateways at other datacenters, to bridge outages of datacenter interconnects.</p>

<p><br/></p>

<p><br/></p>

<p>I used to think of carbon-relay-ng as the python carbon-relay but on steroids,</p>

<p>now it reminds me more of something like nsqd but with an ability to make packet routing decisions by introspecting the carbon protocol,</p>

<p><br/>or perhaps Kafka but much simpler, single-node (no HA), and optimized for the domain of carbon streams.</p>

<p><br/>I&rsquo;d like the HA stuff though, which is why I spend some of my spare time figuring out the intricacies of the increasingly popular <a href="http://raftconsensus.github.io/">raft</a> consensus algorithm.   It seems opportune to have a simpler Kafka-like thing, in Go, using raft, for carbon streams.</p>

<p>(note: InfluxDB <a href="https://github.com/influxdb/influxdb/pull/859">might introduce such a component</a>, so I&rsquo;m also a bit waiting to see what they come up with)</p>

<p><br/></p>

<p><br/></p>

<p>Reminder: notably missing from carbon-relay-ng is round robin and sharding.  I believe sharding/round robin/etc should be part of a broader HA design of the storage system, as I explained in <a href="http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html">On Graphite, Whisper and InfluxDB</a>.  That said, both should be fairly easy to implement in carbon-relay-ng, and I&rsquo;m willing to assist those who want to contribute it.</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
                <a href="http://localhost:1313//tags/golang/index.html" rel="tag">golang</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/monitorama-pdx-metrics20/">Monitorama PDX &amp; my metrics 2.0 presentation</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/monitorama-pdx-metrics20/" rel="bookmark">
					<time class="entry-date" datetime="2014-05-29 10:39:32 -0400 EDT">
						May 29, 2014
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>Earlier this month we had another iteration of the <a href="http://monitorama.com/">Monitorama</a> conference, this time in Portland, Oregon.</p>

<p></p></p>

<p>

<img src="/files/blog/monitorama-audience.jpg" width="800" />

<br/>(photo by <a href="https://www.flickr.com/photos/78527903@N00/sets/72157644593947233/">obfuscurity</a>)

</p>

<p>

I think the conference was better than the first one in Boston, much more to learn.  Also this one was quite focused on telemetry (timeseries metrics processing), lots of talks on timeseries analytics, not so much about things like sensu or nagios.  

<a href="https://vimeo.com/95064249">Adrian Cockroft's keynote</a> brought some interesting ideas to the table, like building a feedback loop into the telemetry to drive infrastructure changes (something we do at Vimeo, I briefly give an example in the intro of my talk) or shortening the time from fault to alert (which I'm excited to start working on soon)

<br/>My other favorite was <a href="https://vimeo.com/95227467">Noah Kantrowitz's talk about applying audio DSP techniques to timeseries</a>,

I always loved audio processing and production.  Combining these two interests hadn't occurred to me so now I'm very excited about the applications.

<br/>The opposite, but just as interesting of an idea - conveying information about system state as an audio stream - came up in <a href="http://puppetlabs.com/podcasts/podcast-insights-monitorama-conference">puppetlab's monitorama recap</a> and that seems to make a lot of sense as well. There's <b>a lot</b> of information in a stream of sound, it is much denser than text, icons and perhaps even graph plots.  Listening to an audio stream that's crafted to represent various information might be a better way to get insights into your system.

</p>

<p><br/></p>

<p><i></p>

<p>I'm happy to see the idea reinforced that telemetry is a key part of modern monitoring.

For me personally, telemetry (the tech and the process) is <b>the most fascinating part of modern technical operations</b>, and I'm glad to be part of the movement pushing this forward.  There's also a bunch of startups in the space (many stealthy ones), validating the market.  I'm curious to see how this will play out.

</p>

<p></i><br/></p>

<p>I had the privilege to present <a href="http://metrics20.org">metrics 2.0</a> and

<a href="http://vimeo.github.io/graph-explorer/">Graph-Explorer</a>.

As usual the <a href="http://www.slideshare.net/Dieterbe/metrics20-34319840">slides are on slideshare</a> and the <a href="https://vimeo.com/95076197">footage on the better video sharing platform</a> ;-) .

<br/>I'm happy with all the positive feedback, although I'm not aware yet of other tools and applications adopting metrics 2.0, and I'm looking forward to see some more of that, because ultimately that's what will show if my ideas are any good.

</p>

<p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/34319840" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" allowfullscreen> </iframe>



<iframe allowfullscreen="allowfullscreen" frameborder="0" height="356" mozallowfullscreen="mozallowfullscreen" src="//player.vimeo.com/video/95076197?title=0&amp;byline=0&amp;portrait=0&amp;color=33a352" webkitallowfullscreen="webkitallowfullscreen" width="633"></iframe>

</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/on-graphite-whisper-and-influxdb/">On Graphite, Whisper and InfluxDB</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/on-graphite-whisper-and-influxdb/" rel="bookmark">
					<time class="entry-date" datetime="2014-05-18 13:22:32 -0400 EDT">
						May 18, 2014
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>Graphite is a neat timeseries metrics storage system that comes with a powerful querying api, mainly due to the whole bunch of <a href="http://graphite.readthedocs.org/en/latest/functions.html">available processing functions</a>.</p>

<p><br/>For medium to large setups, the storage aspect quickly becomes a pain point.  Whisper, the default graphite storage format, is a simple storage format, using one file per metric (timeseries).</p>

<ul>

<li>It can't keep all file descriptors in memory so there's a lot of overhead in constantly opening, seeking, and closing files, especially since usually one write comes in for all metrics at the same time.

</li>

<li>Using the rollups feature (different data resolutions based on age) causes a lot of extra IO.</li>

<li>The format is also simply not optimized for writes.  Carbon, the storage agent that sits in front of whisper has a feature to batch up writes to files to make them more sequential but this doesn't seem to help much.</li>

<li>Worse, due to various <a href="https://github.com/pcn/carbon/blob/new-sending-mechanism/Why_Spooling.md#what-problems-have-we-had">implementation details</a> the carbon agent is surprisingly inefficient and even cpu-bound.  People often run into cpu limitations before they hit the io bottleneck.  Once the writeback queue hits a certain size, carbon will blow up.</li>

</ul>

<p>Common recommendations are to <a href="http://bitprophet.org/blog/2013/03/07/graphite/">run multiple carbon agents</a> and</p>

<p><a href="http://obfuscurity.com/2012/04/Unhelpful-Graphite-Tip-5">running graphite on SSD drives</a>.</p>

<p><br/>If you want to scale out across multiple systems, you can get carbon to shard metrics across multiple nodes, but the complexity can get out of hand and manually maintaining a cluster where nodes get added, fail, get phased out, need recovery, etc involves a lot of manual labor even though <a href="https://github.com/jssjr/carbonate/">carbonate</a> makes this easier.  This is a path I simply don&rsquo;t want to go down.</p>

<p><br/></p>

<p><br/></p>

<p>

<i>These might be reasonable solutions based on the circumstances (often based on short-term local gains), but I believe as a community we should solve the problem at its root, so that everyone can reap the long term benefits.

</i>

</p>

<p><br/></p>

<p>In particular, <a href="http://blog.sweetiq.com/2013/01/using-ceres-as-the-back-end-database-to-graphite/#axzz324uQtk3d">running Ceres instead of whisper</a>, is only a slight improvement, that suffers from most of the same problems.  I don&rsquo;t see any good reason to keep working on Ceres, other than perhaps that it&rsquo;s a fun exercise.   This probably explains the slow pace of development.</p>

<p><br/>However, many mistakenly believe Ceres is &ldquo;the future&rdquo;.</p>

<p><br/><a href="http://www.inmobi.com/blog/2014/01/24/extending-graphites-mileage">Switching to LevelDB</a> seems much more sensible but IMHO still doesn&rsquo;t cut it as a general purpose, scalable solution.</p>

<h4>The ideal backend</h4>

<p>I believe we can build a backend for graphite that</p>

<ul>

<li>can easily scale from a few metrics on my laptop in power-save mode to millions of metrics on a highly loaded cluster</li>

<li>supports nodes joining and leaving at runtime and automatically balancing the load across them</li>

<li>assures high availability and heals itself in case of disk or node failures</li>

<li>is simple to deploy.  think: just run an executable that knows which directories it can use for storage, elasticsearch-style automatic clustering, etc.</li>

<li>has the right read/write optimizations.  I've never seen a graphite system that is not write-focused, so something like <a href="http://en.wikipedia.org/wiki/Log-structured_merge-tree">LSM trees</a> seems to make a lot of sense.</li>

<li>can leverage cpu resources (e.g. for compression)</li>

<li>provides a more natural model for phasing out data.  Optional, runtime-changeable rollups.  And an age limit (possibly, but not necessarily round robin)

</ul>

<p>While we&rsquo;re at it. pub-sub for realtime analytics would be nice too.  Especially when it allows to use the same functions as the query api.</p>

<p><br/>And getting rid of the metric name restrictions such as inability to use dots or slashes.  Efficient sparse series support would be nice too.</p>

<h4>InfluxDB</h4>

<p>There&rsquo;s a lot of databases that you could hook up to graphite.</p>

<p>riak, hdfs based (opentsdb), Cassandra based (kairosdb, blueflood, cyanite), etc.</p>

<p>Some of these are solid and production ready, and would make sense depending on what you already have and have experience with.</p>

<p>I&rsquo;m personally very interested in playing with Riak, but decided to choose InfluxDB as my first victim.</p>

<p><br/></p>

<p><br/></p>

<p>InfluxDB is a young project that will need time to build maturity, but is on track to meet all my goals very well.</p>

<p>In particular, installing it is a breeze (no dependencies), it&rsquo;s specifically built for timeseries (not based on a general purpose database),</p>

<p>which allows them to do a bunch of simplifications and optimizations, is write-optimized, and should meet my goals for scalability, performance, and availability well.</p>

<p>And they&rsquo;re in NYC so meeting up for lunch has proven to be pretty fruitful for both parties.  I&rsquo;m pretty confident that these guys can pull off something big.</p>

<p><br/></p>

<p><br/></p>

<p>Technically, InfluxDB is a &ldquo;timeseries, metrics, and analytics&rdquo; databases with use cases well beyond graphite and even technical operations.</p>

<p>Like the alternative databases, graphite-like behaviors such as rollups management and automatically picking the series in the most appropriate resolutions, is something to be implemented on top of it.  Although you never know, it might end up being natively supported.</p>

<h4>Graphite + InfluxDB</h4>

<p>InfluxDB developers plan to implement a whole bunch of processing functions (akin to graphite, except they can do locality optimizations) and add a dashboard that talks to InfluxDB natively (or use <a href="http://grafana.org/">Grafana</a>), which means at some point you could completely swap graphite for InfluxDB.</p>

<p>However, I think for quite a while, the ability to use the Graphite api, combine backends, and use various graphite dashboards is still very useful.</p>

<p>So here&rsquo;s how my setup currently works:</p>

<ul>

<li>

<a href="https://github.com/graphite-ng/carbon-relay-ng">carbon-relay-ng</a> is a carbon relay in Go.  

It's a pretty nifty program to partition and manage carbon metrics streams.  I use it in front of our traditional graphite system, and have it stream - in realtime - a copy of a subset of our metrics into InfluxDB.  This way I basically have our unaltered Graphite system, and in parallel to it, InfluxDB, containing a subset of the same data.

<br/>With a bit more work it will be a high performance alternative to the python carbon relay, allowing you to manage your streams on the fly.

It doesn't support consistent hashing, because CH should be part of a strategy of a highly available storage system (see requirements above), using CH in the relay still results in a poor storage system, so there's no need for it.

</li>

<li>I contributed the code to InfluxDB to make it listen on the carbon protocol.  So basically, for the purpose of ingestion, InfluxDB can look and act just like a graphite server.  Anything that can write to graphite, can now write to InfluxDB.  (assuming the plain-text protocol, it doesn't support the pickle protocol, which I think is a thing to avoid anyway because almost nothing supports it and you can't debug what's going on)</li>

<li><a href="https://github.com/brutasse/graphite-api">graphite-api</a> is a fork/clone of graphite-web, stripped of needless dependencies, stripped of the composer.  It's conceived for many of the same reasons behind <a href="http://dieter.plaetinck.be/graphite-ng_a-next-gen-graphite-server-in-go.html">graphite-ng</a> (graphite technical debt, slow development pace, etc) though it doesn't go to such extreme lengths and for now focuses on being a robust alternative for the graphite server, api-compatible, trivial to install and with a faster pace of development.

</li>

<li>

That's where <a href="https://github.com/vimeo/graphite-influxdb">graphite-influxdb</a> comes in.  It hooks InfluxDB into graphite-api, so that you can query the graphite api, but using data in InfluxDB.

It should also work with the regular graphite, though I've never tried.  (I have no incentive to bother with that, because I don't use the composer.  And I think it makes more sense to move the composer into a separate project anyway).

</li>

</ul>

<p>With all these parts in place, I can run our dashboards next to each other - one running on graphite with whisper, one on graphite-api with InfluxDB - and simply look whether the returned data matches up, and which dashboards loads graphs faster.</p>

<p>Later i might do more extensive benchmarking and acceptance testing.</p>

<p><br/></p>

<p><br/></p>

<p>If all goes well, I can make carbon-relay-ng fully mirror all data, make graphite-api/InfluxDB the primary, and turn our old graphite box into a live &ldquo;backup&rdquo;.</p>

<p>We&rsquo;ll need to come up with something for rollups and deletions of old data (although it looks like by itself influx is already more storage efficient than whisper too), and I&rsquo;m really looking forward to the InfluxDB team building out the function api, having the same function api available for historical querying as well as realtime pub-sub.  (my goal used to be implementing this in graphite-ng and/or carbon-relay-ng, but if they do this well, I might just abandon graphite-ng)</p>

<p><br/></p>

<p><br/>To be continued..</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
                <a href="http://localhost:1313//tags/golang/index.html" rel="tag">golang</a>
            
                <a href="http://localhost:1313//tags/perf/index.html" rel="tag">perf</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/metrics-2-0-own-website/">Metrics 2.0 now has its own website!</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/metrics-2-0-own-website/" rel="bookmark">
					<time class="entry-date" datetime="2014-04-23 09:10:32 -0400 EDT">
						April 23, 2014
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>

from the website:

</p>

<pre><code>  &lt;p&gt;

  We have pretty good storage of timeseries data, collection agents, and dashboards.  But the idea of giving timeseries a &quot;name&quot; or a

  &quot;key&quot; is profoundly limiting us.  Especially when they're not standardized and missing information.

  &lt;br/&gt;Metrics 2.0 aims for &lt;b&gt;self-describing&lt;/b&gt;, &lt;b&gt;standardized&lt;/b&gt; metrics using &lt;b&gt;orthogonal tags&lt;/b&gt; for every dimension.

  &quot;metrics&quot; being the pieces of information that point to, and describe timeseries of data.

  &lt;/p&gt;By adopting metrics 2.0 you can:

  &lt;ul&gt;

    &lt;li&gt;increase compatibility between tools&lt;/li&gt;

    &lt;li&gt;get immediate understanding of metrics&lt;/li&gt;

    &lt;li&gt;build graphs, plots, dashboards and alerting expressions with minimal hassle&lt;/li&gt;

  &lt;/ul&gt;
</code></pre>

<p>Read more on <a href="http://metrics20.org/">metrics20.org</a>.</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/introduction_talk_to_metrics2-0_and_graph_explorer/">Introduction talk to metrics 2.0 and Graph-Explorer</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/introduction_talk_to_metrics2-0_and_graph_explorer/" rel="bookmark">
					<time class="entry-date" datetime="2014-02-23 16:20:32 -0400 -0400">
						February 23, 2014
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p><a href="http://vimeo.github.io/graph-explorer/">Graph-Explorer</a> at the</p>

<p><a href="http://www.meetup.com/Full-Stack-Engineering-Meetup/">Full-stack engineering meetup</a>.</p>

<p><br/></p>

<p>I could easily talk for hours about this stuff but the talk had to be about 20 minutes so I paraphrased it to be only</p>

<p>about the basic concepts and ideas and some practical use cases and features.  I think it serves as a pretty good introduction</p>

<p>and a showcase of the most commonly used features (graph composition, aggregations and unit conversion), and some new stuff such as alerting and dashboards.</p>

<p>

The talk also briefly covers native metrics 2.0 through your metrics pipeline using <a href="https://github.com/vimeo/statsdaemon">statsdaemon</a> and <a href="https://github.com/vimeo/carbon-tagger">carbon-tagger</a>.  I'm psyched that by formatting metrics at the source a little better and having an aggregation daemon that expresses the performed operations by updating the metric tags, all the foundations are in place for some truly next-gen UI's and applications (one of them already being implemented: graph-explorer can pretty much generate all graphs I need by just phrasing an information need as a proper query)

</p>

<p>

The <a href="http://vimeo.com/87194301">video</a> and <a href="https://www.slideshare.net/Dieterbe/metrics2-0graphexplorer20140218" title="Metrics 2.0 &amp; Graph-Explorer" target="_blank">slides</a> are available and also embedded below.

</p>

<iframe src="//player.vimeo.com/video/87194301?title=0&amp;byline=0&amp;portrait=0&amp;color=33a352"

width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

<iframe src="http://www.slideshare.net/slideshow/embed_code/31440297" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>

<p>

I would love to do another talk that allows me to dive into more of the underlying ideas, the benefits of metrics2.0 for things like metric storage systems, graph renderers, anomaly detection, dashboards, etc.

<br/>

<br/>

Hope you like it!

</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/metrics_2_a_proposal/">Metrics 2.0: a proposal</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/metrics_2_a_proposal/" rel="bookmark">
					<time class="entry-date" datetime="2013-09-14 11:29:32 -0400 EDT">
						September 14, 2013
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<ul>

<li>

Graphite's metrics are strings comprised of dot-separated nodes which, due to their ordering, can be represented as a tree.

Many other places use a similar format (stats in /proc etc).

</li>

<li>

OpenTSDB's metrics are shorter, because they move some of the dimensions (server, etc) into key-value tags.

</li>

</ul>

<p><b>I think we can do better&hellip;</b></p>

<p><br/></p>

<p>I think our metrics format is restrictive and we do our self a disservice using it:</p>

<ul>

<li>the metrics are not fully self-describing (they need additional information to become useful for interpretation, e.g. a unit, and information about what they mean)</li>

<li>they impose needless limitations (no dots allowed, strict ordering)</li>

<li>aggregators have no way to systematically and consistently express the operation they performed</li>

<li>we can't include additional information because that would create a new metric ID</li>

<li>we run into scale/display/correctness problems because storage rollups, rendering consolidation,

as well as aggregation api functions need to be aware of how the metric

is to be aggregated, and this information is often lacking or incorrect.</li>

<li>there's no consistency into what information goes where or how it's called (which field is the one that has the hostname?)</li>

</ul>

<p><b>We can solve all of this.  In an elegant way, even!</b></p>

<p></p></p>

<p>

Over the past year, I've been working a lot on a graphite dashboard (<a href="http://vimeo.github.io/graph-explorer/">Graph-Explorer</a>).

It's fairly unique in the sense that it leverages what I call "structured metrics" to facilitate a powerful way to

build graphs dynamically.  (see also

<a href="http://dieter.plaetinck.be/a_few_common_graphite_problems_and_how_they_are_already_solved.html">

"a few common graphite problems and how they are already solved"</a> and the

<a href="http://vimeo.github.io/graph-explorer/">Graph-Explorer homepage</a>).

I implemented two solutions. (

<a href="https://github.com/vimeo/graph-explorer/tree/master/structured_metrics">structured_metrics</a>,

<a href="https://github.com/vimeo/carbon-tagger">carbon-tagger</a>) for creating, transporting and using these metrics

on which dashboards like Graph-Explorer can rely.

The former converts graphite metrics into a set of key-value pairs (this is "after the fact" so a little annoying),

carbon-tagger uses a prototype extended carbon (graphite) protocol (called "proto2") to maintain an elasticsearch tags database on the fly, but

the format proved too restrictive.

</p>

<p>

Now for <a href="https://github.com/graphite-ng/graphite-ng">Graphite-NG</a> I wanted to rethink the metric, taking the simplicity of Graphite,

the ideas of structured_metrics and carbon_tagger (but less annoying) and OpenTSDB (but more powerful), and propose a new way

to identify and use metrics and organize tags around them.

</p>

<p><p></p>

<p>The proposed ingestion (carbon etc) protocol looks like this:</p>

<pre><![CDATA[

<intrinsic_tags>  <extrinsic_tags> <value> <timestamp>

]]></pre>

<ul>

<li>

intrinsic_tags and extrinsic_tags are space-separated strings, those strings can be a regular value, or a key=value pair and they describe one

dimension (aspect) of the metric.

    <ul>

    <li>an <b>intrinsic</b> tag contributes to the identity of the metric. If this section changes, we get a new metric</li>

    <li>an <b>extrinsic</b> tag provides additional information about the metric.  changes in this set doesn't change the metric identity</li>

    </ul>

<pre><code>Internally, the metric ID is nothing more than the string of intrinsic tags you provided (similar to Graphite style).

When defining a new metric, write down the intrinsic tags/nodes (like you would do with Graphite except you can use &lt;b&gt;any order&lt;/b&gt;), and then keep 'em in that order

(to keep the same ID).  &lt;b&gt;The ordering does not affect your ability to work with the metrics in any way&lt;/b&gt;.

The backend indexes the tags and you'll usually rely on those to work with the metrics, and rarely with the ID itself.
</code></pre>

<p></li></p>

<p><li></p>

<p>A metric in its basic form (without extrinsic tags) would look like:</p>

<pre><![CDATA[

graphite: stats.gauges.db15.mysql.bytes_received

opentsdb: mysql.bytes_received host=db15

proposal: service=mysql server=db15 direction=in unit=B

]]></pre>

<p></li></p>

<p><li></p>

<p>key=val tags are most useful: the key is a name of the aspect, which allows you to express &ldquo;I want to see my metrics averaged/summed/.. by this</p>

<p>aspect, in different graphs based on another aspect, etc&rdquo;</p>

<p>(<a href="https://github.com/vimeo/graph-explorer/wiki/GEQL#special-statements">e.g. GEQL statements</a>),</p>

<p>and because you can use them for filtering,</p>

<p>so I highly recommend to take some time to come up with a good key for every tag.</p>

<p>However sometimes it can be hard to come up with a concise, but descriptive key for a tag, hence they are not mandatory.</p>

<p>For regular words without a key, the backend will assign dummy keys (&lsquo;n1&rsquo;, &lsquo;n2&rsquo;, etc) to facilitate those features without hassle.</p>

<p>Note the two spaces between intrinsic and extrinsic.</p>

<p>With extrinsic tags the example could look like:</p>

<pre>

service=mysql server=db15 direction=in unit=B  src=diamond processed_by_statsd env=prod

</pre>

<p>to mean that this metric came from diamond, went through statsd, and that the machine is currently in the prod environment</p>

<p></li></p>

<p><li>Tags can contain any character except whitespace and null.</p>

<p>Specifically: dots (great for metrics that contain an ip, histogram bin, a percentile, etc) and slashes (unit can be &lsquo;B/s&rsquo; too)</p>

<p></li></p>

<p><li>the unit tag is mandatory. It allows dashboards to show the proper label on the Y-axis, and to do conversions</p>

<p>(for example in Graph Explorer, if your metric is an amount of B used on a disk drive, and you request the increase in GB per day.</p>

<p>it will automatically convert (and derive) the data).</p>

<p>We should aim for standardization of units.  I maintain a table of</p>

<p><a href="https://github.com/vimeo/graph-explorer/wiki/Units-&-Prefixes">standardized units &amp; prefixes</a> which uses SI and IEC as starting point,</p>

<p>and extends it with units commonly used in IT.</li></p>

<p></ul></p>

<h3>Further thoughts/comparisons</h3>

<h4>General</h4>

<ul>

<li>

The concept of extrinsic tags is something I haven't seen before, but I think it makes a lot of sense because we often want to pass extra information

but we couldn't because it would create a new metric.  It also makes the metrics more self-describing.</li>

<li>Sometimes it makes sense for tags only to apply to a metric in a certain time frame.  For intrinsic tags this is already the case by design,

for extrinsic tags the database could maintain a from/to timestamp based on when it saw the tag for the given metric</li>

<li>metric finding: besides the obvious left-to-right auto-complete and pattern matching (which allows searching for substrings in any order), we can now also build

an interface that uses <a href="http://www.elasticsearch.org/guide/reference/api/search/facets/">facet searches</a>

to suggest/auto-complete tags, and filter by toggling on/off tags.

</li>

<li>Daemons that sit in the pipeline and yield aggregated/computed metrics can do this in a much more useful way.  For example a statsd daemon

that computes a rate per second for metrics with 'unit=B' can yield a metric with 'unit=B/s'.</li>

<li>

    We can <a href="https://github.com/vimeo/graph-explorer/wiki/Consistent-tag-keys-and-values">standardize tag keys and values</a>, other than just the unit tag.

    Beyond the obvious compatibility benefits between tools, imagine querying for:

    <ul>

    <li>'unit=(Err|Warn)' and getting all errors and warnings across the entire infrastructure (no matter which tool generated the metric), and grouping/drilling down by tags</li>

    <li>'$hostname direction=in' and seeing everything coming in to the server, network traffic on the NIC, as well as files being uploaded.</li>

    </ul>

<pre><code>Also, metrics that are summary statistics (i.e. with statsd) will get intrinsic tags like 'stat=median' or 'stat=upper_90'.

This has three fantastic consequences:

&lt;ul&gt;

&lt;li&gt;aggregation (rollups from higher to lower resolution) knows what to do without configurating aggregation functions, because it can be deduced from the metric itself&lt;/li&gt;

&lt;li&gt;renderers that have to render &gt;1 datapoints per pixel, will produce more accurate, relevant graphs because they can deduce what the metric is meant to represent&lt;/li&gt;

&lt;li&gt;API functions such as &quot;cumulative&quot;, &quot;summarize&quot; and &quot;smartSummarize&quot; don't need to be configured with an explicit aggregation function&lt;/li&gt;

&lt;/ul&gt;
</code></pre>

<p></li></p>

<p></ul></p>

<h4>From the Graphite perspective, specifically</h4>

<ul>

<li>dots and slashes are allowed</li>

<li>The tree model is slow to navigate, sometimes hard to find your metrics, and makes it really hard to write

queries (target statements) that need metrics in different locations of the tree (because the only way to retrieve multiple metrics in a target is wildcards)</li>

<li>The tree model causes people to obsess over node ordering to find the optimal tree organization, but no ordering allows all query use cases anyway,

so you'll be limited no matter how much time you spend organising metrics.</li>

<li><i>We do away with the tree entirely.  A multi-dimensional tag database is way more powerful</i>and allows for great "metric finding" (see above)</li>

<li>Graphite has no tags support</li>

<li>you don't need to configure aggregation functions anymore, less room for errors ("help, my scale is wrong when i zoom out"), better rendering</li>

<li>when using statsd, you don't need prefixes like "stats.".  In fact that whole prefix/postfix/namespacing thing becomes moot</li>

</ul>

<h4>From the OpenTSDB perspective, specifically</h4>

<ul>

<li>allow dots anywhere</li>

<li>'http.hits' becomes 'http unit=Req' (or 'unit=Req http', as long as you pick one and stick with it)</li>

<li>probably more, I'm not very familiar with it</li>

</ul>

<h4>From the structured_metrics/carbon-tagger perspective, specifically</h4>

<ul>

<li>not every tag requires a key (on metric input), but you still get the same benefits</li>

<li>You're not forced to order the tags in any way</li>

<li>sometimes relying on the 'plugin' tag is convenient but it's not really intrinsic to the metric, now we can use it as extrinsic tag</li>

</li>

</ul>

<h3>Backwards compatibility</h3>

<!-- These changes may seem a little invasive, but it's actually not that bad: TODO is that so?-->

<p><ul></p>

<p><li>Sometimes you merely want the ability to copy a &ldquo;metric id&rdquo; from the app source code, and paste it in a &ldquo;/render/?target=&rdquo; url to see that particular metric.</p>

<p>You can still do this: copy the intrinsic tags string and you have the id.</li></p>

<p><li>if you wanted to, you could easily stay compatible with the official graphite protocol:</p>

<p>for incoming metrics, add a &lsquo;unit=Unknown&rsquo; tag and optionally turn the dots into</p>

<p>spaces (so that every node becomes a tag), so you can mix old-style and new style metrics in the same system.</p>

<p></li></p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/graphite-ng_a-next-gen-graphite-server-in-go/">Graphite-ng: A next-gen graphite server in Go.</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/graphite-ng_a-next-gen-graphite-server-in-go/" rel="bookmark">
					<time class="entry-date" datetime="2013-09-07 20:54:20 -0400 EDT">
						September 7, 2013
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>I&rsquo;ve been a <a href="https://github.com/graphite-project/">graphite</a> contributor for a while (and still am).  It&rsquo;s a <i>great</i> tool for timeseries metrics.</p>

<p>Two weeks ago I started working on <a href="https://github.com/graphite-ng/graphite-ng">Graphite-ng</a>:</p>

<p>it&rsquo;s somewhere between an early clone/rewrite, a redesign, and an experiment playground, written in <a href="http://golang.org">Golang</a>.</p>

<p>The focus of my work so far is the API web server, which is a functioning prototype, it answers requests like</p></p>

<p><div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span style="color: #666666">&lt;!</span>[CDATA[

<span style="color: #BB6688">/render/</span><span style="color: #666666">?</span>target<span style="color: #666666">=</span>sum(scale(stats.web2,<span style="color: #666666">5.12</span>),derivative(stats.web2))

]]<span style="color: #666666">&gt;</span>
</pre></div>
</p>

<p>

I.e. it lets you retrieve your timeseries, processed by function pipelines which are setup on the fly based on a spec in your http/rest arguments.

Currently it only fetches metrics from text files but I'm working on decent metrics storage as well.

</p>

<p>

There's a few reasons why I decided to start a new project from scratch:

<ul>

<li>With graphite, it's a whole ordeal to get all components properly installed.  Deploying in production environments is annoying and even more so

when you just want a graphite setup on your personal netbook.</li>

<li>the graphite development process slows contributors down a lot.  A group of 3rd/4th generation maintainers jointly manages the project, but it's hard to get big changes through,

because they (understandably) don't feel authoritative enough to judge those changes and the predecessors have disappeared or are too busy with other things.  And also ...</li>

<li>there's a high focus on backwards compatibility, which can be a good thing, but it's hard to get rid of old design mistakes,

especially when fixing unclear (but arguably broken) early design decisions (or oversights) lead to different outputs</li>

<li>Graphite suffers feature creep: it has an events system, a PNG renderer, an elaborate composer web UI, etc.

There's a lot of internal code dependencies holding you back from focusing on a specific problem</li>

<li>Carbon (the metrics daemon) has a pretty hard performance and scalability ceiling.

<a href="https://github.com/pcn/carbon/blob/new-sending-mechanism/Why_Spooling.md">Peter's article explains this well</a>; I think we'll need some form of

rewrite.   Peter suggests some solutions but they are basically workarounds for Python's shortcomings.  I'm also thinking of using <a href="http://pypy.org/">pypy</a>.

But last time I checked pypy just wasn't there yet.</li>

<li>I want to become a good Go programmer</li>

</ul>



<i>Note: the Graphite project is still great</i>, the people managing do good work, but it's fairly natural for a code base that large and complicated

to end up in this situation.<i>I'm not at all claiming graphite-ng is, or ever will be better</i> but I need a fresh start to try some disruptive ideas,

using Go means having a runtime very suited for concurrency and parallelism, you can compile the whole thing down into a single executable file,

and its performance looks promising.  Leaving out the non-essentials (see below) allows for an elegant and surprisingly small, hackable code base.

</p>

<p>

The API server I developed sets up a processing pipeline as directed by your query: every processing function runs in a goroutine

for concurrency and the metrics flow through using Go channels.  It literally compiles a program and executes it.  You can add your own functions

to collect, process, and return metrics by writing simple plugins.

<br/>As for timeseries storage, for now it uses simple text files,

but I'm experimenting and thinking what would be the best metric store(s) that works on small scale

(personal netbook install) to large scale ("I have millions of metrics that need to be distributed across nodes,

the system should be HA and self-healing in failure scenarios, easily maintainable, and highly performant") and is still easy to deploy, configure and run.

Candidates are <a href="https://github.com/kisielk/whisper-go">whisper-go</a>, <a href="https://code.google.com/p/kairosdb/">kairosdb</a>,

my own <a href="https://github.com/graphite-ng/graphite-ng/tree/master/carbon-es">elasticsearch experiment</a> etc.

<br/>I won't implement rendering images, because I think client-side rendering using something like <a href="https://github.com/vimeo/timeserieswidget">timeserieswidget</a>

is superior.  I can also leave out events because <a href="https://github.com/Dieterbe/anthracite/">anthracite</a> already does that.

There's a ton of dashboards out there (<a href="http://vimeo.github.io/graph-explorer/">graph-explorer</a>, <a href="https://github.com/obfuscurity/descartes">descartes</a>, <a href="http://graphite.readthedocs.org/en/1.0/tools.html">etc</a>) so that can be left out as well.

</p>

<p>For more information, see <a href="https://github.com/graphite-ng/graphite-ng">the Graphite-ng homepage</a>.</p>

<p><br/></p>

<p><br/>PS: props to <a href="http://felixge.de/">Felix Geisendorfer</a> who suggested a graphite clone in Go first,</p>

<p>it seemed like a huge undertaking but the right thing to do, I had some time so I went for it!</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
                <a href="http://localhost:1313//tags/golang/index.html" rel="tag">golang</a>
            
                <a href="http://localhost:1313//tags/perf/index.html" rel="tag">perf</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/a_few_common_graphite_problems_and_how_they_are_already_solved/">A few common graphite problems and how they are already solved.</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/a_few_common_graphite_problems_and_how_they_are_already_solved/" rel="bookmark">
					<time class="entry-date" datetime="2013-04-04 08:54:20 -0400 EDT">
						April 4, 2013
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<h4>metrics often seem to lack details, such as units and metric types</h4>

<p>looking at a metric name, it&rsquo;s often hard to know</p>

<ul>

<li>the unit a metric is measured in (bits, queries per second, jiffies, etc)</li>

<li>the "type" (a rate, an ever increasing counter, gauge, etc)</li>

<li>the scale/prefix (absolute, relative, percentage, mega, milli, etc)</li>

</ul>

<p><a href="#structured_metrics">structured_metrics</a> solves this by adding these tags to graphite metrics:</p>

<ul>

<li><pre>what</pre>

<i>what is being measured?: bytes, queries, timeouts, jobs, etc</i></li>

<li><pre>target_type</pre>

<i>must be one of the existing <a href="https://github.com/vimeo/graph-explorer#enhanced-metrics">clearly defined target_types</a> (count, rate, counter, gauge)

<br/>These match statsd metric types (i.e. rate is per second, count is per flushInterval)</i></li>

</ul>

<p>In <a href="#graph_explorer">Graph-Explorer</a> these tags are mandatory, so that it can show the unit along with the prefix (i.e. <i>&lsquo;Gb/s&rsquo;</i>) on the axis.</p>

<p><br/></p>

<p>This will also allow you to request graphs in a different unit and the dashboard will know how to convert (say, Mbps to GB/day)</p>

<h4>tree navigation/querying is cumbersome, metrics search is broken.  How do I organize the tree anyway?</h4>

<p>the tree is a simplistic model. There is simply too much dimensionality that can&rsquo;t be expressed in a flat tree.</p>

<p>There&rsquo;s no way you can organize it so that will it satisfy all later needs.</p>

<p>A tag space like <a href="#structured_metrics">structured_metrics</a> makes it <b>obsolete</b>.</p>

<p>with <a href="#graph_explorer">Graph-Explorer</a> you can do (full-text) search on metric name, by any of their tags, and/or by added metadata.</p>

<p>So practically you can filter by things like server, service, unit (e.g. anything expressed in bits/bytes per second,</p>

<p>or anything denoting errors).  All this irrespective of the source of a metric or the &ldquo;location in the tree&rdquo;.</p>

<h4>no interactivity with graphs</h4>

<p><a href="#timeserieswidget">timeserieswidget</a> allows you to easily add interactive graphite graph objects to html pages.</p>

<p>You get modern features like togglable/reorderable metrics, realtime switching between lines/stacked,</p>

<p>information popups on hoover, highlighting, smoothing, and (WIP) realtime zooming.</p>

<p>It has a canvas (<a href="http://www.flotcharts.org/">flot</a>) and svg (<a href="http://code.shutterstock.com/rickshaw/">rickshaw/d3</a>) backend.</p>

<p>So it basically provides a simpler api to use these libraries specifically with graphite.</p>

<p><br/>There&rsquo;s a bunch of different graphite dashboards with different takes on graph composition/configuration and workflow, but the actual rendering of graphs</p>

<p>usually comes down to plotting some graphite targets with a legend.  timeserieswidget aims to be a drop-in plugin that brings all modern features</p>

<p>so that different dashboards can benefit from a common, shared codebase, because <b>static PNGs are a thing from the past</b></p>

<p>

screenshot:

<br/>

<a href="/files/timeserieswidget-rickshaw-stacked.png"><img height="300" src="/files/blog/github/vimeo/timeserieswidget/timeserieswidget-rickshaw-stacked.png" title="yes I'm aware of the irony of a static screenshot of an interactive widget :)"></img></a>

</p>

<h4>events lack text annotations, they are simplistic and badly supported</h4>

<p>Graphite is a great system for time series metrics.  Not for events.  metrics and events are very different things across the board.  drawAsInFinite() is a bit of a hack.</p>

<ul>

<li><a href="#anthracite">anthracite</a> is designed specifically to manage events.

<br/>It brings extra features such as different submission scripts, outage annotations, various ways to see events and reports with uptime/MTTR/etc metrics.</li>

<li><a href="#timeserieswidget">timeserieswidget</a> displays your events on graphs along with their metadata (which can be just some text or even html code).

<br/>this is where client side rendering <i>shines</i></li>

</ul>

<p>

screenshots:

<br/>

<a href="https://raw.github.com/Dieterbe/anthracite/master/screenshots/screenshot-table.png"><img height="300" src="/files/blog/github/Dieterbe/anthracite/screenshot-table.png"></img></a>

<a href="https://raw.github.com/vimeo/timeserieswidget/master/screenshots/flot-annotated-event.png"><img height="300" src="/files/blog/github/vimeo/timeserieswidget/flot-annotated-event.png"></img></a>

</p>

<h4>cumbersome to compose graphs</h4>

<p>There&rsquo;s basically two approaches:</p>

<ul>

<li>interactive composing: with the graphite composer, you navigate through the tree and apply functions.

This is painfull, dashboards like <a href="https://github.com/obfuscurity/descartes">descartes</a> and

<a href="https://github.com/paperlesspost/graphiti">graphiti</a> can make this easier</li>

<li>use a dashboard that uses predefined templates (<a href="https://github.com/ripienaar/gdash">gdash</a> and others)

They often impose a strict navigation path to reach pages which may or may not give you the information you need (usually less or way more)</li>

</ul>

<p>With both approaches, you usually end up with an ever growing pile of graphs that you created and then keep for reference.</p>

<p><br/>This becomes unwieldy but is useful for various use cases and needs.</p>

<p><br/>However, <i>neither approach is convenient for changing information needs</i>.</p>

<p><br/>Especially when troubleshooting, one day you might want to compare the rate of increase of open file handles on a set of specific servers to the traffic</p>

<p>on given network switches, the next day it&rsquo;s something completely different.</p>

<p><br/>With <a href="#graph_explorer">Graph-Explorer</a>:</p>

<ul>

<li>GE gives you a query interface on top of <a href="#structure_metrics">structured_metric</a>'s tag space.  this enables a bunch of things (see above)</li>

<li>you can yield arbitrary targets for each metric, to look at the same thing from a different angle (i.e. as a rate with `derivative()` or as a daily summary),

and you can of course filter by angle</li>

<li><b>You can group metrics into graphs by arbitrary tags</b> (e.g. you can see bytes used of all filesystems on a graph per server, or <i>compare servers on a graph per filesystem</i>).

<b>This feature always results in the "wow that's really cool" every time I show it</b></li>

<li>GE includes 'what' and 'target_type' in the group_by tags by default so basically, if things are in a different unit (B/s vs B vs b etc) it'll put them in

separate graphs (controllable in query)</li>

<li>GE automatically generates the graph title and vertical title (always showing the 'what' and the unit), and shows all metrics' extra tags.

This also gives you a lot of inspiration to modify or extend your query</li>

</ul>

<p>

<a href="/files/blog/github/vimeo/graph-explorer/screenshot.png">

<img src="/files/blog/github/vimeo/graph-explorer/screenshot.png" width="619" height="396"/>

</a>

</p>

<h4>limited options to request a specific time range</h4>

<p><a href="https://github.com/vimeo/graph-explorer#query-parsing-and-execution">GE&rsquo;s query language</a> supports freeform <code>from</code> and <code>to</code> clauses.</p>

<h3>Referenced projects</h3>

<ul>

<li>

<a href="https://github.com/Dieterbe/anthracite" id="anthracite">anthracite</a>:

<br/>

event/change logging/management with a bunch of ingestion scripts and outage reports

</li>

<li>

<a href="https://github.com/vimeo/timeserieswidget" id="timeserieswidget">timeserieswidget</a>:

<br/>

jquery plugin to easily get highly interactive graphite graphs onto html pages (dashboards)

</li>

<li>

<a href="https://github.com/vimeo/graph-explorer/tree/master/structured_metrics" id="structured_metrics">structured_metrics</a>:

<br/>

python library to convert graphite metrics tree into a tag space with clearly defined units and target types, and arbitrary metadata.

</li>

<li>

<a href="https://github.com/vimeo/graph-explorer" id="graph_explorer">graph-explorer</a>:

<br/>

dashboard that provides a query language so you can easily compose graphs on the fly to satisfy varying information needs.

</li>

</ul>

<p>All tools are designed for integration with other tools and each other.</p>

<p>Timeserieswidget gets data from anthracite, graphite and elasticsearch.</p>

<p>Graph-Explorer uses structured_metrics and timeserieswidget.</p>

<h3>Future work</h3>

<p>There&rsquo;s a whole lot going on in the monitoring space, but I&rsquo;d like to highlight a few things I personally want to work more on:</p>

<ul>

<li>

I spoke with Michael Leinartas at Monitorama (and there's also a <a href="https://answers.launchpad.net/graphite/+question/223956">launchpad thread</a>).

We agreed that native tags in graphite are the way forward.  This will address some of the pain points

I'm already fixing with structured_metrics but in a more native way.

I envision submitting metrics would move from:

<pre>

stats.serverdb123.mysql.queries.selects 895 1234567890

</pre>

to something more along these lines:

<pre>

host=serverdb123 service=mysql type=select what=queries target_type=rate 895 1234567890

host=serverdb123 service=mysql type=select unit=Queries/s 895 1234567890

h=serverdb123 s=mysql t=select queries r 895 1234567890

</pre>

</li>

<li>switch Anthracite backend to ElasticSearch for native integration with logstash data (and allow you to use kibana)</li>

</ul>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/hi_planet_devops_and_infratalk/">Hi Planet Devops and Infratalk</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/hi_planet_devops_and_infratalk/" rel="bookmark">
					<time class="entry-date" datetime="2013-03-24 11:36:20 -0400 EDT">
						March 24, 2013
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>so for my new readers: you might know me as Dieterbe on irc, <a href="https://github.com/Dieterbe">github</a> or <a href="https://twitter.com/Dieter_be">twitter</a>.</p>

<p>Since my <a href="/moving-to-nyc.html">move from Belgium to NYC</a> (to do backend stuff at Vimeo) I&rsquo;ve started writing more about devops-y topics</p>

<p>(whereas I used to write more about general hacking and</p>

<p><a href="/tag/arch">arch linux release engineering and (automated) installations</a>).</p>

<p>I&rsquo;ll mention some earlier posts you might be interested in:</p>

<ul>

<li><a href="/profiling_and_behavior_testing_processes_daemons_devopsdays_nyc.html">Profiling and behavior testing of processes and daemons, and Devopsdays NYC</a></li> 

<li><a href="/graph-explorer-a-graphite-dashboard-unlike-any-other.html">Graph-Explorer: A graphite dashboard unlike any other</a></li>

<li><a href="/client-side-rendered-graphite-charts-for-all.html">Client-side rendered, highly interactive graphite charts for all</li>

<li><a href="/dell_crowbar_openstack_swift.html">Dell crowbar openstack swift</a></li>

<li><a href="/anthracite-event-database-enrich-monitoring-dashboards-visual-numerical-analysis-events-business-impact.html">

Anthracite, an event/change management database</a> to track what has a business and/or operational impact and show annotated events on dashboards</li>

<li><a href="/histogram-statsd-graphing-over-time-with-graphite.html">Histograms in statsd, and graphing them over time with graphite</a></li>

<li><a href="/stop-abusing-si-prefixes.html">stop abusing SI prefixes</a> cause 1000 &ne; 1024</li>

<li><a href="/what_the_open_source_community_can_learn_from_devops.html">What the open source community can learn from devops</a></li>

</ul>

<p>FWIW, I&rsquo;m attending <a href="http://monitorama.com">Monitorama</a> next weekend in Boston.</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/profiling_and_behavior_testing_processes_daemons_devopsdays_nyc/">Profiling and behavior testing of processes and daemons, and Devopsdays NYC</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/profiling_and_behavior_testing_processes_daemons_devopsdays_nyc/" rel="bookmark">
					<time class="entry-date" datetime="2013-01-21 15:25:14 -0400 -0400">
						January 21, 2013
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p><a href="https://github.com/Dieterbe/profile-process"></p>

<p><img style="float:left;margin:0 5px 0 0;" src="/files/presentation-simple-black-box/images/profile_io.png" width="100px" height="100px"/></p>

<p></a></p>

<p>I wanted the ability to run a given process and get</p>

<p><br/>a plot of key metrics (cpu usage, memory usage, disk i/o) throughout the duration of the process run.</p>

<p><br/>Something light-weight with minimal dependencies so I can easily install it on a server for a one-time need.</p>

<p><br/>Couldn&rsquo;t find a tool for it, so I wrote <a href="https://github.com/Dieterbe/profile-process">profile-process</a></p>

<p><br/>which does exactly that in &lt;100 lines of python.</p>

<p><br/></p>

<p><br/></p>

<h3>black-box behavior testing processes/daemons</h3>

<p><a href="https://github.com/vimeo/simple-black-box"></p>

<p><img style="float:left;margin:0 5px 0 0;" src="/files/presentation-simple-black-box/images/screenshot-no-pause.png" width="150px" height="100px"/></p>

<p></a></p>

<p>I wrote <a href="https://github.com/vimeo/simple-black-box">simple-black-box</a> to do this.</p>

<p><br/>It runs the subject(s) in a crafted sandbox, sends input (http requests, commands, &hellip;)</p>

<p><br/>and allows to make assertions on http/statsd requests/responses, network listening state, processes running, log entries,</p>

<p><br/>file existence/checksums in the VFS/swift clusters, etc.</p>

<p><br/>Each test-case is a scenario.</p>

<p><br/>It also can use <a href="http://logstash.net/">logstash</a> to give a centralized &ldquo;distributed stack trace&rdquo; when you need to debug a failure after</p>

<p>multiple processes interacting and acting upon received messages; or to compare behavior across different scenario runs.</p>

<p><br/>You can integrate this with profile-process to compare runtime behaviors across testcases/scenarios.</p>

<h3>Simple-black-box talk @ Devopsdays NYC</h3>

<p>I did a quick <a href="http://devopsdays.org/events/2012-newyork/proposals/SimpleBlackBox/">5min talk</a>, despite some display/timing issues it was well received.</p>

<p>(in particular I got some really positive feedback from one person and still wonder if that was a recruiter attempting to</p>

<p>hire me -but being shy about it&hellip;- it was quite awkward)</p>

<p><br/>&rarr; <a href="/files/presentation-simple-black-box/slideshow.html">slides</a></p>

<p><br/>&rarr; <a href="http://new.livestream.com/devopsdaysorg/nyc2013/videos/9609498">raw uncut video</a>.  Go to &lsquo;New York, January 18th, 2013&rsquo; from 02:36:25 to 02:41:15</p>

<h3>More random thoughts about Devopsdays NYC</h3>

<p><center><a href="http://devopsdays.org/events/2012-newyork/"><img src="/files/blog/devopsdays/workers.png" width="300px" height="300px"/></a></center></p>

<ul>

<li>I'm getting tired of people on stage making a big revelation out of adding an index to a database column.

This happens too often at web-ops/devops conferences,  it's embarrassing.

But at least it's not like the "how we made our site 1000x faster"-style Velocity talks that should have been named "caching and query optimization for newbies"

</li>

<li>Paperless post confirms again they got their act together and keeps us up to date with their great work.

<a href="http://dev.paperlesspost.com/">Follow them.</a></li>

<li>

<a href="http://devopsdays.org/events/2012-newyork/proposals/KnightsOftheProvisioningRoundTable/">Knights of the Provisioning Round Table - Bare Metal Provisioning</a>

was mostly (to my surprise) 4 individuals presenting their solution instead of a real round-table, but (to my surprise again) they were not as similar/repetitive as I expected

and the pros/cons of all solutions were compared more in depth than I dared to hope.

I <a href="/dell_crowbar_openstack_swift.html">covered dell crowbar before</a> and like it, though I wonder when this thing is actually gonna be reliable.

</li>

<li>Dave Zwieback and John Willis gave hilarious talks</li>

<li>Tried to start an open space discussion around <b>collaboration patterns and anti-patterns</b>, which I think is a very interesting subject,

  because <b>how individuals in a team collaborate is crucial to success</b>, but yet very little is written about it (that I could find).

  I would hope we can distill the years of aggregate experience of people into concise patterns and anti-patterns and document how (not) well they work

  for development styles (such as agile/scrum), team size, company structure/hierarchy, roadmap/technical debt pressure, etc.  And especially in light

  of any of these things changing, because I've found <b>people can be very change-resistive</b>.

</li>

<li><a href="http://devopsdays.org/events/2012-newyork/proposals/DevopsAtObamaForAmerica/">DevOps At Obama for America & The Democratic National Committee</a>

was good, I thought it would be a rehash of what was said at <a href="http://codeforward_newyorkcity.eventbrite.com/">Coding Forward New York City: Meet the Developers Behind the Obama Campaign</a>

but there were a bunch of interesting insights about state of the art technology in here (mostly Amazon stuff)</li>

<li>A bunch of talks where the same could have been said in half the time, or less</li>

</ul>

<p>Random thoughts about some sponsors:</p>

<ul>

<li>Librato is quite cool.

It's basically how my open source tool <a href="https://github.com/vimeo/graph-explorer">graph-explorer</a> would look like after

finishing a bunch of TODO's, combining it with graphite, polishing it all up, and offering it as a hosted solution.

I find it interesting if this is a successful business with only such a limited scope</li>

<li>

Even cooler is <a href="http://www.datadoghq.com/">datadog</a>.  It goes beyond just metrics and doesn't just provide hosted graphing,

it provides a solution for a philosophy that aims for a centralized insight of all your operational data,

related collaboration and prioritized alerts that are to the point.  They get a lot of things right, the open source world has some catching up to do</li>

</ul>

<p>Interesting that both use Cassandra and free-form tags for flexibility, validating the approach I&rsquo;m taking with graph-explorer.  Now Graphite</p>

<p>could use a distributed metrics storage backend over which one can do map-reduce style jobs to gather intelligence from metrics archives</p>

<p>(maybe based on Cassandra too?), but that&rsquo;s another story.</p>

<p><br/></p>

<p><br/></p>

<p>Anyway, living in NYC with its vibrant ecosystem of devops people and companies organizing plenty of meet-ups and talks on their own</p>

<p>makes it less pressing to have an event like Devopsdays, though it was certainly a good event, thanks to the sponsors and the volunteers.</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
                <a href="http://localhost:1313//tags/perf/index.html" rel="tag">perf</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/graph-explorer-a-graphite-dashboard-unlike-any-other/">Graph-Explorer: A graphite dashboard unlike any other</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/graph-explorer-a-graphite-dashboard-unlike-any-other/" rel="bookmark">
					<time class="entry-date" datetime="2013-01-09 09:25:36 -0400 -0400">
						January 9, 2013
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p><br/></p>

<p>

In web operations, mostly when troubleshooting but also for capacity planning,

I often find myself having very specific information needs from my time-series, and these information needs vary a lot over time.

This usually means I need to correlate or compare things that no one anticipated.  Things that relate to specific machines, specific services across machines,

or a few specific metrics of which only the ops team knows how they are related and cross various scopes (application, network, system, etc).

<br/>I should have an easy way to filter metrics by any information in the metric's name or values.

<br/>I should be able to group metrics into graphs the way I want. (example: when viewing filesystem usage of servers,

I should be able to group by server (one graph per server listing the filesystems, but also by mountpoint to compare servers on one graph).

<br/>I should be able -with minimal effort- to view metrics by their gauge/count value,

but also by their rate of change and where appropriate, as a percentage of a maximum (like diskspace used).

<br/>It should be trivial to manipulate the graph interactively (toggling things on/off, switching between lines/stacked mode, inspecting datapoints, zooming, ...).

<br/>It should show me all events, colorcoded by type, with text description, and interactive so that it can use hyperlinks.

<br/>And most of all, the code should be as simple as possible and it should be easy to get running.

</p>

<p><a href="https://github.com/vimeo/graph-explorer"><img width="50%" height="50%" src="/files/blog/github/vimeo/graph-explorer/screenshot.png"/></a></p>

<p>

Dashboards which show specific predefined KPI's (this covers most graphite dashboards) are clearly unsuitable for this use case.

Template-based "metric exploration" dashboards like cacti and ganglia are in my experience way too limited.

Graph composing dashboards (like the stock graphite one, or graphiti) require much manual work to get the graph you want.

I couldn't find anything even close to what I wanted, so I started <a href="https://github.com/vimeo/graph-explorer">Graph-Explorer</a>.

</p>

<p>

The approach I'm taking is using plugins which add metadata to metrics (tags for server, service, mountpoint, interface name, ...),

having them define how to render as a count, as a rate, as a percent of some max allowed value (or a metric containing the max),

and providing a query language which allow you to match/filter metrics, group them into graphs by tag, and render them how you want them.

The plugins promote standardized metric naming and reuse across organisations, not in the least because most correspond to plugins for the

<a href="https://github.com/BrightcoveOS/Diamond">Diamond monitoring agent</a>.

</p>

<p>Furthermore, because it uses <a href="/client-side-rendered-graphite-charts-for-all.html">my graphitejs plugin</a> (which now btw supports flot as a backend for

fast canvas-based graphs and annotated events from <a href="/anthracite-event-database-enrich-monitoring-dashboards-visual-numerical-analysis-events-business-impact.html">anthracite</a>)

 the manual interactions mentioned earlier are supported or at least on the roadmap.

</p>

<p><a href="https://github.com/vimeo/graph-explorer">Graph Explorer</a> is not yet where I want it, but it&rsquo;s already a very useful tool at Vimeo.</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/client-side-rendered-graphite-charts-for-all/">Client-side rendered graphite charts for all</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/client-side-rendered-graphite-charts-for-all/" rel="bookmark">
					<time class="entry-date" datetime="2012-11-14 08:49:56 -0400 -0400">
						November 14, 2012
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>allows various interactivity features, such as:</p>

<ul>

<li>interactive realtime zooming and panning of the graph, timeline sliders</li>

<li>realtime switching between various rendering modes (lines, stacked, etc)</li>

<li>toggling certain targets on/off, reordering them, highlighting their plot when hoovering over the legend, etc</li>

<li>basic data manipulation, such as smoothing to see how averages compare (akin to movingAverage, but now interactive)

<li>popups detailing the exact metrics when hoovering over the chart's datapoints</li>

<li>popups for annotated events. (a good use for <a href="/anthracite-event-database-enrich-monitoring-dashboards-visual-numerical-analysis-events-business-impact.html">anthracite</a>).

</ul>

<p>Those are all features of charting libraries such as <a href="http://www.flotcharts.org/">flot</a> and <a href="http://code.shutterstock.com/rickshaw/">rickshaw</a>,

the only remaining work is creating a library that unleashes the power of such a framework, integrates it with the graphite

api datasource, and makes it available over a simple but powerful api.

<br/>

<br/>That's what I'm trying to achieve with <a href="https://github.com/Dieterbe/graphitejs">github.com/Dieterbe/graphitejs</a>.

<br/>It's based on rickshaw.

<br/>It gives you a JavaScript api to which you specify your graphite targets and some options and it'll give you your graph with extra interactivity sauce.

Note that graphite has a good and rich api, one that is widely known and understood, that's why I decided to keep it exposed and not abstract any more than needed.

</p>

<p><p>There are many graphite dashboards with a different focus, but as far as</p>

<p>plotting graphs, what they need is usually very similar: a plot to draw multiple</p>

<p>graphite targets, a legend, an x-axis, 1 or 2 y-axis, lines or stacked bands, so</p>

<p>I think there&rsquo;s a lot of value in having many dashboard projects share the same code for the actual charts, and I hope we can work together on this.</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/anthracite-event-database-enrich-monitoring-dashboards-visual-numerical-analysis-events-business-impact/">Anthracite, an event database to enrich monitoring dashboards and to allow visual and numerical analysis of events that have a business impact</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/anthracite-event-database-enrich-monitoring-dashboards-visual-numerical-analysis-events-business-impact/" rel="bookmark">
					<time class="entry-date" datetime="2012-11-12 08:49:56 -0400 -0400">
						November 12, 2012
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>Graphite can show events such as <a href="http://codeascraft.etsy.com/2010/12/08/track-every-release/">code deploys</a> and

<a href="https://github.com/joemiller/puppet-graphite_event">puppet changes</a> as vertical markers on your graph.

With the advent of new graphite dashboards and interfaces where we can have popups and annotations to show metadata for each event (by means of client-side rendering),

it's time we have a database to track all events along with categorisation and text descriptions (which can include rich text and hyperlinks).

Graphite is meant for time series (metrics over time), Anthracite aims to be the companion for annotated events.<br>

More precisely, <strong>Anthracite aims to be a database of "relevant events"</strong> (see further down), <strong>for the purpose of enriching monitoring dashboards,

as well as allowing visual and numerical analysis of events that have a business impact</strong> (for the latter, see "<i>Thoughts on incident nomenclature, severity levels and incident analysis</i>" below)<br>

It has a TCP receiver, a database (sqlite3), a http interface to deliver event data in many formats and a simple web frontend for humans.</p>

<p>design goals:</p>

<ul>

    <li>do one thing and do it well.  aim for integration.</li>

    <li>take inspiration from graphite:

        <ul>

        <li>simple TCP protocol</li>

        <li>automatically create new event types as they are used</li>

        <li>run on port 2005 by default (carbon is 2003,2004)</li>

        <li>deliver events in various formats (html, raw, json, sqlite,...)</li>

        <li>stay out of the way</li>

        </ul>

<pre><code>&lt;/li&gt;

&lt;li&gt;super easy to install and run: install dependencies, clone repo. &lt;i&gt;the app is ready to run&lt;/i&gt;&lt;/li&gt;
</code></pre>

<p></ul></p>

<p>I have a working prototype on <a href="https://github.com/Dieterbe/anthracite">github.com/Dieterbe/anthracite</a></p>

<h4>About "relevant events"</h4>

<p>I recommend you submit any event that <strong>has</strong> or <strong>might have</strong> a <strong>relevant</strong> effect on:</p>

<ul>

    <li>your application behavior</li>

    <li>monitoring itself (for example you fixed a bug in metrics reporting. it shouldn't look like the app behavior changed)</li>

    <li>the business (press coverage, viral videos, etc), because this also affects your app usage and metrics.</li>

</ul>

<h4>Formats and conventions</h4>

<p>The TCP receiver listens for lines in this format:</p>

<pre><![CDATA[

<unix_timestamp> <type> <description>

]]></pre>

<p>There are no restrictions for type and description, other than that they must be non-empty strings.

<br/>I do have some suggestions which I'll demonstrate through fictive examples;

<br/>but note that there's room for improvement, see the section below)</p>

<pre><![CDATA[

# a deploy_* type for each project

ts deploy_vimeo.com "deploy e8e5e4 initiated by Nicolas -- github.com/Vimeo/main/compare/foobar..e8e5e4"

ts puppet "all nodes of class web_cluster_1: modified apache.conf; restart service apache"

ts incident_sev2_start "mysql2 crashed, site degraded"

ts incident_sev2_resolved "replaced db server"

ts incident "hurricane Sandy, systems unaffected but power outages among users, expect lower site usage"

# in those exceptional cases of manual production changes, try to not forget adding your event

ts manual_dieter "i have to try this firewall thing on the LB"

ts backup "backup from database slave vimeomysql22"

]]></pre>

<h4>Thoughts on incident nomenclature, severity levels and incident analysis</h4>

<p>Because there are so many unique and often subtle pieces of information pertaining to each individual incident, it&rsquo;s often hard to map an incident to</p>

<p>a simple severity level or keyword.</p>

<p>When displaying events as popups on graphs I think no severity levels are needed, the graphs and event descriptions are much more clear than any</p>

<p>severity level could convey.<br></p>

<p>However, I do think these levels are very useful for reporting and numerical analysis.<br></p>

<p>On slide 53 of <a href="http://www.slideshare.net/jallspaw/ops-metametrics-the-currency-you-pay-for-change">the metametrics slidedeck</a> Allspaw</p>

<p>mentions severity levels, which can be paraphrased in terms of service degradation for the end user:</p>

<p>1 (full), 2 (major), 3 (minor), 4 (no).<br></p>

<p>I would like to extend this notion into the opposite spectrum, and have similar levels on the positivie scale,</p>

<p>so that they represent positive incidents (like viral videos, press mentions, &hellip;) as opposed to problematic ones (outages).</p>

<p><br/>For incident analysis we need a rich nomenclature and schema:</p>

<p><b>incidents can (presumably) have a positive or negative impact, can be self-induced or not, and can be categorized with severity levels</b>;</p>

<p><b>they can also be planned for (maintenance, release announcements) or not</b>.  While we&rsquo;re at it,</p>

<p>how about events to mark point in time where the cause was detected, as well as resolved, so we can calculate TTD and TTR (see Allspaw slidedeck)?</p>

<p><br/>Since basically any event can have a positive or negative impact, an option is to leave out the type &lsquo;incident&rsquo; and give a severity field for every</p>

<p>event type.</p>

<p><br/>I&rsquo;m thinking of a good nomenclature and a schema to express all this.</p>

<p>(btw, notice how in common ops literature the word incident is usually associated with outages and bad things;</p>

<p>while actually an incident can just as well be a positive event) as well as UI features to support this analysis.</p>

<h4>I need your help</h4>

<p>The tcp receiver works, the backend works, i have a crude (but functional) web app, and a simple http api to retrieve the events in all kinds of formats.</p>

<p>Next up are:</p>

<ul>

<li>monitoring dashboard for graphite that gathers events from anthracite, can show metadata, and can mark a timeframe between start and stop events</li>

<li>plugings for puppet, chef to automatically submit their relevant events</li>

<li>a better web UI and actually provide features to do statistics on events and analysis such as TTD, TTR, with colors for severity levels etc</li>

</ul>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/histogram-statsd-graphing-over-time-with-graphite/">Histograms in statsd, and graphing them over time with graphite.</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/histogram-statsd-graphing-over-time-with-graphite/" rel="bookmark">
					<time class="entry-date" datetime="2012-11-07 18:45:11 -0400 -0400">
						November 7, 2012
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>I submitted a pull request to statsd which adds <a href="https://github.com/etsy/statsd/pull/162">histogram support</a>.</p>

<p><img style="float:left;margin:0 5px 0 0;" src="/files/Black_cherry_tree_histogram.svg.png" alt="Example histogram, from Wikipedia"/></p>

<p><br/>(refresher: a histogram is [a visualization of] a frequency distribution of data,</p>

<p>paraphrasing your data by keeping frequencies for entire classes (ranges of data).</p>

<p><a href="http://en.wikipedia.org/wiki/Histogram">histograms - Wikipedia</a>)</p>

<p><br/>It&rsquo;s commonly documented how to plot single histograms, that is a 2D diagram consisting of rectangles whose</p>

<ul>

<li>area is proportional to the frequency of a variable</li>

<li>whose width is equal to the class interval</li>

</ul>

<p>Class intervals go on x-axis, frequencies on y-axis.</p>

<p><br/></p>

<p><br/></p>

<p>Note: histogram class intervals are supposed to have the same width.</p>

<p><br/>My implementation allows arbitrary class intervals with potentially different widths,</p>

<p>as well as an upper boundary of infinite.</p>

<p></p></p>

<h4>Plotting histograms.. over time</h4>

<p>

We want to plot histograms over time, and not just for a few select points in time (in which case you can just make several histograms),

but a contiguous range of time, preferably through graphite's 2D graphs cause graphite is neat and common enough.

<br/>Time goes on x-axis, that's pretty much a given.  So I'm trying to explore ways to visualize both class intervals as well as frequencies on the y-axis.

</p>

<p>The example I'll use are page rendering timings, condensed into classes with upper boundaries of 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50 and infinite seconds</p>

<p><p></p>

<p>Tips and notes:</p>

<ul>

<li>

the histogram implementation stores absolute frequencies, but it's easy to get relative frequencies in percent, like so:

<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span style="color: #666666">&lt;!</span>[CDATA[

target<span style="color: #666666">=</span>scale(divideSeries(stats.timers.<span style="color: #666666">&lt;</span>your_metric<span style="color: #666666">&gt;</span>.bin_<span style="color: #666666">*</span>,stats.timers.render_time.count),<span style="color: #666666">100</span>)

]]<span style="color: #666666">&gt;</span>
</pre></div>


</li>

<li>I'll be using relative frequencies here because it normalizes the scale of the y-axis</li>

<li>In this use case each class has a notion of desirability (<i>low render time good, high render time bad</i>),

<br/>I think it makes sense to use color to represent this.  This extends to a lot of operational metrics which one would be using histograms for.

<br/>(unlike non-software histograms that represent demographics or tree heights, where classes usually have nothing to do with desirability or quality).

<br/>As it turns out, it's fairly easy to programmatically compute colors between green and red in order to have mathematically correct "steps" of color.

<br/>However, <a href="http://stackoverflow.com/questions/340209/generate-colors-between-red-and-green-for-a-power-meter">Looks like HSV values are more suited

than RGB</a> but <a href="https://github.com/graphite-project/graphite-web/issues/93">

graphite doesn't support HSV (yet)</a> (although one could convert HSV to RGB).

Also <a href="http://vis4.net/blog/posts/goodbye-redgreen-scales/">it looks like

green-purple would be a better choice for people with color blindness</a>.  I haven't gone too far in this topic.

</li>

<li>Since I choose to go with color gradients, it means I better use stacked graphs, otherwise it would be too hard to distinguish which graph is what</li>

<li>None of this is restricted to timing data.  The metric type under which histograms are (and should be) implemented is called "timing", which is

a misleading name but <a href="https://github.com/etsy/statsd/issues/98">we're working on renaming it</a>.</li>

</ul>

<h4>First version</h4>

<p><div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span style="color: #666666">&lt;!</span>[CDATA[

http<span style="color: #666666">:</span><span style="color: #408080; font-style: italic">//localhost:9000/render/?height=300&amp;</span>

width<span style="color: #666666">=740&amp;</span>from<span style="color: #666666">=-24</span>h<span style="color: #666666">&amp;</span>title<span style="color: #666666">=</span>Render time histogram<span style="color: #666666">&amp;</span>

vtitle<span style="color: #666666">=</span>relative frequency <span style="color: #008000; font-weight: bold">in</span> <span style="color: #666666">%&amp;</span>yMax<span style="color: #666666">=100&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_01,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;2FFF00&#39;</span>),<span style="color: #BA2121">&#39;0.01&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_05,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;64DD0E&#39;</span>),<span style="color: #BA2121">&#39;0.05&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_1,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;9CDD0E&#39;</span>),<span style="color: #BA2121">&#39;0.1&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_5,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;DDCC0E&#39;</span>),<span style="color: #BA2121">&#39;0.5&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_1,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;DDB70E&#39;</span>),<span style="color: #BA2121">&#39;1&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_5,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;FF6200&#39;</span>),<span style="color: #BA2121">&#39;5&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_10,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;FF3C00&#39;</span>),<span style="color: #BA2121">&#39;10&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_50,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;FF1E00&#39;</span>),<span style="color: #BA2121">&#39;50&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_inf,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;FF0000&#39;</span>),<span style="color: #BA2121">&#39;inf&#39;</span>)<span style="color: #666666">&amp;</span>

lineMode<span style="color: #666666">=</span>slope<span style="color: #666666">&amp;</span>areaMode<span style="color: #666666">=</span>stacked<span style="color: #666666">&amp;</span>drawNullAsZero<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">false</span><span style="color: #666666">&amp;</span>hideLegend<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">false</span>

]]<span style="color: #666666">&gt;</span>
</pre></div>
</p>

<p><img src="/files/rendertime-histogram.png" width="740" height="300" alt="rendertime histogram" /></p>

<p><br/>Turns out we mainly see the vast majority that performs well, simply because with this way of rendering,</p>

<p>the higher the frequency of a class, the more prominent.  Bad values are hard to see because there&rsquo;s not many of them,</p>

<p>despite being more interesting.</p>

<p>A thought I had at this point was to make all &ldquo;class bands&rdquo; equally wide and use a green-to-red gradient to denote the frequency values,</p>

<p>or even just keep the current color assignments but rely on something like opacity to express frequencies. Alas, none of this is currently</p>

<p>possible with graphite, as far as I can tell.  Though I would like to explore this further.  Especially because I think it wouldn&rsquo;t be hard to implement</p>

<p>in graphite.</p>

<p><br/></p>

<p><br/></p>

<p>So, let&rsquo;s see what <i>can</i> be done right now.</p>

<h4>Leaving out the smallest class</h4>

<p>This adaption is basically the same as before, but leaves out the smallest class (which took most space), this way</p>

<p>the other bands are a bit more visible but the effect isn&rsquo;t as clear as we want.</p>

<p><div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span style="color: #666666">&lt;!</span>[CDATA[

http<span style="color: #666666">:</span><span style="color: #408080; font-style: italic">//localhost:9000/render/?height=300&amp;</span>

width<span style="color: #666666">=740&amp;</span>from<span style="color: #666666">=-24</span>h<span style="color: #666666">&amp;</span>title<span style="color: #666666">=</span>Render time histogram<span style="color: #666666">&amp;</span>

vtitle<span style="color: #666666">=</span>relative frequency <span style="color: #008000; font-weight: bold">in</span> <span style="color: #666666">%</span>, leaving out first <span style="color: #008000; font-weight: bold">class</span><span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_05,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;64DD0E&#39;</span>),<span style="color: #BA2121">&#39;0.05&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_1,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;9CDD0E&#39;</span>),<span style="color: #BA2121">&#39;0.1&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_5,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;DDCC0E&#39;</span>),<span style="color: #BA2121">&#39;0.5&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_1,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;DDB70E&#39;</span>),<span style="color: #BA2121">&#39;1&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_5,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;FF6200&#39;</span>),<span style="color: #BA2121">&#39;5&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_10,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;FF3C00&#39;</span>),<span style="color: #BA2121">&#39;10&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_50,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;FF1E00&#39;</span>),<span style="color: #BA2121">&#39;50&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_inf,stats.timers.render_time.count),<span style="color: #666666">100</span>),<span style="color: #BA2121">&#39;FF0000&#39;</span>),<span style="color: #BA2121">&#39;inf&#39;</span>)<span style="color: #666666">&amp;</span>

lineMode<span style="color: #666666">=</span>slope<span style="color: #666666">&amp;</span>areaMode<span style="color: #666666">=</span>stacked<span style="color: #666666">&amp;</span>drawNullAsZero<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">false</span><span style="color: #666666">&amp;</span>hideLegend<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">false</span>

]]<span style="color: #666666">&gt;</span>
</pre></div>
</p>

<p><img src="/files/rendertime-histogram-leaving-out-first-class.png" width="740" height="300" alt="rendertime histogram leaving out first class" /></p>

<h4>Per-band scaling</h4>

<p>Finally, the bigger the values represented by each class the more we inflate the band,</p>

<p>so the more problematic cases become more visible, despite having a lower frequency.</p>

<p><div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span style="color: #666666">&lt;!</span>[CDATA[

http<span style="color: #666666">:</span><span style="color: #408080; font-style: italic">//localhost:9000/render/?height=300&amp;</span>

width<span style="color: #666666">=740&amp;</span>from<span style="color: #666666">=-24</span>h<span style="color: #666666">&amp;</span>title<span style="color: #666666">=</span>Render time histogram<span style="color: #666666">&amp;</span>

vtitle<span style="color: #666666">=</span>rel. freq <span style="color: #008000; font-weight: bold">with</span> scale adjustment per band<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_01,stats.timers.render_time.count),<span style="color: #666666">0.01</span>),<span style="color: #BA2121">&#39;2FFF00&#39;</span>),<span style="color: #BA2121">&#39;0.01&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_05,stats.timers.render_time.count),<span style="color: #666666">0.04</span>),<span style="color: #BA2121">&#39;64DD0E&#39;</span>),<span style="color: #BA2121">&#39;0.05&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_1,stats.timers.render_time.count),<span style="color: #666666">0.05</span>),<span style="color: #BA2121">&#39;9CDD0E&#39;</span>),<span style="color: #BA2121">&#39;0.1&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_0_5,stats.timers.render_time.count),<span style="color: #666666">0.4</span>),<span style="color: #BA2121">&#39;DDCC0E&#39;</span>),<span style="color: #BA2121">&#39;0.5&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_1,stats.timers.render_time.count),<span style="color: #666666">0.5</span>),<span style="color: #BA2121">&#39;DDB70E&#39;</span>),<span style="color: #BA2121">&#39;1&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_5,stats.timers.render_time.count),<span style="color: #666666">4</span>),<span style="color: #BA2121">&#39;FF6200&#39;</span>),<span style="color: #BA2121">&#39;5&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_10,stats.timers.render_time.count),<span style="color: #666666">5</span>),<span style="color: #BA2121">&#39;FF3C00&#39;</span>),<span style="color: #BA2121">&#39;10&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_50,stats.timers.render_time.count),<span style="color: #666666">40</span>),<span style="color: #BA2121">&#39;FF1E00&#39;</span>),<span style="color: #BA2121">&#39;50&#39;</span>)<span style="color: #666666">&amp;</span>

target<span style="color: #666666">=</span>alias(color(scale(divideSeries(stats.timers.render_time.bin_inf,stats.timers.render_time.count),<span style="color: #666666">60</span>),<span style="color: #BA2121">&#39;FF0000&#39;</span>),<span style="color: #BA2121">&#39;inf&#39;</span>)<span style="color: #666666">&amp;</span>

lineMode<span style="color: #666666">=</span>slope<span style="color: #666666">&amp;</span>areaMode<span style="color: #666666">=</span>stacked<span style="color: #666666">&amp;</span>drawNullAsZero<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">false</span><span style="color: #666666">&amp;</span>hideLegend<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">false</span>

]]<span style="color: #666666">&gt;</span>
</pre></div>
</p>

<p><img src="/files/rendertime-histogram-higher-focus-for-higher-class-interval.png" width="740" height="300" alt="rendertime histogram with higher focus for higher class interval" /></p>

<p><br/>I started off by scaling each band by the width of the class interval.  This is actually more arbitrary than it may seem.</p>

<p><br/>The point is that now it&rsquo;s easier to spot acute as well as long-standing problems, but note you can&rsquo;t really read statistics from this graph because of the per-band scaling.</p>

<p><br/>Note also that outliers contribute to the outer band(s) and are given as much focus as non-outliers in the same bands.</p>

<p>In a system over which you have no complete control (i.e. if you were graphing histograms of time until first byte or page loaded at client,</p>

<p>where you rely on the internet as a transport) it makes sense to give less attention to outliers and focus on optimizing for as many users as possible,</p>

<p>I think it there&rsquo;s no reliable way to subtract outliers from the upper bands and you should also graph averages and percentiles and understand what each</p>

<p>graph does.</p>

<p>But anyway here I want to include outliers, because they represent latencies we can fix.</p>

<h4>Final notes</h4>

<p>While the tools we have are by no means perfect, I&rsquo;m seeing gradual improvement in the monitoring space.  This work is only a small piece</p>

<p>of the puzzle. The rendering of histograms can be improved but at this point I think they are good enough to be usable. The real challenge is</p>

<p>putting in place automated trending, anomaly detection and alerting.  If we can figure that out, there&rsquo;s less need to be looking at graphs in the first place.</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
                <a href="http://localhost:1313//tags/perf/index.html" rel="tag">perf</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/dell_crowbar_openstack_swift/">Dell crowbar openstack swift</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/dell_crowbar_openstack_swift/" rel="bookmark">
					<time class="entry-date" datetime="2012-05-02 11:50:11 -0400 EDT">
						May 2, 2012
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>

It provides a central management interface which leverages chef, but extends it to a broader scope: it takes care of configuring the bios and raid firmware, BMC/IPMI, doing a PXE autoinstall and setting up chef, which of course does everything else you might need.

<br/>The crowbar admin machine also runs Nagios and Ganglia and the configuration is automatically kept in sync with the roles you apply to your nodes. (adding support for a different monitoring/alerting solution is probably not too hard.  I'm thinking about graphite support or even integrating with an existing graphite setup)

<br/>It uses a plugin system called barclamps and it's vendor-agnostic.  (minus the barclamps that deal with your hardware)

It can even suggest roles for your nodes based on best practices (like: for an X-node openstack cluster we recommend Y proxy's and Z foobars) and hardware properties it discovers (storage vs cpu power vs ram etc)

<br/>Next step would be automating purchase orders, delivery handling and physical installation in racks.

</p>

<p>I think this tool deserves more attention and should be added to your devops toolchain for the cloud (<i>triple buzzword bonus!!!</i>)</p>

<p>

I've currently been using it to easily setup an <a href="https://github.com/dellcloudedge/crowbar/wiki/Swift--barclamp">openstack swift</a> test cluster on <a href="https://github.com/dellcloudedge/crowbar/wiki/Running-Crowbar-in-VirtualBox-VMs">a virtualbox infrastructure</a> (even the latter is easily automated with a shell script), and I'm looking forward to use it on physical hardware and have it take care of the bios and raid settings, I'm expecting it will even set the LCD's on the frontpanel to something interesting automatically. (btw the <a href="http://www.dell.com/poweredge">12G poweredge servers</a> have some neat new features)

</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/openstack/index.html" rel="tag">openstack</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/what_the_open_source_community_can_learn_from_devops/">What the open source community can learn from Devops</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/what_the_open_source_community_can_learn_from_devops/" rel="bookmark">
					<time class="entry-date" datetime="2010-09-03 22:26:22 -0400 EDT">
						September 3, 2010
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>For the sake of getting a point across, I'll simplify some things.</p>

<h3>First, a crash course on Devops...</h3>

<p><br />

A commonly used organisatorial idiom used in tech companies is that of developers and operations.</p>

<p>Developers:</p>

<ul>

<li>develop a product</li>

<li>improve their product based on feedback from production usage</li>

</ul>

<p>Operations:</p>

<ul>

<li>put the product in production, making it available for users/customers</li>

<li>give feedback to devs</li>

</ul>

<p>Experience shows this model often falls short.  'Dev' and 'Ops' being too artificially separated from each other, resulting in improper communication, clashing procedures and tools,<br />

resulting in devs disliking ops ("we need to push this out to users, ops are holding us back"), and the other way around ("again new code that will cause trouble, and we will have to figure it out")<br />

It doesn't take a genius to see this is pretty ineffective.  There's a better way: integrating and reconciling dev and ops, so that all involved know the hard parts of each others' jobs, and in fact letting each other do the others' job.  (developers being responsible for their own checkouts, ops working on the code, etc).  Most of all it's about culture over processes.  About being smart and nice human beings.<br />

The exact methods are still being experimented with and preached about, and has recently gotten the name "Devops".<br />

There is a really good <a href="http://www.jedi.be/blog/2010/02/12/what-is-this-devops-thing-anyway/">Devops explanation</a> online, with more details.  Read it.</p>

<p>Often enough we're talking about teams working for the same company, usually under the same roof, so it isn't too terribly hard to implement these ideas.</p>

<h3>Now, let's look at the open source community</h3>

<p>Open source developers ("upstream"):</p>

<ul>

<li>develop stuff</li>

<li>improve their stuff based on feedback from end users</li>

</ul>

<p>Distributions ("downstream"):</p>

<ul>

<li>package stuff and make it available to end users</li>

<li>get bugreports, which often get forwarded to upstream</li>

</ul>

<p>Looks familiar?</p>

<h4>The problems are similar too...</h4>

<p>Like above, the problems stem from both parties not working together enough, and doing things on their own.</p>

<p>Some upstreams:</p>

<ul>

<li>like to use "weird" (home grown) build systems</li>

<li>violate FHS</li>

<li>use home grown packaging systems. Languages and applications with plugins like to do this</li>

<li>mix bugfixes, security patches and feature additions in the same code branch (often there is not enough manpower to maintain them in separation, and the need for it is dependent on how/when downstreams ship it anyway)</li>

<li>run into the chicken/egg problem:  they need to release software to have it shipped and tested, but it should only be released after being properly tested.  ("Release early, release often" alleviates this, but it's not always that easy)

</ul>

<p>..making it hard for downstreams.<br />

Even for each other: unannounced/frequent API changes come to mind.</p>

<p>Dowstreams, often:</p>

<ul>

<li>Lack discipline and/or tools to properly report back to upstream.  Users don't like te report the same issue on two bugtrackers</li>

<li>Have to make hard choices.  Not shipping software at all or patching beyond recognition, often enough without knowing how the software really works or is implemented.</li>

<li>Don't contribute patches back to upstream.  Posting them on some obscure albeit "public" mailing list or code archive isn't the most effective either.  Patches that are sent back often don't get merged, making it hard for other downstreams to find them. (and hence, they work on their own patches)</li>

</ul>

<p>Nothing pleases an upstream more then complaints from end users running into problems that only happen witch patches applied by the distributor (patches that are deemed necessary to make the app work properly in the distro.  The irony..)</p>

<p>Some distributions focus on shipping "only stable software", causing them to be obsolete by definition. (Time to production often extends in the order of years), and are forced to apply so many patches that they are essentially forking their upstreams.  Add poor feedback loops to the list and the situation is about as ineffecient as it can get.<br />

Other distributions limit their role to giving you the real open source software experience in it's current state, and that state is not always pretty.</p>

<h4>but they are much harder to solve</h4>

<ul>

<li>Upstream and downstream are separated much more, resulting in very little communication between both parties.  So the incompatibilities manifest themselves even harder.</li>

<li>Among distributions, there are very different visions on and implementations of tools and processes.  Pretty much each distro has a vision which separates it from the others.<br />

Among upstreams, there are as well some different ideas on how things should be done.  Luckily enough upstream developers agree on some things.  But there are some "clusters" doing things their - often radically different - way (freedesktop.org and suckless.org come to mind)<br />

The amount of incompatibilities is pretty much the carthesian product of the amount of distributions with the amount of "different visions" among upstreams</li>

<li>Despite their differences, some upstreams and downstreams actually do have some common ground, but as they don't involve each other in tools nor processes, they hardly benefit from each other</li>

</ul>

<p>So, in contrast to popular belief, <b>open source is not a magical wonderland where everyone works nicely together.</b></p>

<p>Tech companies are usually on their way if they understand and can introduce agile and devops, but I think in the open source ecosystem it's much harder to bring unity.</p>

<p>Luckily, some smart people are already working on bridging the gap between up- and downstream, and between each other.<br />

some examples:</p>

<ul>

<li><a href="http://www.transifex.net/">transifex.net</a> provides a common translation infrastructure and service</li>

<li><a href="https://launchpad.net/">launchpad.net</a> provides code hosting and cross-project issue tracking (from what I heard, tickets reported for downstreams can easily be linked to the relevant upstream project.  But I never tried it)</li>

<p><!-- kde distro mailing list -->

</ul>

<p>I also think about Fosdem's cross-distro miniconf and the freedesktop.org project, which encourage closer cooperation between different downstreams and desktop projects, respectively.</p>

<p>So, how can we solve this?  How can we maximize the end-user experience with more efficient communication and tools?<br />

Some ideas I have:</p>

<ul>

<li>upstreams should definitely abandon cvs and svn.  And avoid using no version control.  Use a version control system that makes forking and contributing more easy, like git.  Downstreams can then fork all the repos and apply the patches in a separate branch.  It will improve cooperation between up- and downstream</li>

<li>upstreams: try to be compatible with how your downstreams and end-users want to use your software.  Accept patches that add configure flags to enable/disable support for the xdg basedir spec, for example.  You don't need to put them in your master branch.</li>

<li>Provide the source for each release so that it can easily be fetched and checksummed</li>

<li>upstreams should definitely avoid coming up with their own "package management" or "self upgrade" solutions.  Let downstream do their job and make your software compatible with existing packaging solutions</li>

<li>use common build systems.  Provide makefiles with an install and uninstall target.  Don't violate FHS.  Make sure your Makefile and software easily allows using a different prefix.  I even do this for repo's that contain 1 or 2 shell scripts. (<a href="http://github.com/Dieterbe/libui-sh/blob/master/Makefile">example</a>)

<li>Ideally, we would have something like git, but for issue tracking.  Everyone could still host their own issue tracker, but integration between up- and downstream could become much more efficient</li>

<li>talk to each other, brainstorm.  we can make everyone's life easier</li>

</ul>

<p>I don't think we should try to go much beyond some common infrastructure/tools and some best practices.  People will always have different opinions on how things should be done.  And that's a good thing, it's the very definiton of the open source community: scratch your own itch.</p>

<p>What do you think?</p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/foss/index.html" rel="tag">foss</a>
            
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
		</span>
	</footer>
</article> 

			
				<article class="post type-post status-publish format-standard hentry">

	
	<header class="entry-header">

	

		<div class="entry-meta">
			<span class="cat-links">
                
			</span>
		</div>

        <h1 class="entry-title"><a href="http://localhost:1313/post/rrdtool_updating_rra_settings_and_keeping_your_collected_data/">RRDtool: updating RRA settings and keeping your collected data</a></h1>

		<div class="entry-meta">
			<span class="entry-date">
				<a href="http://localhost:1313/post/rrdtool_updating_rra_settings_and_keeping_your_collected_data/" rel="bookmark">
					<time class="entry-date" datetime="2009-12-09 15:05:14 -0400 -0400">
						December 9, 2009
					</time>
				</a>
			</span>
		</div>

	</header>
	
	<div class="entry-content">
		<p>Especially when using <a href="community.zenoss.org">zenoss</a> (the monitoring solution I mostly work with at <a href="http://www.kangaroot.net/">Kangaroot</a>), which uses very conservative RRD settings by default (i.e. 5-minute intervals for only the first 50 hours).  Zenoss provides a way for you to change the way RRD&rsquo;s are created, but not to apply those settings to already existing RRD files, which I found out <em>after</em> I started monitoring everything ;)</p></p>

<p><a href="http://oss.oetiker.ch/rrdtool/doc/rrdresize.en.html">rrdresize</a> can help: it (just) adds or removes locations for rows.<br />

In my case it was not good enough because zenoss uses a variety of resolutions (step sizes), and so if you add rows to all of them rrdtool - when graphing - will often pick a higher resolution RRA that just had rows added (and hence contain unknown values), even though you have the values, albeit at a lower resolution.</p>

<p>So you need a way to update all rows in the RRA's.<br />

I <a href="http://stackoverflow.com/questions/802902/export-import-rrdtool-database-with-differents-rra/1867163#1867163">found</a> a <a href="http://code.google.com/p/pmptools/source/browse/trunk/rrd/rrdmove">perl tool</a> that does just that.  (I think, I didn't study all details).  So, you install that in your /home/zenoss for instance and then you run the following script, which creates new rrd files with the new settings and uses the perl script to copy all data into it.</p>

<p><div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%">&lt;!<span style="color: #666666">[</span>CDATA<span style="color: #666666">[</span>

<span style="color: #408080; font-style: italic">#!/bin/sh</span>

<span style="color: #408080; font-style: italic"># invoke me like this:</span>

<span style="color: #408080; font-style: italic"># find /usr/local/zenoss/zenoss/perf/ -name &#39;*.rrd&#39; -exec ./newrrd.sh {} \; &gt;&gt; newrrd-logfile</span>



<span style="color: #19177C">file</span><span style="color: #666666">=</span><span style="color: #19177C">$1</span>

<span style="color: #19177C">backupdir</span><span style="color: #666666">=</span>/home/zenoss/rrds-backup

<span style="color: #19177C">newdir</span><span style="color: #666666">=</span>/home/zenoss/rrds-new

<span style="color: #666666">[</span> -d <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$backupdir</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">]</span> <span style="color: #666666">||</span> mkdir -p <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$backupdir</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">||</span> <span style="color: #008000">exit </span>2

<span style="color: #666666">[</span> -d <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$newdir</span><span style="color: #BA2121">&quot;</span>    <span style="color: #666666">]</span> <span style="color: #666666">||</span> mkdir -p <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$newdir</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">||</span> <span style="color: #008000">exit </span>2

<span style="color: #666666">[</span> -f <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$file</span><span style="color: #BA2121">&quot;</span>      <span style="color: #666666">]</span> <span style="color: #666666">||</span> <span style="color: #008000">exit </span>3



<span style="color: #008000">echo</span> <span style="color: #BA2121">&quot;Processing </span><span style="color: #19177C">$file</span><span style="color: #BA2121"> ..&quot;</span>

<span style="color: #19177C">base</span><span style="color: #666666">=</span><span style="color: #BA2121">&quot;`basename &quot;</span><span style="color: #19177C">$file</span><span style="color: #BA2121">&quot;`&quot;</span>

<span style="color: #666666">[</span> ! -f <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$backupdir</span><span style="color: #BA2121">/</span><span style="color: #19177C">$base</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">]</span> <span style="color: #666666">||</span> mv <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$backupdir</span><span style="color: #BA2121">/</span><span style="color: #19177C">$base</span><span style="color: #BA2121">&quot;</span> <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$backupdir</span><span style="color: #BA2121">/</span><span style="color: #19177C">$base</span><span style="color: #BA2121">&quot;</span>.old <span style="color: #666666">||</span> <span style="color: #008000">exit </span>4

cp <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$file</span><span style="color: #BA2121">&quot;</span> <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$backupdir</span><span style="color: #BA2121">/</span><span style="color: #19177C">$base</span><span style="color: #BA2121">&quot;</span>

<span style="color: #008000">cd</span> <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$newdir</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">&amp;&amp;</span> rrdtool create <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$base</span><span style="color: #BA2121">&quot;</span> <span style="color: #BB6622; font-weight: bold">\</span>

--step <span style="color: #BA2121">&#39;300&#39;</span> <span style="color: #BB6622; font-weight: bold">\</span>

--start <span style="color: #BA2121">&#39;1230768000&#39;</span> <span style="color: #BB6622; font-weight: bold">\</span>

<span style="color: #BA2121">&#39;DS:ds0:GAUGE:900:U:U&#39;</span> <span style="color: #BB6622; font-weight: bold">\</span>

<span style="color: #BA2121">&#39;RRA:AVERAGE:0.5:1:122640&#39;</span> <span style="color: #BB6622; font-weight: bold">\</span>

<span style="color: #BA2121">&#39;RRA:AVERAGE:0.5:6:55536&#39;</span> <span style="color: #BB6622; font-weight: bold">\</span>

<span style="color: #BA2121">&#39;RRA:MAX:0.5:6:55536&#39;</span>

/home/zenoss/rrdremove.pl <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$backupdir</span><span style="color: #BA2121">/</span><span style="color: #19177C">$base</span><span style="color: #BA2121">&quot;</span> <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$base</span><span style="color: #BA2121">&quot;</span> | grep -v <span style="color: #666666">2009</span> <span style="color: #408080; font-style: italic"># hide some output</span>

cp <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$base</span><span style="color: #BA2121">&quot;</span> <span style="color: #BA2121">&quot;</span><span style="color: #19177C">$file</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">||</span> <span style="color: #008000">exit </span>5

<span style="color: #008000">echo</span> <span style="color: #BA2121">&quot;Done&quot;</span>

<span style="color: #666666">]]</span>&gt;
</pre></div>
<p></p>

<p>Oh and btw, <a href="http://www.famzah.net/rrdwizard/index.php">rrdwizard</a> is a cool webapp when you&rsquo;re feeling too lazy/have forgotten how to write rrdtool commands</p></p>

	</div>

	<footer class="entry-meta">
		<span class="tag-links">		
			
                <a href="http://localhost:1313//tags/devops/index.html" rel="tag">devops</a>
            
                <a href="http://localhost:1313//tags/monitoring/index.html" rel="tag">monitoring</a>
            
		</span>
	</footer>
</article> 

			
		
		</div>
	</div>
	<div id="secondary">

	

	<div id="primary-sidebar" class="primary-sidebar widget-area" role="complementary">

        

        

		<aside id="categories-3" class="widget widget_categories">


    
     


    <br/>
	<h1 class="widget-title">Popular tags</h1>

    <a href="http://localhost:1313/tags/monitoring">monitoring</a>
    25
    <br/>
    <a href="http://localhost:1313/tags/devops">devops</a>
    23
    <br/>
    <a href="http://localhost:1313/tags/golang">golang</a>
    8
    <br/>
    <br/>
    <br/>

	<h1 class="widget-title">Other tags</h1>

    
        
            <a href="http://localhost:1313/tags/arch">arch</a>
            19
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/bash">bash</a>
            15
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/cakephp">cakephp</a>
            8
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/conf">conf</a>
            9
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/dauth">dauth</a>
            1
            <br/>
        
    
        
        
    
        
            <a href="http://localhost:1313/tags/drums">drums</a>
            1
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/drupal">drupal</a>
            5
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/foss">foss</a>
            45
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/git">git</a>
            3
            <br/>
        
    
        
        
    
        
            <a href="http://localhost:1313/tags/information-age">information-age</a>
            3
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/life">life</a>
            33
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/linux">linux</a>
            20
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/lua">lua</a>
            1
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/mail">mail</a>
            1
            <br/>
        
    
        
        
    
        
            <a href="http://localhost:1313/tags/music">music</a>
            5
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/mysql">mysql</a>
            3
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/n900">n900</a>
            2
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/netlog">netlog</a>
            6
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/openstack">openstack</a>
            1
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/perf">perf</a>
            10
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/photos">photos</a>
            3
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/php">php</a>
            11
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/productivity">productivity</a>
            6
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/python">python</a>
            5
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/real-life">real-life</a>
            2
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/thesis">thesis</a>
            3
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/travel">travel</a>
            2
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/uzbl">uzbl</a>
            6
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/vimeo">vimeo</a>
            3
            <br/>
        
    
        
            <a href="http://localhost:1313/tags/web2.0">web2.0</a>
            9
            <br/>
        
    
</aside>


	</div>

</div>

</div>

		</div>

		<footer id="colophon" class="site-footer" role="contentinfo">

			<div class="site-info">
				<a href="http://gohugo.io">Proudly powered by Hugo</a>
			</div>
		</footer>
	</div>

	<script type='text/javascript' src='http://localhost:1313//js/functions.js'></script>
<script>document.write('<script src="http://'
        + (location.host || 'localhost').split(':')[0]
		+ ':1313/livereload.js?mindelay=10"></'
        + 'script>')</script></body>
</html>
