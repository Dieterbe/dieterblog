<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>devops on Dieter&#39;s blog</title>
    <link>http://dieter.plaetinck.be/tags/devops/</link>
    <description>Recent content in devops on Dieter&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Dec 2016 19:13:03 +0000</lastBuildDate>
    <atom:link href="/tags/devops/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Practical fault detection: redux. Next-generation alerting now as presentation</title>
      <link>http://dieter.plaetinck.be/post/practical-fault-detection-redux-next-generation-alerting-now-as-presentation/</link>
      <pubDate>Sat, 10 Dec 2016 19:13:03 +0000</pubDate>
      
      <guid>http://dieter.plaetinck.be/post/practical-fault-detection-redux-next-generation-alerting-now-as-presentation/</guid>
      <description>&lt;p&gt;This summer I had the opportunity to present my &lt;a href=&#34;http://dieter.plaetinck.be/post/practical-fault-detection-alerting-dont-need-to-be-data-scientist/&#34;&gt;practical fault detection&lt;/a&gt; concepts and &lt;a href=&#34;http://dieter.plaetinck.be/post/practical-fault-detection-on-timeseries-part-2/&#34;&gt;hands-on approach&lt;/a&gt; as conference presentations.&lt;/p&gt;
&lt;p&gt;First at &lt;a href=&#34;http://conferences.oreilly.com/velocity/vl-ca-2016/public/schedule/detail/49335&#34;&gt;Velocity&lt;/a&gt; and then at &lt;a href=&#34;https://www.usenix.org/conference/srecon16europe/program/presentation/plaetinck&#34;&gt;SRECon16 Europe&lt;/a&gt;.  The latter page also contains the recorded video.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://dieter.plaetinck.be/files/poor-mans-fault-detection.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you&#39;re interested at all in tackling non-trivial timeseries alerting use cases (e.g. working with seasonal or trending data) this video should be useful to you.&lt;/p&gt;
&lt;p&gt;It&#39;s basically me trying to convey in a concrete way why I think the big-data and math-centered algorithmic approaches come with a variety of problems making them unrealistic and unfit,
whereas the real breakthroughs happen when tools recognize the symbiotic relationship between operators and software, and focus on supporting a collaborative, iterative process to managing alerting over time. There should be a harmonious relationship between operator and monitoring tool, leveraging the strengths of both sides, with minimal factors harming the interaction.
From what I can tell, &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun&lt;/a&gt; is pioneering this concept of a modern alerting IDE and is far ahead of other alerting tools in terms of providing high alignment between alerting configuration, the infrastructure being monitored, and individual team members, which are all moving targets, often even fast moving.  In my experience this results in high signal/noise alerts and a happy team.
(according to Kyle, the bosun project leader, my take &lt;a href=&#34;https://twitter.com/kylembrandt/status/804409406846746624&#34;&gt;is a useful one&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;That said, figuring out the tool and using it properly has been, and remains, rather hard.  I know many who rather not fight the learning curve.  Recently the bosun team has been making strides at making it easier for newcomers -
e.g. &lt;a href=&#34;https://github.com/bosun-monitor/bosun/pull/1817&#34;&gt;reloadable configuration&lt;/a&gt; and &lt;a href=&#34;https://grafana.net/plugins/bosun-app&#34;&gt;Grafana integration&lt;/a&gt; - but there is lots more to do.
Part of the reason is that some of the UI tabs aren&#39;t implemented for non-opentsdb databases and integrating Graphite for example into the tag-focused system that is bosun, is bound to be a bit weird.  (that&#39;s on me)&lt;/p&gt;
&lt;p&gt;For an interesting juxtaposition, we released &lt;a href=&#34;http://docs.grafana.org/guides/whats-new-in-v4/&#34;&gt;Grafana v4 with alerting functionality&lt;/a&gt; which approaches the problem from the complete other side: simplicity and a unified dashboard/alerting workflow first, more advanced alerting methods later.  I&#39;m doing what I can to make the ideas of both projects converge, or at least make the projects take inspiration from each other and combine the good parts. (just as I hope to bring the ideas behind &lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;graph-explorer&lt;/a&gt; into Grafana, eventually&amp;hellip;)&lt;/p&gt;
&lt;p&gt;Note:
One thing that somebody correctly pointed out to me, is that I&#39;ve been inaccurate with my terminology.
Basically, machine learning and anomaly detection can be as simple or complex as you want to make it. In particular, what we&#39;re doing with our alerting software (e.g. bosun) can rightfully also be considered machine learning, since we construct models that learn from data and make predictions.  It may not be what we think of at first, and indeed, even a simple linear regression is a machine learning model.  So most of my critique was more about the big data approach to machine learning, rather than machine learning itself.  As it turns out then the key to applying machine learning successfully is tooling that assists the human operator in every possible way, which is what IDE&#39;s like bosun do and how I should have phrased it, rather than presenting it as an alternative to machine learning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Focusing on open source monitoring.  Joining raintank.</title>
      <link>http://dieter.plaetinck.be/post/focusing-on-open-source-monitoring-joining-raintank/</link>
      <pubDate>Fri, 03 Jul 2015 09:22:02 -0700</pubDate>
      
      <guid>http://dieter.plaetinck.be/post/focusing-on-open-source-monitoring-joining-raintank/</guid>
      <description>&lt;h2&gt;Goodbye Vimeo&lt;/h2&gt;
&lt;p&gt;
It&#39;s never been as hard saying goodbye to the people and the work environment as it is now.
&lt;br/&gt;
Vimeo was created by dedicated film creators and enthusiasts, just over 10 years ago, and today it still shows.
From the quirky, playful office culture, &lt;a href=&#34;https://vimeo.com/staff&#34;&gt;the staff created short films&lt;/a&gt;,
to the &lt;a href=&#34;https://vimeo.com/categories&#34;&gt;tremendous curation effort&lt;/a&gt; and &lt;a href=&#34;https://vimeo.com/channels/staffpicks/videos&#34;&gt;staff picks&lt;/a&gt; including &lt;a href=&#34;http://websta.me/p/797128421285501516_12986477&#34;&gt;monthly staff screenings&lt;/a&gt; where we get to see the best of the best videos on the Internet each month,
to the dedication towards building the best platform and community on the web to enjoy videos and the uncompromising commitment to supporting movie creators and working in their best interest.
&lt;br/&gt;Engineering wise, there has been plenty of opportunity to make an impact and learn.
&lt;br/&gt;Nonetheless, I have to leave and I&#39;ll explain why.  First I want to mention a few more things.
&lt;/p&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/bye_vimeo.jpg&#34; alt=&#34;vimeo goodbye drink&#34; /&gt;
&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;
In Belgium I used to hitchhike to and from work so that each day brought me opportunities to have conversations with a diverse, fantastic assortment of people.  I still fondly remember some of those memories. (and it was also usually faster than taking the bus!)
&lt;br/&gt;Here in NYC this isn&#39;t really feasible, so I tried the next best thing.  A mission to have lunch with every single person in the company, starting with those I don&#39;t typically interact with.  I managed to have lunch with 95 people, get to know them a bit, find some gems of personalities and anecdotes, and have conversations on a tremendous variety of subjects, some light-hearted, some deep and profound.  It was fun and I hope to be able to keep doing such social experiments in my new environment.

&lt;/p&gt;
&lt;p&gt;
	Vimeo is also part of my life in an unusually personal way.  When I came to New York (my first ever visit to the US) in 2011 to interview, I also met a pretty fantastic woman in a random bar in Williamsburg. We ended up traveling together in Europe, I decided to move the US and we moved in together.  I&#39;ve had the pleasure of being submerged in both American and Greek culture for the last few years, but the best part is that today we are engaged and I feel like the luckiest guy in the world.  While I&#39;ve tried to keep work and personal life somewhat separate, Vimeo has made an undeniable ever lasting impact on my life that I&#39;m very grateful for.
&lt;/p&gt;
&lt;p&gt;
At Vimeo I found an area where a bunch of my interests converge: operational best practices, high performance systems, number crunching, statistics and open source software.  Specifically, timeseries metrics processing in the context of monitoring.  While I have enjoyed my opportunity &lt;a href=&#34;http://dieter.plaetinck.be/tags/monitoring/&#34;&gt;to make contributions in this space&lt;/a&gt; to help our teams and other companies who end up using my tools, I want to move out of the cost center of the company, I want to be in the department that creates the value.  If I want to focus on open source monitoring, I should align my incentives with those of my employer.  Both for my and their sake.
&lt;b&gt;I want to make more profound contributions to the space.  The time has come for me to join a company for which the main focus is making open source monitoring better.&lt;/b&gt;
&lt;/p&gt;
&lt;h2&gt;Hello raintank!&lt;/h2&gt;
Over the past two years or so I&#39;ve talked to many people in the industry about monitoring, many of them trying to bring me into their team.
I never found a perfect fit but as we transitioned from 2014 into 2015, the stars seemingly aligned for me.
Here&#39;s why I&#39;m very excited to join the &lt;a href=&#34;http://www.raintank.io/&#34;&gt;raintank&lt;/a&gt; crew:
&lt;ul&gt;
&lt;li&gt;I&#39;m a strong believer in open source.  I believe fundamental infrastructure tooling should be modifiable and in your control.  It&#39;s partially a philosophical argument, but also what I believe will separate long lasting business from short term successes.
&lt;br/&gt;SaaS is great, but not if it&#39;s built to lock you in.  In this day and age, I think you should &#34;lock in&#34; your customers by providing a great experience they can&#39;t say no to, not by having them build technical debt as they integrate into your stack.
&lt;br/&gt;That said, integrating with open source also incurs technical debt, and some closed source service providers are so good that the debt is worth it.  But I don&#39;t believe this lasts long term, especially given the required pricing models.  I think you can entice customers more by lowering the debt they build up (i.e. use standardized protocols, tooling and making it easy for them to leave) as they adopt your service.
&lt;!-- I was really interested in datadog for example.  I liked the team, I liked the product they are/were building and I saw they would become successfull, but it&#39;s not the kind of business I want to build towards long term. --&gt;
&lt;br/&gt;raintank&#39;s commitment to open source is not a gimmick but stems from a fundamental conviction that 100% open source is the right thing to do.  That by providing freedom, you get loyalty.
I think they came up with a good business model that combines the benefits of SaaS with those of open source.  At least it&#39;s a bet I want to take.
It&#39;s not an easy feat but I think we got a good formula. (basically openSAAS, the stack is open source, integrates in the wider ecosystem, you can run it yourself, you own your data, you can use our SAAS, or create a mixed setup.)&lt;/li&gt;
&lt;br&gt;
&lt;iframe src=&#34;https://player.vimeo.com/video/131790481&#34; width=&#34;500&#34; height=&#34;281&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;li&gt;Something I value a lot in coworkers, besides &#34;technical&#34; abilities is emotional intelligence and professional maturity.  Part of me thinks it&#39;s a vastly underestimated quality in general, although perhaps I just happen to find it more important than average.  Either way, the new team seems solid on both fronts.  I&#39;m mostly impressed by the founder/CEO Raj Dutt who has shown a very personal and graceful approach.
	Strong high-stakes relationships with people in general, and coworkers in particular are a very worthy pursuit and two months in I can still vouch for the level of alignment and maturity that I haven&#39;t experienced before.
	&lt;br&gt;
	&lt;br&gt;
	&lt;img src=&#34;http://dieter.plaetinck.be/files/20150429_194521_HDR.jpg&#34; alt=&#34;Yours truly and Raj Dutt&#34; /&gt;
	&lt;br&gt;
	&lt;br&gt;
	&lt;li&gt;I (and my fiancee) want to see the world. The common conception that significant leisure travel can&#39;t be combined with hard, or even normal amounts of work seems so backwards to me.  Today more than ever, we have globalization, we are developing a sharing economy (airbnb, lyft, ...).  There&#39;s no reason we can&#39;t do great work while enjoying our personal time to the fullest.  Working remotely, and/or with remote colleagues requires more discipline but I simply only want to work with disciplined people.  Working on a fixed schedule, in a fixed office location is needlessly arcane, constrained and inefficient. It puts a damper on life.
		Almost all software companies I know see open office plans as a great thing, but it&#39;s usually an attempt at compensating for employee&#39;s poor communication skills, forcing them
		to talk to the detriment of people who need focus and get distracted.  Isolation of workers does not preclude healthy communication and collaboration.
		I get a lot more done in isolation, with sync-ups and face-to-face when appropriate, especially if it&#39;s on my schedule. Working remote is a great way to facilitate this and I&#39;m happy that raintank not only agrees with the idea, but actually encourages travel, and encourages me to follow whatever time allocation works best for me.
		I work hard but we travel where ever we want, we&#39;ll have plenty of time to spend with our families in Belgium and Cyprus.  And yet I don&#39;t think I ever worked this closely with anyone.&lt;/li&gt;
	&lt;br&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/office_shasta.jpg&#34; alt=&#34;office mount shasta&#34; /&gt;
	&lt;br&gt;
	&lt;br&gt;
&lt;li&gt;I&#39;ve always wanted to be in a company from the start and experience the &#34;true&#34; start-up feel.  I have a lot of opinions on organization, culture, and product and so I&#39;m glad to have the opportunity to make that kind of impact as well.
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;OK, so what am I really up to?&lt;/h2&gt;
&lt;p&gt;
	Grafana is &lt;a href=&#34;http://play.grafana.org/dashboard/db/stats&#34;&gt;pretty much the leading open source metrics dashboard&lt;/a&gt; right now.
So it only makes sense that raintank is a heavy Grafana user and contributor.
My work, logically, revolves around codifying some of the experience and ideas I have, and making
them accessible through the polished interface that is Grafana, which now also has a full time UX designer working on it.
Since according to the &lt;a href=&#34;https://infogr.am/grafana_user_survey_mar2015&#34;&gt;Grafana user survey&lt;/a&gt; &lt;b&gt;alerting is the most sorely missed non-feature of Grafana&lt;/b&gt;,
we are working hard on rectifying this and it is my full-time focus.  If you&#39;ve followed my blog you know I have some thoughts on where the sweet spot lies in clever alerting.
In short, take the claims of anomaly detection via machine learning with a big grain of salt, and instead, focus on enabling operators to express complex logic simply, quickly,
and in an agile way.  My latest favorite project, &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun&lt;/a&gt; exemplifies this approach (highly recommend giving this a close look).
&lt;/p&gt;
&lt;p&gt;
The way I&#39;m thinking of it now, the priorities (and sequence of focus) for alerting within Grafana will probably be something like this:
&lt;ul&gt;
	&lt;li&gt;cover the low hanging fruit: simple threshold checks with email notifications actually go a long way&lt;/li&gt;
	&lt;li&gt;gradually provide more power and sophistication (reduction functions, boolean logic, etc), while keeping things in a nice UI&lt;/li&gt;
	&lt;li&gt;provide full-on integration with advanced alerting systems such as Bosun. Iterative workflow, Signal/noise analysis, etc&lt;/li&gt;
&lt;/ul&gt;

There&#39;s a lot of thought work, UX and implementation details around this topic,
I&#39;ve created a &lt;a href=&#34;https://github.com/grafana/grafana/issues/2209&#34;&gt;github ticket&lt;/a&gt; to kick off a discussion and am curious to hear your thoughts.

Finally, if any of this sounds interesting to you, you can sign up to the &lt;a href=&#34;http://grafana.org/&#34;&gt;grafana newsletter&lt;/a&gt; or the &lt;a href=&#34;http://raintank.io/&#34;&gt;raintank newsletter&lt;/a&gt; which will get you info on the open source platform as well
as the SaaS product.  Both are fairly low volume.

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/office_sausolito.jpg&#34; alt=&#34;office sausolito&#34; /&gt;
&lt;i&gt;It may look like I&#39;m not doing much from my temporary Mill Valley office, but trust me, cool stuff is coming!&lt;/i&gt;



&lt;!--
some reminisic about meeting up with torkel and talking buziness models in pdx
 how i see myself integrating in the team:
the way I see it, there&#39;s some good, next-gen ideas and implementation in some of the things i&#39;ve built and/or have experience with (metrics2.0, graph-explorer, bosun) but clearly some of these projects (esp the former two) have been handicapped by a steep learning curve and weak UX.  Part of the reason I&#39;m excited in joining in RT is that over time I will be able to gently infuse some of these re-envisioned ideas into the grafana package, with a much higher focus on user friendlyness.
nyc skin issues
alternative funding? crowd etc


https://vimeo.com/blog/post:702
grown as backend engineer, carbon-relay-ng most fun
my various projects
contributor to graphite, influxdb, bosun, diamond, statsd, graphite-api,
https://github.com/brutasse/graphite-api
https://github.com/Dieterbe/timeserieswidget
https://github.com/Dieterbe/profile-process
http://vimeo.github.io/graph-explorer/
https://github.com/vimeo/graphite-influxdb
https://github.com/vimeo/carbon-tagger
https://github.com/vimeo/statsdaemon
https://github.com/vimeo/graphite-api-influxdb-docker
https://github.com/vimeo/whisper-to-influxdb
https://github.com/vimeo/smoketcp
https://github.com/vimeo/timeserieswidget
https://github.com/vimeo/simple-black-box
https://groups.google.com/forum/#!forum/it-telemetry

https://github.com/python-diamond
https://github.com/bosun-monitor

https://github.com/Dieterbe/anthracite
https://github.com/Dieterbe/influx-cli

https://github.com/graphite-ng/graphite-ng
https://github.com/graphite-ng/carbon-relay-ng
http://metrics20.org/
humbled by big players who saw my work and invited me to work with them
basically 2 big camps.


it seems like every week a monitoring startup launches somewhere.  There is, and will be, more and more competition.  It&#39;s almost ludicrous to start or join a new one.
--&gt;
</description>
    </item>
    
    <item>
      <title>Practical fault detection on timeseries part 2: first macros and templates</title>
      <link>http://dieter.plaetinck.be/post/practical-fault-detection-on-timeseries-part-2/</link>
      <pubDate>Mon, 27 Apr 2015 09:05:02 -0400</pubDate>
      
      <guid>practical-fault-detection-on-timeseries-part-2</guid>
      <description>&lt;h2&gt;Target use case&lt;/h2&gt;
As in the previous article, we focus on the specific category of timeseries metrics driven by user activity.
Those series are expected to fluctuate in at least some kind of (usually daily) pattern, but is expected to have a certain smoothness to it. Think web requests per second or uploads per minute.   There are a few characteristics that are considered faulty or at least worth our attention:
&lt;br/&gt;
&lt;table&gt;
&lt;tr&gt;
    &lt;td style=&#34;padding:0px;&#34; width=&#34;160px;&#34;&gt;&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-good.png&#34;&gt;&lt;img width=&#34;160px;&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-good.png&#34;/&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&#34;padding:0px;&#34; width=&#34;160px;&#34;&gt;&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-spikes.png&#34;&gt;&lt;img width=&#34;160px;&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-spikes.png&#34;/&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&#34;padding:0px;&#34; width=&#34;160px;&#34;&gt;&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-erratic.png&#34;&gt;&lt;img width=&#34;160px;&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-erratic.png&#34;/&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&#34;padding:0px;&#34; width=&#34;160px;&#34;&gt;&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-timeseries-median-drop.png&#34;&gt;&lt;img width=&#34;160px;&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-timeseries-median-drop.png&#34;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;&lt;b style=&#34;color: green;&#34;&gt;looks good&lt;/b&gt;&lt;br/&gt;consistent pattern&lt;br/&gt;consistent smoothness&lt;/td&gt;
  &lt;td&gt;&lt;b style=&#34;color: red;&#34;&gt;sudden deviation (spike)&lt;/b&gt;&lt;br/&gt;Almost always something broke or choked.&lt;br/&gt;could also be pointing up. ~ peaks and valleys&lt;/td&gt;
  &lt;td&gt;&lt;b style=&#34;color: red;&#34;&gt;increased erraticness&lt;/b&gt;&lt;br/&gt;Sometimes natural&lt;br/&gt;often result of performance issues&lt;/td&gt;
  &lt;td&gt;&lt;b style=&#34;color: red;&#34;&gt;lower values than usual&lt;/b&gt; (in the third cycle)&lt;br/&gt;Often caused by changes in code or config, sometimes innocent.  But best to alert operator in any case [*]&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;br/&gt;
[*] Note that some regular patterns can look like this as well. For example weekend traffic lower than weekdays, etc.  We see this a lot.
&lt;br/&gt;The illustrations don&#39;t portray this for simplicity.   But the alerting logic below supports this just fine by comparing to same day last week instead of yesterday, etc.


&lt;h2&gt;Introducing the new approach&lt;/h2&gt;

The &lt;a href=&#34;http://dieter.plaetinck.be/practical-fault-detection-alerting-dont-need-to-be-data-scientist.html&#34;&gt;previous article&lt;/a&gt; demonstrated using graphite to compute standard deviation.
This let us alert on the erraticness of the series in general and as a particularly interesting side-effect, on spikes up and down.
The new approach is more refined and concrete by leveraging some of bosun&#39;s and Grafana&#39;s strengths.  We can&#39;t always detect the last case above via erraticness checking (a lower amount may be introduced gradually, not via a sudden drop) so now we monitor for that as well, covering all cases above.

We use 
&lt;ul&gt;
&lt;li&gt;Bosun macros which encapsulate all the querying and processing&lt;/li&gt;
&lt;li&gt;Bosun template for notifications&lt;/li&gt;
&lt;li&gt;A generic Grafana dashboard which aids in troubleshooting&lt;/li&gt;
&lt;/ul&gt;
We can then leverage this for various use cases, as long as the expectations of the data are as outlined above.
We use this for web traffic, volume of log messages, uploads, telemetry traffic, etc.
For each case we simply define the graphite queries and some parameters and leverage the existing mentioned Bosun and Grafana configuration.
&lt;br/&gt;
&lt;p&gt;
The best way to introduce this is probably by showing how a notification looks like:
&lt;br/&gt;
&lt;center&gt;
&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-dm-notification.png&#34;&gt;&lt;img height=&#34;600px&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-dm-notification.png&#34;/&gt;&lt;/a&gt;
&lt;br/&gt;
(image redacted to hide confidential information
&lt;br/&gt;the numbers are not accurate and for demonstration purposes only)
&lt;/center&gt;
&lt;p&gt;
As you can tell by the sections, we look at some global data (for example &#34;all web traffic&#34;, &#34;all log messages&#34;, etc), and also
by data segregated by a particular dimension (for example web traffic by country, log messages by key, etc)
&lt;br/&gt;
To cover all problematic cases outlined above, we do 3 different checks:
(note, everything is parametrized so you can tune it, see further down)
&lt;ul&gt;
&lt;li&gt;Global volume: comparing the median value of the last 60 minutes or so against the corresponding 60 minutes last week and expressing it as a &#34;strength ratio&#34;.  Anything below a given threshold such as 0.8 is alerted on&lt;/li&gt;
&lt;li&gt;Global erraticness. To find all forms of erraticness (increased deviation), we use a refined formula.  See details below.  A graph of the input data is included so you can visually verify the series&lt;/li&gt;
&lt;li&gt;On the segregated data: compare current (hour or so) median against median derived from the corresponding hours during the past few weeks, and only allow a certain amount of standard deviations difference&lt;/li&gt;
&lt;/ul&gt;

If any, or multiple of these conditions are in warning or critical state, we get 1 alert that gives us all the information we need.
&lt;br/&gt;
Note the various links to GE (Graph-Explorer) and Grafana for timeshifts.
The Graph-Explorer links are just standard GEQL queries, I usually use this if i want to be easily manage what I&#39;m viewing (compare against other countries, adjust time interval, etc) because that&#39;s what GE is really good at.
The timeshift view is a Grafana dashboard that takes in a Graphite expression as a template variable, and can hence be set via a GET parameter by using the url  &lt;pre&gt;http://grafana/#/dashboard/db/templatetimeshift?var-patt=expression&lt;/pre&gt;
It shows the current past week as red dots, and the past weeks before that as timeshifts in various shades of blue representing the age of the data. (darker is older).
&lt;br/&gt;
&lt;br/&gt;

&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-screenshot-template-timeshift.png&#34;&gt;&lt;img src=&#34;http://dieter.plaetinck.be/files/practical-alerting-screenshot-template-timeshift.png&#34; /&gt;&lt;/a&gt;
&lt;br/&gt;
This allows us to easily spot when traffic becomes too low, overly erratic, etc as this example shows:
&lt;br/&gt;
&lt;br/&gt;

&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeshift-use.png&#34;&gt;&lt;img src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeshift-use.png&#34; /&gt;&lt;/a&gt;
&lt;br/&gt;

&lt;h2&gt;Getting started&lt;/h2&gt;

Note: I Won&#39;t explain the details of the bosun configuration.  Familiarity with bosun is assumed.  The &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun documentation&lt;/a&gt; is pretty complete.
&lt;br/&gt;
&lt;br/&gt;
&lt;a href=&#34;https://gist.github.com/Dieterbe/d1892fa0b4454b892216&#34;&gt;Gist with bosun macro, template, example use, and Grafana dashboard definition&lt;/a&gt;.  Load the bosun stuff in your bosun.conf and import the dashboard in Grafana.
&lt;br/&gt;
&lt;br/&gt;
The pieces fit together like so:

&lt;ul&gt;
&lt;li&gt;The alert is where we define the graphite queries, the name of the dimension segregated by (used in template), how long the periods are, what the various thresholds are and the expressions to be fed into Grafana and Graph-Explorer.
&lt;br/&gt;
It also lets you set an importance which controls the sorting of the segregated entries in the notification (see screenshot).  By default it is based on the historical median of the values but you could override this.  For example for a particular alert we maintain a lookup table with custom importance values.&lt;/li&gt;
&lt;li&gt;The macros are split in two:
&lt;ol&gt;
&lt;li&gt;dm-load loads all the initial data based on your queries and computes a bunch of the numbers.&lt;/li&gt;
&lt;li&gt;dm-logic does some final computations and evaluates the warning and critical state expressions.&lt;/li&gt;
&lt;/ol&gt;
They are split so that your alerting rule can leverage the returned tags from the queries in dm-load to use a lookup table to set the importance variable or other thresholds, such as s_min_med_diff on a case-by-case basis, before calling dm-logic.
&lt;br/&gt;
We warn if one or more segregated items didn&#39;t meet their median requirements, and if erraticness exceeds its threshold (note that the latter can be disabled).
&lt;br&gt;Critical is when more than the specified number of segregated items didn&#39;t meet their median requirements, the global volume didn&#39;t meet the strength ratio, or if erraticness is enabled and above the critical threshold.
&lt;/li&gt;
&lt;li&gt;The template is evaluated and generates the notification like shown above&lt;/li&gt;
&lt;li&gt;Links to Grafana (timeshift) and GE are generated in the notification to make it easy to start troubleshooting&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Erraticness formula refinements&lt;/h2&gt;
You may notice that the formula has changed to
&lt;pre&gt;
(deviation-now * median-historical) /
((deviation-historical * median-now) + 0.01)
&lt;/pre&gt;
&lt;img style=&#34;float: right; margin: 45px;&#34; width=&#34;40%&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-notes-cleaned-small.jpg&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;Current deviation is compared to an automatically chosen historical deviation value (so no more need to manually set this)&lt;/li&gt;
&lt;li&gt;Accounts for difference in volume: for example if traffic at any point is much higher, we can also expect the deviation to be higher.  With the previous formula we would have cases where in the past the numbers were very low, and naturally the deviation then was low and not a reasonable standard to be held against when traffic is higher, resulting in trigger happy alerting with false positives.
&lt;br/&gt;Now we give a fair weight to the deviation ratio by making it inversely proportional to the median ratio&lt;/li&gt;
&lt;li&gt;The + 0.01 is to avoid division by zero&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
streak covers cases where values are very low so that stdev is in the same order (like low volume logs) and we can&#39;t properly use the erraticness or x-deviations. for example logs with very little traffic, datapoints representing healthy traffic can look like (1, 2, 0, 3, 1, ..)
although i think the ratio of medians (median_now/median_then) should work just as well as streak

&lt;img src=&#34;http://dieter.plaetinck.be/files/practical-alerting-low-values-zeroes.png&#34; /&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/practical-alerting-low-values-low.png&#34; /&gt;
--&gt;

&lt;h2&gt;Still far from perfect&lt;/h2&gt;
While this has been very helpful to us, I want to highlight a few things that could be improved.
&lt;ul&gt;
&lt;li&gt;With these alerts, you&#39;ll find yourself wanting to iteratively fine tune the various parameters and validate the result of your changes by comparing the status-over-time timeline before and after the change.  While Bosun already makes iterative development easier and lets you &lt;a href=&#34;http://bosun.org/public/ss_rule_timeline.png&#34;&gt;run test rules against old data and look at a the status over time&lt;/a&gt;, the interface could be improved by
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bosun-monitor/bosun/issues/636&#34;&gt;showing timeseries (with event markers where relevant) alongside the status visualization&lt;/a&gt;, so you have context to interpret the status timeline&lt;/li&gt;
&lt;li&gt;routinely building &lt;a href=&#34;https://github.com/grafana/grafana/pull/1569&#34;&gt;a knowledge base of time ranges annotated with a given state for a given alerting concern, which would help in validating the generated status timeline, both visually and in code.  We could compute percentage of issues found, missed, etc&lt;/a&gt;. &#34;unit tests for alerting&#34; my boss called it.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Template could be prettier.  In particular the plots often don&#39;t render very well.  We&#39;re looking into closer Grafana-Bosun integration so I think that will be resolved at some point.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bosun-monitor/bosun/issues/719&#34;&gt;Current logic doesn&#39;t take past outages into account. &#34;just taking enough periods in graphiteBand()&#34; helps alleviate it mostly, but it&#39;s not very robust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See that drop in the screenshot a bit higher up? That one was preceded by a code deploy event in anthracite which made some changes where a drop in traffic was actually expected.  Would love to be able to mark stuff like this in deploys (like putting in the commit message something like &#34;expect 20-50 drop&#34; and have the monitoring system leverage that.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;In conclusion&lt;/h2&gt;
I know many people are struggling with poor alerting rules (static thresholds?)
&lt;br/&gt;As I explained in the previous article I fondly believe that the commonly cited solutions (anomaly detection via machine learning) are a very difficult endeavor and results can be achieved much quicker and simpler.
&lt;br/&gt;While this only focuses on one class of timeseries (it won&#39;t work on diskspace metrics for example) I found this class to be in the most dire need of better fault detection.  Hopefully this is useful to you. Good luck and let me know how it goes!

</description>
    </item>
    
    <item>
      <title>Practical fault detection &amp; alerting.  You don&#39;t need to be a data scientist</title>
      <link>http://dieter.plaetinck.be/post/practical-fault-detection-alerting-dont-need-to-be-data-scientist/</link>
      <pubDate>Thu, 29 Jan 2015 09:08:02 -0400</pubDate>
      
      <guid>practical-fault-detection-alerting-dont-need-to-be-data-scientist</guid>
      <description>&lt;br/&gt;
&lt;br/&gt;

&lt;h2&gt;It&#39;s not all about math&lt;/h2&gt;
&lt;p&gt;
I&#39;ve seen smart people who are good programmers decide to tackle anomaly detection on their timeseries metrics.
(anomaly detection is about building algorithms which spot &#34;unusual&#34; values in data, via statistical frameworks).  This is a good reason to brush up on statistics, so you can apply some of those concepts.
But ironically, in doing so, they often seem to think that they are now only allowed to implement algebraic mathematical formulas. No more if/else, only standard deviations of numbers.  No more for loops, only moving averages. And so on.
&lt;br/&gt;When going from thresholds to something (&lt;i&gt;anything&lt;/i&gt;) more advanced, suddenly people only want to work with mathematical formula&#39;s.  Meanwhile we have entire Turing-complete programming languages available, which allow us to execute any logic, as simple or as rich as we can imagine.  Using only math massively reduces our options in implementing an algorithm.
&lt;br/&gt;
&lt;br/&gt;For example I&#39;ve seen several presentations in which authors demonstrate how they try to fine-tune moving average algorithms and try to get a robust base signal to check against but which is also not affected too much by previous outliers, which raise the moving average and might mask subsequent spikes).
&lt;br/&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/fault-detection-moving-average.png&#34;&gt;
from &lt;a href=&#34;https://speakerdeck.com/astanway/a-deep-dive-into-monitoring-with-skyline&#34;&gt;A Deep Dive into Monitoring with Skyline&lt;/a&gt;
&lt;br/&gt;
&lt;br/&gt;
But you can&#39;t optimize both, because a mathematical formula at any given point can&#39;t make the distinction between past data that represents &#34;good times&#34; versus &#34;faulty times&#34;.
&lt;br/&gt;However: we wrap the output of any such algorithm with some code that decides what is a fault (or &#34;anomaly&#34; as labeled here) and alerts against it, so why would we hold ourselves back in feeding this useful information back into the algorithm?
&lt;br/&gt;I.e. &lt;b&gt;assist the math with logic&lt;/b&gt; by writing some code to make it work better for us:  In this example, we could modify the code to just retain the old moving average from before the time-frame we consider to be faulty.  That way, when the anomaly passes, we resume &#34;where we left off&#34;.  For timeseries that exhibit seasonality and a trend, we need to do a bit more, but the idea stays the same.   Restricting ourselves to only math and statistics cripples our ability to detect actual &lt;b&gt;faults&lt;/b&gt; (problems).
&lt;/p&gt;
&lt;p&gt;
Another example: During his &lt;a href=&#34;https://coderanger.net/talks/echo/&#34;&gt;Monitorama talk&lt;/a&gt;, Noah Kantrowitz made the interesting and thought provoking observation that Nagios flap detection is basically a low-pass filter.  A few people suggested re-implementing flap detection as a low-pass filter.  This seems backwards to me because reducing the problem to a pure mathematical formula loses information.  The current code has the high-resolution view of above/below threshold and can visualize as such.  Why throw that away and limit your visibility?
&lt;/p&gt;

&lt;h2&gt;Unsupervised machine learning... let&#39;s not get ahead of ourselves.&lt;/h2&gt;
&lt;a href=&#34;https://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;Etsy&#39;s Kale&lt;/a&gt; has ambitious goals: you configure a set of algorithms, and those algorithms get applied to &lt;b&gt;all&lt;/b&gt; of your timeseries.  Out of that should come insights into what&#39;s going wrong.  The premise is that the found anomalies are relevant and indicative of faults that require our attention.
&lt;br/&gt;I have quite a variety amongst my metrics.  For example diskspace metrics exhibit a sawtooth pattern (due to constant growth and periodic cleanup),
crontabs cause (by definition) periodic spikes in activity, user activity causes a fairly smooth graph which is characterized by its daily pattern and often some seasonality and a long-term trend.
&lt;br/&gt;
&lt;br/&gt;
&lt;img width=&#34;70%&#34; src=&#34;http://dieter.plaetinck.be/files/anomaly-detection-cases.png&#34;&gt;
&lt;br/&gt;
&lt;br/&gt;Because they look differently, anomalies and faults look different too.  In fact, within each category there are multiple problematic scenarios. (e.g. user activity based timeseries should not suddenly drop, but also not be significantly lower than other days, even if the signal stays smooth and follows the daily rhythm)
&lt;br/&gt;
&lt;br/&gt;I have a hard time believing that running the same algorithms on all of that data, and doing minimal configuration on them, will produce meaningful results. At least I expect a very low signal/noise ratio.  Unfortunately, of the people who I&#39;ve asked about their experiences with Kale/Skyline, the only cases where it&#39;s been useful is where skyline input has been restricted to a certain category of metrics - it&#39;s up to you do this filtering (perhaps via carbon-relay rules), potentially running multiple skyline instances - and sufficient time is required hand-selecting the appropriate algorithms to match the data.  This reduces the utility.
&lt;br/&gt;&#34;Minimal configuration&#34; sounds great but this doesn&#39;t seem to work.
&lt;br/&gt;
Instead, something like &lt;a href=&#34;http://bosun.org/&#34;&gt;Bosun&lt;/a&gt; (see further down) where you can visualize your series, experiment with algorithms and see the results in place on current and historical data, to manage alerting rules seems more practical.
&lt;br/&gt;
&lt;br/&gt;Some companies (all proprietary) take it a step further and pay tens of engineers to work on algorithms that inspect all of your series, classify them into categories, &#34;learn&#34; them and automatically configure algorithms that will do anomaly detection, so it can alert anytime something looks unusual (though not necessarily faulty).
This probably works fairly well, but has a high cost, still can&#39;t know everything there is to know about your timeseries, is of no help if your timeseries is behaving faulty from the start and still alerts on anomalous, but irrelevant outliers.
&lt;br/&gt;
&lt;br/&gt;

I&#39;m &lt;b&gt;suggesting we don&#39;t need to make it that fancy&lt;/b&gt; and we can do much better by &lt;b&gt;injecting some domain knowledge&lt;/b&gt; into our monitoring system:
&lt;ul&gt;
&lt;li&gt;using minimal work of classifying metrics via metric meta-data or rules that parse metric id&#39;s, we can automatically infer knowledge of how the series is supposed to behave (e.g. assume that disk_mb_used looks like sawtooth, frontend_requests_per_s daily seasonal, etc) and apply fitting processing accordingly.
&lt;br/&gt;Any sysadmin or programmer can do this, it&#39;s a bit of work but should make a hands-off automatic system such as Kale more accurate.
&lt;br/&gt;Of course, adopting &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics 2.0&lt;/a&gt; will help with this as well. Another problem with machine learning is they would have to infer how metrics relate against each other, whereas with metric metadata this can easily be inferred (e.g.: what are the metrics for different machines in the same cluster, etc)&lt;/li&gt;
&lt;li&gt;hooking into service/configuration management: you probably already have a service, tool, or file that knows how your infrastructure looks like and which services run where.  We know where user-facing apps run, where crontabs run, where we store log files, where and when we run cleanup jobs.  We know in what ratios traffic is balanced across which nodes, and so on.  Alerting systems can leverage this information to apply better suited fault detection rules.  And you don&#39;t need a large machine learning infrastructure for it. (as an aside: I have a lot more ideas on cloud-monitoring integration)&lt;/li&gt;
&lt;li&gt;Many scientists are working on algorithms that find cause and effect when different series exhibit anomalies, so they can send more useful alerts.  But again here, a simple model of the infrastructure gives you service dependencies in a much easier way.&lt;/li&gt;
&lt;li&gt;hook into your event tracking. If you have something like &lt;a href=&#34;https://github.com/Dieterbe/anthracite/&#34;&gt;anthracite&lt;/a&gt; that lists upcoming press releases, then your monitoring system knows not to alert if suddenly traffic is a bit higher.  In fact, you might want to alert if your announcement did not create a sudden increase in traffic.  If you have a large scale infrastructure, you might go as far as tagging upcoming maintenance windows with metadata so the monitoring knows which services or hosts will be affected (and which shouldn&#39;t).
&lt;/ul&gt;
&lt;br/&gt;
Anomaly detection is useful if you don&#39;t know what you&#39;re looking for, or providing an extra watching eye on your log data.  Which is why it&#39;s commonly used for detecting fraud in security logs and such.
For operational metrics of which admins know what they mean, should and should not look like, and how they relate to each other, we can build more simple and more effective solutions.


&lt;h2&gt;The trap of complex event processing... no need to abandon familiar tools&lt;/h2&gt;
On your quest into better alerting, you soon read and hear about real-time stream processing, and CEP (complex event processing) systems.
It&#39;s not hard to be convinced on their merits:  who wouldn&#39;t want real-time as-soon-as-the-data-arrives-you-can-execute-logic-and-fire-alerts?
&lt;br/&gt;They also come with a fairly extensive and flexible language that lets you program or compose monitoring rules using your domain knowledge.
I believe I&#39;ve heard of &lt;a href=&#34;https://storm.apache.org/&#34;&gt;storm&lt;/a&gt; for monitoring, but &lt;a href=&#34;http://riemann.io/&#34;&gt;Riemann&lt;/a&gt; is the best known of these tools that focus on open source monitoring.
It is a nice, powerful tool and probably the easiest of the CEP tools to adopt.  It can also produce very useful dashboards.
However, these tools come with their own API or language, and programming against real-time streams is quite a paradigm shift which can be hard to justify.  And while their architecture and domain specificity works well for large scale situations, these benefits aren&#39;t worth it for most (medium and small) shops I know:  it&#39;s a lot easier (albeit less efficient) to just query a datastore over and over and program in the language you&#39;re used to.  With a decent timeseries store (or one written to hold the most recent data in memory such as &lt;a href=&#34;https://github.com/dgryski/carbonmem&#34;&gt;carbonmem&lt;/a&gt;) this is not an issue, and the difference in timeliness of alerts becomes negligible!


&lt;h2&gt;An example: finding spikes&lt;/h2&gt;
Like many places, we were stuck with static thresholds, which don&#39;t cover some common failure scenarios.
So I started asking myself some questions:
&lt;br&gt;
&lt;br&gt;
&lt;center&gt;
    &lt;i&gt;which behavioral categories of timeseries do we have, what kind of issues can arise in each category,
        &lt;br/&gt;how does that look like in the data, and what&#39;s the simplest way I can detect each scenario?&lt;/i&gt;
&lt;/center&gt;
&lt;br/&gt;
Our most important data falls within the user-driven category from above where various timeseries from across the stack are driven by, and reflect user activity.  And within this category, the most common problem (at least in my experience) is spikes in the data.  I.e. a sudden drop in requests/s or a sudden spike in response time.  As it turned out, this is much easier to detect than one might think:
&lt;br/&gt;
&lt;img style=&#34;float:left; margin:15px;&#34; src=&#34;http://dieter.plaetinck.be/files/poor-mans-fault-detection.png&#34;&gt;
&lt;br/&gt;
In this example I just track the standard deviation of a moving window of 10 points.  &lt;a href=&#34;http://en.wikipedia.org/wiki/Standard_deviation&#34;&gt;Standard deviation&lt;/a&gt; is simply a measure of how much numerical values differ from each other.  Any sudden spike bumps the standard deviation.   We can then simply set a threshold on the deviation.  Fairly trivial to set up, but has been highly effective for us.
&lt;br/&gt;
&lt;br/&gt;You do need to manually declare what is an acceptable standard deviation value to be compared against, which you will typically deduce from previous data.  This can be annoying until you build an interface to speed up, or a tool to automate this step.
&lt;br/&gt;In fact, it&#39;s trivial to collect previous deviation data (e.g. from the same time of the day, yesterday, or the same time of the week, last week) and automatically use that to guide a threshold.  (&lt;a href=&#34;http://bosun.org/&#34;&gt;Bosun&lt;/a&gt; - see the following section - has &#34;band&#34; and &#34;graphiteBand&#34; functions to assist with this).  This is susceptible to previous outliers, but you can easily select multiple previous timeframes to minimize this issue in practice.
&lt;br/&gt;
&lt;a href=&#34;https://groups.google.com/forum/#!topic/it-telemetry/Zb2H4DP6qtk&#34;&gt;it-telemetry thread&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
So without requiring fancy anomaly detection, machine learning, advanced math, or event processing, we are able to reliably detect faults using simple, familiar tools.  Some basic statistical concepts (standard deviation, moving average, etc) are a must, but nothing that requires getting a PhD.  In this case I&#39;ve been using &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.10/functions.html#graphite.render.functions.stdev&#34;&gt;Graphite&#39;s stdev function&lt;/a&gt; and &lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer&#39;s&lt;/a&gt; alerting feature to manage these kinds of rules, but it doesn&#39;t allow for a very practical iterative workflow, so the non-trivial rules will be going into &lt;a href=&#34;http://bosun.org/&#34;&gt;Bosun&lt;/a&gt;.
&lt;br/&gt;BTW, you can also &lt;a href=&#34;http://obfuscurity.com/2012/05/Polling-Graphite-with-Nagios&#34;&gt;use a script to query Graphite from a Nagios check and do your logic&lt;/a&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;

&lt;!--
divideSeries(stdev(avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)),10),avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)))

why keepLastValue?
* sumSeries -&gt; none counts as 0, so you can experience big drops which would trigger anomaly detection or failover
* averageSeries -&gt; effectively ignores none values, so your accuracy can drop a lot in light of none values.

of course this masks when your monitoring breaks, so you still need something else to detect anomalies in the &#34;out-of-date-ness&#34; of your points.
--&gt;

&lt;h2&gt;Workflow is key.  A closer look at bosun&lt;/h2&gt;
One of the reasons we&#39;ve been chasing self-learning algorithms is that we have lost faith in the feasibility of a more direct approach.  We can no longer imagine building and maintaining alerting rules because we have no system that provides powerful alerting, helps us keep oversight and streamlines the process of maintaining and iteratively developing alerting.
&lt;br/&gt;I recently discovered &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun&lt;/a&gt;, an alerting frontend (&#34;IDE&#34;) by Stack Exchange, &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/brandt&#34;&gt;presented at Lisa14&lt;/a&gt;.  I highly recommend watching the video.  They have identified various issues that made alerting a pain, and built a solution that makes human-controlled alerting much more doable.  We&#39;ve been using it for a month now with good results (I also gave it support for Graphite).

I&#39;ll explain its merits, and it&#39;ll also become apparent how this ties into some of the things I brought up above:
&lt;img style=&#34;float:left; margin:35px;&#34; src=&#34;http://bosun.org/public/ss_rule_timeline.png&#34; width=&#34;15%&#34;&gt;
&lt;ul&gt;
&lt;li&gt;in each rule you can query any data you need from any of your datasources (currently graphite, openTSDB, and elasticsearch).  You can call various &lt;a href=&#34;http://bosun.org/configuration.html&#34;&gt;functions, boolean logic, and math&lt;/a&gt;.  Although it doesn&#39;t expose you a full programming language, the bosun language as it stands is fairly complete, and can be extended to cover
new needs.  You choose your own alerting granularity (it can automatically instantiate alerts for every host/service/$your_dimension/... it finds within your metrics, but you can also trivially aggregate across dimensions, or both).  This makes it easy to create advanced alerts that cover a lot of ground, making sure you don&#39;t get overloaded by multiple smaller alerts.  And you can incorporate data of other entities within the system, to simply make better alerting decisions.&lt;/li&gt;
&lt;li&gt;you can define your own templates for alert emails, which can contain any html code.  You can trivially plot graphs, build tables, use colors and so on.  Clear, context-rich alerts which contain all information you need!&lt;/li&gt;
&lt;li&gt;As alluded to, the bosun authors spent a lot of time &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/brandt&#34;&gt;thinking about, and solving&lt;/a&gt; the workflow of alerting.  As you work on advanced fault detection and alerting rules you need to be able to see the value of all data (including intermediate computations) and visualize it.  Over time, you will iteratively adjust the rules to become better and more precise.  Bosun supports all of this.  You can execute your rules on historical data and see exactly how the rule performs over time, by displaying the status in a timeline view and providing intermediate values.  And finally, you can see how the alert emails will be rendered &lt;i&gt;as you work on the rule and the templates&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;

The &lt;a href=&#34;http://bosun.org/examples.html&#34;&gt;examples&lt;/a&gt; section gives you an idea of the things you can do.
&lt;br/&gt;I haven&#39;t seen anything solve a pragmatic alerting workflow like bosun (hence their name &#34;alerting IDE&#34;), and its ability to not hold you back as you work on your alerts is refreshing. Furthermore, the built-in processing functions are very &lt;b&gt;complimentary to graphite&lt;/b&gt;:
Graphite has a decent API which works well at aggregating and transforming one or more series into one new series; the bosun language is great at reducing series to single numbers, providing boolean logic, and so on, which you need to declare alerting expressions.  This makes them a great combination.
&lt;br/&gt;Of course Bosun isn&#39;t perfect either.  Plenty of things can be done to make it (and alerting in general) better.  But it does exemplify many of my points, and it&#39;s a nice leap forward in our monitoring toolkit.

&lt;h2&gt;Conclusion&lt;/h2&gt;
Many of us aren&#39;t ready for some of the new technologies, and some of the technology isn&#39;t - and perhaps never will be - ready for us.
As an end-user investigating your options, it&#39;s easy to get lured in a direction that promotes over-complication and stimulates your inner scientist but just isn&#39;t realistic.
&lt;br/&gt;Taking a step back, it becomes apparent we &lt;b&gt;can&lt;/b&gt; setup automated fault detection.  But instead of using machine learning, use metadata, instead of trying to come up with all-encompassing holy grail of math, use several rules of code that you prototype and iterate over, then reuse for similar cases.  Instead of requiring a paradigm shift, use a language you&#39;re familiar with.  Especially by polishing up the workflow, we can make many &#34;manual&#34; tasks much easier and quicker.  I believe we can keep polishing up the workflow, distilling common patterns into macros or functions that can be reused, leveraging metric metadata and other sources of truth to configure fault detection, and perhaps even introducing &#34;metrics coverage&#34;, akin to &#34;code coverage&#34;: verify how much, and which of the metrics are adequately represented in alerting rules, so we can easily spot which metrics have yet to be included in alerting rules.  I think there&#39;s a lot of things we can do to make fault detection work better for us, but we have to look in the right direction.

&lt;h2&gt;PS: leveraging metrics 2.0 for anomaly detection&lt;/h2&gt;
In my last &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/plaetinck&#34;&gt;metrics 2.0 talk, at LISA14&lt;/a&gt; I explored a few ideas on leveraging metrics 2.0 metadata for alerting and fault detection, such as automatically discovering error metrics across the stack, getting high level insights via tags, correlation, etc. If you&#39;re interested, it&#39;s in the video from 24:55 until 29:40
&lt;br/&gt;
&lt;br/&gt;
&lt;center&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/metrics20-alerting.png&#34; width=&#34;50%&#34;&gt;
&lt;/center&gt;

&lt;!--
It&#39;s not about alerts anyway.

alerts are an immensely crude approach to raising operator awareness.
They are basically boolean: either they interrupt your workflow or they don&#39;t.  There&#39;s no in between.
Yes, you can just check your alert emails &#34;once in a while&#34;, but then realize that after an email or text is sent,
there is no way to update them with new information.  Which is really limiting once you start thinking about it.
Updates have to be provided via new &#34;alerts&#34;, or they are available in the monitoring interface but there&#39;s no way to tell
by just glancing at your alert overview.  You might be looking at very out of date information.
-&gt; only sent alerts for critical things.
--&gt;
</description>
    </item>
    
    <item>
      <title>IT-Telemetry Google group.  Trying to foster more collaboration around operational insights.</title>
      <link>http://dieter.plaetinck.be/post/it-telemetry-google-group-collaboration-operational-insights/</link>
      <pubDate>Sat, 06 Dec 2014 16:01:02 -0400</pubDate>
      
      <guid>it-telemetry-google-group-collaboration-operational-insights</guid>
      <description>The discipline of collecting infrastructure &amp; application performance metrics, aggregation, storage, visualizations and alerting has many terms associated with it...  Telemetry. Insights engineering.  Operational visibility.
I&#39;ve seen a bunch of people present their work in advancing the state of the art in this domain:  
&lt;br/&gt;from &lt;a href=&#34;http://mabrek.github.io/&#34;&gt;Anton Lebedevich&#39;s statistics for monitoring series&lt;/a&gt;, &lt;a href=&#34;https://vimeo.com/95069158&#34;&gt;Toufic Boubez&#39; talks on anomaly detection&lt;/a&gt; and Twitter&#39;s work on &lt;a href=&#34;https://blog.twitter.com/2014/breakout-detection-in-the-wild&#34;&gt;detecting mean shifts&lt;/a&gt; to projects such as &lt;a href=&#34;http://flapjack.io/&#34;&gt;flapjack&lt;/a&gt; (which aims to offload the alerting responsibility from your monitoring apps), the &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics 2.0 standardization effort&lt;/a&gt; or &lt;a href=&#34;https://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;Etsy&#39;s Kale stack&lt;/a&gt; which tries to bring interesting changes in timeseries to your attention with minimal configuration.
&lt;br/&gt;
&lt;br/&gt;

Much of this work is being shared via conference talks and blog posts, especially around anomaly and fault detection, and I couldn&#39;t find a location for collaboration, quicker feedback and discussions on more abstract (algorithmic/mathematical) topics or those that cross project boundaries.  So I created the &lt;a href=&#34;https://groups.google.com/forum/#!forum/it-telemetry&#34;&gt;IT-telemetry&lt;/a&gt; Google group.  If I missed something existing, let me know.  I can shut this down and point to whatever already exists.  Either way I hope this kind of avenue proves useful to people working on these kinds of problems.
</description>
    </item>
    
    <item>
      <title>A real whisper-to-InfluxDB program.</title>
      <link>http://dieter.plaetinck.be/post/a-real-whisper-to-influxdb-program/</link>
      <pubDate>Tue, 30 Sep 2014 08:37:48 -0400</pubDate>
      
      <guid>a-real-whisper-to-influxdb-program</guid>
      <description>The &lt;a href=&#34;http://dieter.plaetinck.be/graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay.html&#34;&gt;whisper-to-influxdb migration script&lt;/a&gt; I posted earlier is pretty bad.  A shell script, without concurrency, and an undiagnosed performance issue.
I hinted that one could write a Go program using the unofficial &lt;a href=&#34;https://github.com/kisielk/whisper-go&#34;&gt;whisper-go&lt;/a&gt; bindings and the &lt;a href=&#34;https://github.com/influxdb/influxdb/tree/master/client&#34;&gt;influxdb Go client library&lt;/a&gt;.
That&#39;s what I did now, it&#39;s at &lt;a href=&#34;https://github.com/vimeo/whisper-to-influxdb&#34;&gt;github.com/vimeo/whisper-to-influxdb&lt;/a&gt;.
It uses configurable amounts of workers for both whisper fetches and InfluxDB commits,
but it&#39;s still a bit naive in the sense that it commits to InfluxDB one serie at a time, irrespective of how many records are in it.
My series, and hence my commits have at most 60k records, and presumably InfluxDB could handle a lot more per commit, so we might leverage better batching later.  Either way, this way I can consistently commit about 100k series every 2.5 hours (or 10/s), where each serie has a few thousand points on average, with peaks up to 60k points. I usually play with 1 to 30 InfluxDB workers. 
Even though I&#39;ve hit a few &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/985&#34;&gt;InfluxDB&lt;/a&gt; &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/970&#34;&gt;issues&lt;/a&gt;, this tool has enabled me to fill in gaps after outages and to do a restore from whisper after a complete database wipe.

</description>
    </item>
    
    <item>
      <title>InfluxDB as a graphite backend, part 2</title>
      <link>http://dieter.plaetinck.be/post/influxdb-as-graphite-backend-part2/</link>
      <pubDate>Wed, 24 Sep 2014 07:56:01 -0400</pubDate>
      
      <guid>influxdb-as-graphite-backend-part2</guid>
      <description>&lt;h4&gt;Progress made&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;InfluxDB saw two major releases:
&lt;ul&gt;
&lt;li&gt;0.7 (and followups), which was mostly about some needed features and bug fixes&lt;/li&gt;
&lt;li&gt;0.8 was all about bringing some major refactorings in the hands of early adopters/testers: support for multiple storage engines, configurable shard spaces, rollups and retention schemes. There was some other useful stuff like speed and robustness improvements for the graphite input plugin (by yours truly) and various things like regex filtering for &#39;list series&#39;.  Note that a bunch of older bugs remained open throughout this release (most notably the broken &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/334&#34;&gt;derivative aggregator&lt;/a&gt;), and a bunch of new ones appeared. Maybe this is why the release was mostly in the dark.  In this context, it&#39;s not so bad, because we let graphite-api do all the processing, but if you want to query InfluxDB directly you might hit some roadblocks.&lt;/li&gt;
&lt;li&gt;An older fix, but worth mentioning: series names can now also contain any character, which means you can easily use &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics2.0&lt;/a&gt; identifiers.  This is a welcome relief after having struggled with Graphite&#39;s restrictions on metric keys.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://graphite-api.readthedocs.org&#34;&gt;graphite-api&lt;/a&gt; received various bug fixes and support for templating, statsd instrumentation and caching.
&lt;br/&gt;Much of this was driven by graphite-influxdb: the caching allows us to cache metadata and the statsd integration gives us insights into the performance of the steps it goes through of building a graph (getting metadata from InfluxDB, querying InfluxDB, interacting with cache, post processing data, etc).&lt;/li&gt;
&lt;li&gt;the progress on InfluxDB and graphite-api in turn enabled &lt;a href=&#34;https://github.com/vimeo/graphite-influxdb&#34;&gt;graphite-influxdb&lt;/a&gt; to become faster and simpler (note: graphite-influxdb requires InfluxDB 0.8).  Furthermore you can now configure series resolutions (but different retentions per serie is on the roadmap, see &lt;i&gt;State of affairs and what&#39;s coming&lt;/i&gt;), and of course it also got a bunch of bugfixes.&lt;/li&gt;
&lt;/ul&gt;
Because of all these improvements, all involved components are now ready for serious use.

&lt;h4&gt;Putting it all together, with docker&lt;/h4&gt;
&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; probably needs no introduction, it&#39;s a nifty tool to build an environment with given software installed, and allows to easily deploy it and run it in isolation.
&lt;a href=&#34;https://github.com/vimeo/graphite-api-influxdb-docker&#34;&gt;graphite-api-influxdb-docker&lt;/a&gt; is a very creatively named project that generates the - also very creatively named - docker image &lt;a href=&#34;https://registry.hub.docker.com/u/vimeo/graphite-api-influxdb/&#34;&gt;graphite-api-influxdb&lt;/a&gt;, which contains graphite-api and graphite-influxdb, making it easy to hook in a customized configuration and get it up and running quickly.  This is the recommended way to set this up, and this is what we run in production.

&lt;h4&gt;The setup&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;a server running InfluxDB and graphite-api with graphite-influxdb via the docker approach described above:
&lt;pre&gt;
dell PowerEdge R610
24 x Intel(R) Xeon(R) X5660  @ 2.80GHz
96GB RAM
perc raid h700
6x600GB seagate 10k rpm drives in raid10 = 1.6 TB, Adaptive Read Ahead, Write Back, 64 kB blocks, no read caching
no sharding/shard spaces, compiled from git just before 0.8, using LevelDB (not rocksdb, which is now the default)
LevelDB max-open-files = 10000 (lsof shows about 30k open files total for the InfluxDB process), LRU 4096m, everything else is default I think.
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;a server running graphite-web, carbon, and whisper:
&lt;pre&gt;
dell PowerEdge R710
16 x Intel(R) Xeon(R) E5640  @ 2.67GHz
96GB RAM
perc raid h700
8x150GB seagate 15k rm in raid5 = 952 GB, Read Ahead, Write Back, 64 kB blocks, no read caching
MAX_UPDATES_PER_SECOND = 1000  # to sequentialize writes
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;a relay server running carbon-relay-ng that sends the same production load into both.  (about 2500 metrics/s, or 150k minutely)&lt;/li&gt;
&lt;/ul&gt;
As you can tell, on both machines RAM is vastly over provisioned, and they have lots of cpu available (the difference in cores should be negligible), but the difference in RAID level is important to note: RAID 5 comes with a write penalty. Even though the whisper machine has more, and faster disks, it probably has a disadvantage for writes.  Maybe.  Haven&#39;t done raid stuff in a long time, and I haven&#39;t it measured it out.
&lt;br/&gt;&lt;b&gt;Clearly you&#39;ll need to take the results with a grain of salt, as unfortunately I do not have 2 systems available with the same configuration and their baseline (raw) performance is unknown.&lt;/b&gt;.
&lt;br/&gt;Note: no InfluxDB clustering, see &lt;i&gt;State of affairs and what&#39;s coming&lt;/i&gt;.

&lt;h4&gt;The empirical validation &amp;amp; migration&lt;/h4&gt;
Once everything was setup and I could confidently send 100% of traffic to InfluxDB via carbon-relay-ng, it was trivial to run our dashboards with a flag deciding which server to go to.
This way I have literally been running our graphite dashboards next to each other, allowing us to compare both stacks on:
&lt;ul&gt;
&lt;li&gt;visual differences: after a bunch of work and bug fixing, we got to a point where both dashboards looked almost exactly the same.  (note that graphite-api&#39;s implementation of certain functions can behave slightly different, see for example this &lt;a href=&#34;https://github.com/brutasse/graphite-api/issues/66&#34;&gt;divideSeries bug&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;speed differences by simply refreshing both pages and watching the PNGs load, with some assistance from firebug&#39;s network requests profiler.  The difference here was big: graphs served up by graphite-api + InfluxDB loaded considerably faster.  A page with 40 graphs or so would load in a few seconds instead of 20-30 seconds (on both first, as well as subsequent hits).  This is for our default, 6-hour timeframe views.  When cranking the timeframes up to a couple of weeks, graphite-api + InfluxDB was still faster.&lt;/li&gt;
&lt;/ul&gt;
Soon enough my colleagues started asking to make graphite-api + InfluxDB the default, as it was much faster in all common cases.  I flipped the switch and everybody has been happy.
&lt;br/&gt;
&lt;br/&gt;
When loading a page with many dashboards, the InfluxDB machine will occasionally spike up to 500% cpu, though I rarely get to see any iowait (!), even after syncing the block cache (i just realized it&#39;ll probably still use the cache for reads after sync?)
&lt;br/&gt;The carbon/whisper machine, on the other hand, is always fighting iowait, which could be caused by the raid 5 write amplification but the random io due to the whisper format probably has more to do with it.  Via the MAX_UPDATES_PER_SECOND I&#39;ve tried to linearize writes, with mixed success.  But I&#39;ve never gone to deep into it.  So basically &lt;b&gt;comparing write performance would be unfair in these circumstances, I am only comparing reads in these tests&lt;/b&gt;.  Despite the different storage setups, the Linux block cache should make things fair for reads.   Whisper&#39;s iowait will handicap the reads, but I always did successive runs with fully loaded PNGs to make sure the block cache was warm for reads.

&lt;h4&gt;A &#34;slightly more professional&#34; benchmark&lt;/h4&gt;
I could have stopped here, but the validation above was not very scientific.  I wanted to do a somewhat more formal benchmark, to measure read speeds (though I did not have much time so it had to be quick and easy).
&lt;br/&gt;I wanted to compare InfluxDB vs whisper, and specifically how performance scales as you play with parameters such as number of series, points per series, and time range fetched (i.e. amount of points).  I &lt;a href=&#34;https://groups.google.com/forum/#!topic/influxdb/0VeUQCqzgVg&#34;&gt;posted the benchmark on the InfluxDB mailing list&lt;/a&gt;.  Look there for all information. I just want to reiterate the conclusion here:  I was surprised.  Because of the results above, I had assumed that InfluxDB would perform reads noticeably quicker than whisper but this is not the case.  (maybe because whisper reads are nicely sequential - it&#39;s mostly writes that suffer from the whisper format)
&lt;br/&gt;This very much contrasts my earlier findings where the graphite-api+InfluxDB powered dashboards clearly take the lead.  I have yet to figure out why this is.  Maybe something to do with the performance of graphite-web vs graphite-api itself, gunicorn vs apache, worker configuration, or maybe InfluxDB only starts outperforming whisper as concurrency increases.  Some more investigation is definitely needed!

&lt;h4&gt;Future benchmarks&lt;/h4&gt;
The simple benchmark above was very simple to execute, as it only requires influx-cli and whisper-fetch (so you can easily check for yourself), but clearly there is a need to test more realistic scenarios with concurrent reads, and doing some write benchmarks would be nice too.
&lt;br/&gt;We should also look into cpu and memory usage.  I have had the luxury of being able to completely ignore memory usage, but others seem to notice excessive InfluxDB memory usage.
&lt;br/&gt;conclusion: many tests and benchmarks should happen, but I don&#39;t really have time to conduct them.  Hopefully other people in the community will take this on.

&lt;h4&gt;Disk space efficiency&lt;/h4&gt;
Last time I checked, using LevelDB I was pretty close to 24B per record (which makes sense because time, seq_no and value are all 64bit values, and each record has those 3 fields).  (this was with snappy compression enabled, so it didn&#39;t seem to give much benefit).
&lt;br/&gt;Whisper seems to consume 12 Bytes per record - a 32bit timestamp and a 64bit float value - making it considerably more storage efficient than InfluxDB/levelDB for now.
&lt;br/&gt;Some notes on this though:
&lt;ul&gt;
&lt;li&gt;whisper explicitly encodes None values, with InfluxDB those are implied (and require no space).  We have some clusters of metrics that have very sparse data, so whisper gives us a lot of overhead here, but this is different for everyone.  (note: Ceres should also be better at handling sparse data)&lt;/li&gt;
&lt;li&gt;Whisper and Influxdb both explictly encode the timestamp for every record.  Influxdb uses 64bit so you can do very high resolution (up to microseconds), whisper is limited to per-second data.  Ceres AFAIK doesn&#39;t explicitly encode the timestamp at every record, which should also give it a space advantage.&lt;/li&gt;
&lt;li&gt;I&#39;ve been using a data format in InfluxDB where every record is timestamp-sequence_number-value.  It currently works best overall, and so that&#39;s how the graphite ingestion plugin stores it and the graphite-influxdb plugin queries for it.  But it exacerbates the overhead of the timestamp and sequence number.
&lt;br/&gt;We could technically use a row format where we use more variables as part of the record, storing them as columns instead of separate series, which would improve this dynamic (but currently comes with a big tradeoff in performance characteristics - see the &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/582&#34;&gt;column indexes&lt;/a&gt; ticket).
&lt;br/&gt;Another thing is that we could technically come up with a storage format for InfluxDB that is optimized for even-spaced metrics, it wouldn&#39;t need sequence numbers, and timestamps could be implicit instead of explicit, saving a lot of space.  We could even go further and introduce types (int, etc) for values which would consume even less space.
&lt;/ul&gt;
&lt;br/&gt;
It would be great if somebody with more Ceres experience could chip in here, as - in the context of space efficiency - it looks like a neat little format.
Also, I&#39;m probably not making proper use of the compression features that InfluxDB&#39;s storage engines support.  This also requires some more looking into.


&lt;h4&gt;State of affairs and what&#39;s coming&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;InfluxDB typically performs pretty well, but not in all cases.  More validation is needed. It wouldn&#39;t surprise me at this point if tools like hbase/Cassandra/riak clearly outperform InfluxDB, as long as we keep in mind that InfluxDB is a young project.  A year, or two, from now, it&#39;ll probably perform much better. (and then again, it&#39;s not all about raw performance.  InfluxDB&#39;s has other strengths)&lt;/li&gt;
&lt;li&gt;A long time goal which is now a reality:  &lt;b&gt;You can use any Graphite dashboard on top of InfluxDB, as long as the data is stored in a graphite-compatible format.&lt;/b&gt;.  Again, the easiest to get running is via &lt;a href=&#34;https://github.com/vimeo/graphite-api-influxdb-docker&#34;&gt;graphite-api-influxdb-docker&lt;/a&gt;.  There are two issues to be mentioned, though:
&lt;ul&gt;
&lt;li&gt;graphite-influxdb needs to query InfluxDB for metadata, and this &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/884&#34;&gt;can be slow&lt;/a&gt;.  If you have millions of metrics, it can take tens of seconds before querying for the data even starts.  I am trying to work with the InfluxDB people on a solution.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/brutasse/graphite-api/issues/57&#34;&gt;graphite-api doesn&#39;t work with metric id&#39;s that have equals signs in them&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;li&gt;With the 0.8 release out the door, the shard spaces/rollups/retention intervals feature will start stabilizing, so we can start supporting multiple retention intervals per metric&lt;/li&gt;
&lt;li&gt;Because InfluxDB clustering is &lt;a href=&#34;https://github.com/influxdb/influxdb/pull/903&#34;&gt;undergoing major changes&lt;/a&gt;, and because clustering is not a high priority for me, I haven&#39;t needed to worry about this.  I&#39;ll probably only start looking at clustering somewhere in 2015 because I have more pressing issues.&lt;/li&gt;
&lt;li&gt;Once the new clustering system and the storage subsystem have matured (sounds like a v1.0 ~ v1.2 to me) we&#39;ll get more speed improvements and robustness.  Most of the integration work is done, it&#39;s just a matter of doing smaller improvements, bug fixes and waiting for InfluxDB to become better.  Maintaining this stack aside, I personally will start focusing more on:
    &lt;ul&gt;
    &lt;li&gt;per-second resolution in our data feeds, and potentially storage&lt;/li&gt;
    &lt;li&gt;realtime (but basic) anomaly detection, realtime graphs for some key timeseries.  Adrian Cockcroft had an inspirational piece in his &lt;a href=&#34;https://vimeo.com/95064249&#34;&gt;Monitorama keynote&lt;/a&gt; about how alerts from timeseries should trigger within seconds.&lt;/li&gt;
    &lt;li&gt;Mozilla&#39;s awesome &lt;a href=&#34;http://hekad.readthedocs.org&#34;&gt;heka&lt;/a&gt; project (this &lt;a href=&#34;https://vimeo.com/98689689&#34;&gt;heka video&lt;/a&gt; is great), which should help a lot with the above.  Also looking at &lt;a href=&#34;http://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;Etsy&#39;s kale stack&lt;/a&gt; for anomaly detection&lt;/li&gt;
    &lt;li&gt;metrics 2.0 and making sure metrics 2.0 works well with InfluxDB.  Up to now I find the series / columns as a data model too limiting and arbitrary, it could be so much more powerful, ditto for the query language.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can we do anything else to make InfluxDB (+graphite) faster? Yes!
&lt;ul&gt;
&lt;li&gt;Long term, of course, InfluxDB should have powerful enough processing functions and query syntax, so that we don&#39;t even need a graphite layer anymore.&lt;/li&gt;
&lt;li&gt;A storage engine optimized for fixed intervals would probably help, timestamps and sequence numbers currently consume 2/3 of the record... and there&#39;s no reason to explicitly store either one in this use case.  I&#39;ve even rarely seen people make use of the sequence number in any other InfluxDB use case.  See all the remarks in the &lt;i&gt;Disk space efficiency&lt;/i&gt; section above.  Finally we could have InfluxDB have fill in None values without it doing &#34;group by&#34; (timeframe consolidation), which would shave off runtime overhead.&lt;/li&gt;
&lt;li&gt;Then of course, there are projects to replace graphite-web/graphite-api with a Go codebase: &lt;a href=&#34;https://github.com/graphite-ng/graphite-ng&#34;&gt;graphite-ng&lt;/a&gt; and &lt;a href=&#34;https://github.com/dgryski/carbonapi&#34;&gt;carbonapi&lt;/a&gt;.  the latter is more production ready, but depends on some custom tooling and io using protobufs.  But it performs an order of magnitude better than the python api server!  I haven&#39;t touched graphite-ng in a while, but hopefully at some point I can take it up again&lt;/li&gt;
&lt;/ul&gt;
&lt;li&gt;Another thing to keep in mind when switching to graphite-api + InfluxDB: you loose the graphite composer.  I have a few people relying on this, so I can either patch it to talk to graphite-api (meh), separate it out (meh) or replace it with a nicer dashboard like tessera, grafana or descartes.  (or Graph-Explorer, but it can be a bit too much of a paradigm shift).&lt;/li&gt;
&lt;li&gt;some more InfluxDB stuff I&#39;m looking forward to:
&lt;ul&gt;
&lt;li&gt;binary protocol and result streaming (faster communication and responses!) (the latter might not get implemented though)&lt;/li&gt;
&lt;li&gt;&#34;list series&#34; speed improvements (if metadata querying gets fast enough, we won&#39;t need ES anymore for metrics2.0 index)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/influxdb/influxdb/pull/635&#34;&gt;InfluxDB instrumentation&lt;/a&gt; so we actually start getting an idea of what&#39;s going on in the system, a lot of the testing and troubleshooting is still in the dark.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tracking exceptions in graphite-api is &lt;a href=&#34;https://github.com/brutasse/graphite-api/search?q=exception&amp;type=Issues&amp;utf8=%E2%9C%93&#34;&gt;much harder than it should be&lt;/a&gt;.  Currently there&#39;s no way to display exceptions to the user (in the http response) or to even log them.  So sometimes you&#39;ll get http 500 responses and don&#39;t know why.  You can use the &lt;a href=&#34;http://graphite-api.readthedocs.org/en/latest/configuration.html#extra-sections&#34;&gt;sentry integration&lt;/a&gt; which works all right, but is clunky.  Hopefully this will be addressed soon.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Conclusion&lt;/h4&gt;
The graphite-influxdb stack works and is ready for general consumption.  It&#39;s easy to install and operate, and performs well.
It is expected that InfluxDB will over time mature and ultimately meet all my &lt;a href=&#34;http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html&#34;&gt;requirements of the ideal backend&lt;/a&gt;.  It definitely has a long way to go.  More benchmarks and tests are needed.  Keep in mind that we&#39;re not doing large volumes of metrics. For small/medium shops this solution should work well, but on larger scales you will definitely run into issues.  You might conclude that InfluxDB is not for you (yet) (there are alternative projects, after all).
&lt;br/&gt;
&lt;br/&gt;
Finally, a closing thought:
&lt;br/&gt;&lt;i&gt;Having graphs and dashboards that look nice and load fast is a good thing to have, but keep in mind that graphs and dashboards should be a last resort.  It&#39;s a solution if all else fails.  The fewer graphs you need, the better you&#39;re doing.
&lt;br/&gt;How can you avoid needing graphs?  Automatic alerting on your data.
&lt;br/&gt;
&lt;br/&gt;I see graphs as a temporary measure: they provide headroom while you develop an understanding of the operational behavior of your infrastructure, conceive a model of it, and implement the alerting you need to do troubleshooting and capacity planning.  Of course, this process consumes more resources (time and otherwise), and these expenses are not always justifiable, but I think this is the ideal case we should be working towards.&lt;/i&gt;

&lt;br/&gt;
&lt;br/&gt;
Either way, good luck and have fun!
</description>
    </item>
    
    <item>
      <title>Graphite &amp; Influxdb intermezzo: migrating old data and a more powerful carbon relay</title>
      <link>http://dieter.plaetinck.be/post/graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay/</link>
      <pubDate>Sat, 20 Sep 2014 15:18:32 -0400</pubDate>
      
      <guid>graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay</guid>
      <description>&lt;h4&gt;Migrating data from whisper into InfluxDB&lt;/h4&gt;

&lt;i&gt;&#34;How do i migrate whisper data to influxdb&#34;&lt;/i&gt; is a question that comes up regularly, and I&#39;ve always replied it should be easy to write a tool
to do this.  I personally had no need for this, until a recent small influxdb outage where I wanted to sync data from our backup server (running graphite + whisper) to influxdb, so I wrote a script:

{{&lt; highlight &#34;bash&#34; &#34;style=default&#34; &gt;}}
#!/bin/bash
# whisper dir without trailing slash.
wsp_dir=/opt/graphite/storage/whisper
start=$(date -d &#39;sep 17 6am&#39; +%s)
end=$(date -d &#39;sep 17 12pm&#39; +%s)
db=graphite
pipe_path=$(mktemp -u)
mkfifo $pipe_path
function influx_updater() {
    influx-cli -db $db -async &lt; $pipe_path
}
influx_updater &amp;
while read wsp; do
  series=$(basename ${wsp//\//.} .wsp)
  echo &#34;updating $series ...&#34;
  whisper-fetch.py --from=$start --until=$end $wsp_dir/$wsp.wsp | grep -v &#39;None$&#39; | awk &#39;{print &#34;insert into \&#34;&#39;$series&#39;\&#34; values (&#34;$1&#34;000,1,&#34;$2&#34;)&#34;}&#39; &gt; $pipe_path
done &lt; &lt;(find $wsp_dir -name &#39;*.wsp&#39; | sed -e &#34;s#$wsp_dir/##&#34; -e &#34;s/.wsp$//&#34;)
{{&lt; /highlight &gt;}}&lt;p&gt;


It relies on the recently introduced asynchronous inserts feature of &lt;a href=&#34;https://github.com/Dieterbe/influx-cli&#34;&gt;influx-cli&lt;/a&gt; - which commits inserts in batches to improve the speed - and the whisper-fetch tool.
&lt;br/&gt;
You could probably also write a Go program using the unofficial &lt;a href=&#34;https://github.com/kisielk/whisper-go&#34;&gt;whisper-go&lt;/a&gt; bindings and the &lt;a href=&#34;https://github.com/influxdb/influxdb/tree/master/client&#34;&gt;influxdb Go client library&lt;/a&gt;.  But I wanted to keep it simple.  Especially when I found out that whisper-fetch is not a bottleneck: starting whisper-fetch, and reading out - in my case - 360 datapoints of a file always takes about 50ms, whereas InfluxDB at first only needed a few ms to flush hundreds of records, but that soon increased to seconds.
&lt;br/&gt;Maybe it&#39;s a bug in my code, I didn&#39;t test this much, because I didn&#39;t need to; but people keep asking for a tool so here you go.  Try it out and maybe you can fix a bug somewhere.  Something about the write performance here must be wrong.

&lt;h4&gt;A more powerful carbon-relay-ng&lt;/h4&gt;
&lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt; received a bunch of love and has been a great help in my graphite+influxdb experiments.
&lt;p&gt;
&lt;a href=&#34;http://dieter.plaetinck.be/files/carbon-relay-web-ui.png&#34;&gt;&lt;img width=&#34;441&#34; src=&#34;http://dieter.plaetinck.be/files/carbon-relay-web-ui.png&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
Here&#39;s what changed:
&lt;ul&gt;
&lt;li&gt;First I made it so that you can adjust routes at runtime while data is flowing through, via a telnet interface.&lt;/li&gt;
&lt;li&gt;Then &lt;a href=&#34;https://github.com/pauloconnor&#34;&gt;Paul O&#39;Connor&lt;/a&gt; built an embedded web interface to manage your routes in an easier and prettier way (pictured above)&lt;/li&gt;
&lt;li&gt;The relay now also emits performance metrics via statsd (I want to make this better by using &lt;a href=&#34;https://github.com/rcrowley/go-metrics&#34;&gt;go-metrics&lt;/a&gt; which will hopefully get &lt;a href=&#34;https://github.com/rcrowley/go-metrics/issues/68&#34;&gt;expvar support&lt;/a&gt; at some point - any takers?).&lt;/li&gt;
&lt;li&gt;Last but not least, I borrowed &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng/tree/master/nsqd&#34;&gt;the diskqueue&lt;/a&gt; code from &lt;a href=&#34;http://nsq.io/&#34;&gt;NSQ&lt;/a&gt; so now we can also spool to disk to bridge downtime of endpoints and re-fill them when they come back up&lt;/li&gt;
&lt;/ul&gt;
Beside our metrics storage, I also plan to put our anomaly detection (currently playing with &lt;a href=&#34;http://hekad.readthedocs.org/en/v0.7.1/&#34;&gt;heka&lt;/a&gt; and &lt;a href=&#34;http://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;kale&lt;/a&gt;) and &lt;a href=&#34;https://github.com/vimeo/carbon-tagger&#34;&gt;carbon-tagger&lt;/a&gt; behind the relay, centralizing all routing logic, making things more robust, and simplifying our system design.  The spooling should also help to deploy to our metrics gateways at other datacenters, to bridge outages of datacenter interconnects.
&lt;br/&gt;
&lt;br/&gt;
I used to think of carbon-relay-ng as the python carbon-relay but on steroids,
now it reminds me more of something like nsqd but with an ability to make packet routing decisions by introspecting the carbon protocol,
&lt;br/&gt;or perhaps Kafka but much simpler, single-node (no HA), and optimized for the domain of carbon streams.
&lt;br/&gt;I&#39;d like the HA stuff though, which is why I spend some of my spare time figuring out the intricacies of the increasingly popular &lt;a href=&#34;http://raftconsensus.github.io/&#34;&gt;raft&lt;/a&gt; consensus algorithm.   It seems opportune to have a simpler Kafka-like thing, in Go, using raft, for carbon streams.
(note: InfluxDB &lt;a href=&#34;https://github.com/influxdb/influxdb/pull/859&#34;&gt;might introduce such a component&lt;/a&gt;, so I&#39;m also a bit waiting to see what they come up with)
&lt;br/&gt;
&lt;br/&gt;
Reminder: notably missing from carbon-relay-ng is round robin and sharding.  I believe sharding/round robin/etc should be part of a broader HA design of the storage system, as I explained in &lt;a href=&#34;http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html&#34;&gt;On Graphite, Whisper and InfluxDB&lt;/a&gt;.  That said, both should be fairly easy to implement in carbon-relay-ng, and I&#39;m willing to assist those who want to contribute it.
</description>
    </item>
    
    <item>
      <title>Monitorama PDX &amp; my metrics 2.0 presentation</title>
      <link>http://dieter.plaetinck.be/post/monitorama-pdx-metrics20/</link>
      <pubDate>Thu, 29 May 2014 10:39:32 -0400</pubDate>
      
      <guid>monitorama-pdx-metrics20</guid>
      <description>&lt;p&gt;
I think the conference was better than the first one in Boston, much more to learn.  Also this one was quite focused on telemetry (timeseries metrics processing), lots of talks on timeseries analytics, not so much about things like sensu or nagios.  
&lt;a href=&#34;https://vimeo.com/95064249&#34;&gt;Adrian Cockroft&#39;s keynote&lt;/a&gt; brought some interesting ideas to the table, like building a feedback loop into the telemetry to drive infrastructure changes (something we do at Vimeo, I briefly give an example in the intro of my talk) or shortening the time from fault to alert (which I&#39;m excited to start working on soon)
&lt;br/&gt;My other favorite was &lt;a href=&#34;https://vimeo.com/95227467&#34;&gt;Noah Kantrowitz&#39;s talk about applying audio DSP techniques to timeseries&lt;/a&gt;,
I always loved audio processing and production.  Combining these two interests hadn&#39;t occurred to me so now I&#39;m very excited about the applications.
&lt;br/&gt;The opposite, but just as interesting of an idea - conveying information about system state as an audio stream - came up in &lt;a href=&#34;http://puppetlabs.com/podcasts/podcast-insights-monitorama-conference&#34;&gt;puppetlab&#39;s monitorama recap&lt;/a&gt; and that seems to make a lot of sense as well. There&#39;s &lt;b&gt;a lot&lt;/b&gt; of information in a stream of sound, it is much denser than text, icons and perhaps even graph plots.  Listening to an audio stream that&#39;s crafted to represent various information might be a better way to get insights into your system.
&lt;/p&gt;
&lt;br/&gt;
&lt;i&gt;
&lt;p&gt;I&#39;m happy to see the idea reinforced that telemetry is a key part of modern monitoring.
For me personally, telemetry (the tech and the process) is &lt;b&gt;the most fascinating part of modern technical operations&lt;/b&gt;, and I&#39;m glad to be part of the movement pushing this forward.  There&#39;s also a bunch of startups in the space (many stealthy ones), validating the market.  I&#39;m curious to see how this will play out.
&lt;/p&gt;
&lt;/i&gt;&lt;br/&gt;

&lt;p&gt;I had the privilege to present &lt;a href=&#34;http://metrics20.org&#34;&gt;metrics 2.0&lt;/a&gt; and
&lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer&lt;/a&gt;.
As usual the &lt;a href=&#34;http://www.slideshare.net/Dieterbe/metrics20-34319840&#34;&gt;slides are on slideshare&lt;/a&gt; and the &lt;a href=&#34;https://vimeo.com/95076197&#34;&gt;footage on the better video sharing platform&lt;/a&gt; ;-) .
&lt;br/&gt;I&#39;m happy with all the positive feedback, although I&#39;m not aware yet of other tools and applications adopting metrics 2.0, and I&#39;m looking forward to see some more of that, because ultimately that&#39;s what will show if my ideas are any good.
&lt;/p&gt;
&lt;p&gt;
&lt;iframe src=&#34;http://www.slideshare.net/slideshow/embed_code/34319840&#34; width=&#34;427&#34; height=&#34;356&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;iframe allowfullscreen=&#34;allowfullscreen&#34; frameborder=&#34;0&#34; height=&#34;356&#34; mozallowfullscreen=&#34;mozallowfullscreen&#34; src=&#34;//player.vimeo.com/video/95076197?title=0&amp;amp;byline=0&amp;amp;portrait=0&amp;amp;color=33a352&#34; webkitallowfullscreen=&#34;webkitallowfullscreen&#34; width=&#34;633&#34;&gt;&lt;/iframe&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On Graphite, Whisper and InfluxDB</title>
      <link>http://dieter.plaetinck.be/post/on-graphite-whisper-and-influxdb/</link>
      <pubDate>Sun, 18 May 2014 13:22:32 -0400</pubDate>
      
      <guid>on-graphite-whisper-and-influxdb</guid>
      <description>&lt;ul&gt;
&lt;li&gt;It can&#39;t keep all file descriptors in memory so there&#39;s a lot of overhead in constantly opening, seeking, and closing files, especially since usually one write comes in for all metrics at the same time.
&lt;/li&gt;
&lt;li&gt;Using the rollups feature (different data resolutions based on age) causes a lot of extra IO.&lt;/li&gt;
&lt;li&gt;The format is also simply not optimized for writes.  Carbon, the storage agent that sits in front of whisper has a feature to batch up writes to files to make them more sequential but this doesn&#39;t seem to help much.&lt;/li&gt;
&lt;li&gt;Worse, due to various &lt;a href=&#34;https://github.com/pcn/carbon/blob/new-sending-mechanism/Why_Spooling.md#what-problems-have-we-had&#34;&gt;implementation details&lt;/a&gt; the carbon agent is surprisingly inefficient and even cpu-bound.  People often run into cpu limitations before they hit the io bottleneck.  Once the writeback queue hits a certain size, carbon will blow up.&lt;/li&gt;
&lt;/ul&gt;
Common recommendations are to &lt;a href=&#34;http://bitprophet.org/blog/2013/03/07/graphite/&#34;&gt;run multiple carbon agents&lt;/a&gt; and
&lt;a href=&#34;http://obfuscurity.com/2012/04/Unhelpful-Graphite-Tip-5&#34;&gt;running graphite on SSD drives&lt;/a&gt;.
&lt;br/&gt;If you want to scale out across multiple systems, you can get carbon to shard metrics across multiple nodes, but the complexity can get out of hand and manually maintaining a cluster where nodes get added, fail, get phased out, need recovery, etc involves a lot of manual labor even though &lt;a href=&#34;https://github.com/jssjr/carbonate/&#34;&gt;carbonate&lt;/a&gt; makes this easier.  This is a path I simply don&#39;t want to go down.
&lt;br/&gt;
&lt;br/&gt;
&lt;p&gt;
&lt;i&gt;These might be reasonable solutions based on the circumstances (often based on short-term local gains), but I believe as a community we should solve the problem at its root, so that everyone can reap the long term benefits.
&lt;/i&gt;
&lt;/p&gt;
&lt;br/&gt;

In particular, &lt;a href=&#34;http://blog.sweetiq.com/2013/01/using-ceres-as-the-back-end-database-to-graphite/#axzz324uQtk3d&#34;&gt;running Ceres instead of whisper&lt;/a&gt;, is only a slight improvement, that suffers from most of the same problems.  I don&#39;t see any good reason to keep working on Ceres, other than perhaps that it&#39;s a fun exercise.   This probably explains the slow pace of development.
&lt;br/&gt;However, many mistakenly believe Ceres is &#34;the future&#34;.
&lt;br/&gt;&lt;a href=&#34;http://www.inmobi.com/blog/2014/01/24/extending-graphites-mileage&#34;&gt;Switching to LevelDB&lt;/a&gt; seems much more sensible but IMHO still doesn&#39;t cut it as a general purpose, scalable solution.

&lt;h4&gt;The ideal backend&lt;/h4&gt;
I believe we can build a backend for graphite that
&lt;ul&gt;
&lt;li&gt;can easily scale from a few metrics on my laptop in power-save mode to millions of metrics on a highly loaded cluster&lt;/li&gt;
&lt;li&gt;supports nodes joining and leaving at runtime and automatically balancing the load across them&lt;/li&gt;
&lt;li&gt;assures high availability and heals itself in case of disk or node failures&lt;/li&gt;
&lt;li&gt;is simple to deploy.  think: just run an executable that knows which directories it can use for storage, elasticsearch-style automatic clustering, etc.&lt;/li&gt;
&lt;li&gt;has the right read/write optimizations.  I&#39;ve never seen a graphite system that is not write-focused, so something like &lt;a href=&#34;http://en.wikipedia.org/wiki/Log-structured_merge-tree&#34;&gt;LSM trees&lt;/a&gt; seems to make a lot of sense.&lt;/li&gt;
&lt;li&gt;can leverage cpu resources (e.g. for compression)&lt;/li&gt;
&lt;li&gt;provides a more natural model for phasing out data.  Optional, runtime-changeable rollups.  And an age limit (possibly, but not necessarily round robin)
&lt;/ul&gt;

While we&#39;re at it. pub-sub for realtime analytics would be nice too.  Especially when it allows to use the same functions as the query api.
&lt;br/&gt;And getting rid of the metric name restrictions such as inability to use dots or slashes.  Efficient sparse series support would be nice too.

&lt;h4&gt;InfluxDB&lt;/h4&gt;

There&#39;s a lot of databases that you could hook up to graphite.
riak, hdfs based (opentsdb), Cassandra based (kairosdb, blueflood, cyanite), etc.

Some of these are solid and production ready, and would make sense depending on what you already have and have experience with.
I&#39;m personally very interested in playing with Riak, but decided to choose InfluxDB as my first victim.
&lt;br/&gt;
&lt;br/&gt;
InfluxDB is a young project that will need time to build maturity, but is on track to meet all my goals very well.
In particular, installing it is a breeze (no dependencies), it&#39;s specifically built for timeseries (not based on a general purpose database),
which allows them to do a bunch of simplifications and optimizations, is write-optimized, and should meet my goals for scalability, performance, and availability well.
And they&#39;re in NYC so meeting up for lunch has proven to be pretty fruitful for both parties.  I&#39;m pretty confident that these guys can pull off something big.
&lt;br/&gt;
&lt;br/&gt;
Technically, InfluxDB is a &#34;timeseries, metrics, and analytics&#34; databases with use cases well beyond graphite and even technical operations.
Like the alternative databases, graphite-like behaviors such as rollups management and automatically picking the series in the most appropriate resolutions, is something to be implemented on top of it.  Although you never know, it might end up being natively supported.


&lt;h4&gt;Graphite + InfluxDB&lt;/h4&gt;

InfluxDB developers plan to implement a whole bunch of processing functions (akin to graphite, except they can do locality optimizations) and add a dashboard that talks to InfluxDB natively (or use &lt;a href=&#34;http://grafana.org/&#34;&gt;Grafana&lt;/a&gt;), which means at some point you could completely swap graphite for InfluxDB.

However, I think for quite a while, the ability to use the Graphite api, combine backends, and use various graphite dashboards is still very useful.
So here&#39;s how my setup currently works:

&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt; is a carbon relay in Go.  
It&#39;s a pretty nifty program to partition and manage carbon metrics streams.  I use it in front of our traditional graphite system, and have it stream - in realtime - a copy of a subset of our metrics into InfluxDB.  This way I basically have our unaltered Graphite system, and in parallel to it, InfluxDB, containing a subset of the same data.
&lt;br/&gt;With a bit more work it will be a high performance alternative to the python carbon relay, allowing you to manage your streams on the fly.
It doesn&#39;t support consistent hashing, because CH should be part of a strategy of a highly available storage system (see requirements above), using CH in the relay still results in a poor storage system, so there&#39;s no need for it.
&lt;/li&gt;
&lt;li&gt;I contributed the code to InfluxDB to make it listen on the carbon protocol.  So basically, for the purpose of ingestion, InfluxDB can look and act just like a graphite server.  Anything that can write to graphite, can now write to InfluxDB.  (assuming the plain-text protocol, it doesn&#39;t support the pickle protocol, which I think is a thing to avoid anyway because almost nothing supports it and you can&#39;t debug what&#39;s going on)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/brutasse/graphite-api&#34;&gt;graphite-api&lt;/a&gt; is a fork/clone of graphite-web, stripped of needless dependencies, stripped of the composer.  It&#39;s conceived for many of the same reasons behind &lt;a href=&#34;http://dieter.plaetinck.be/graphite-ng_a-next-gen-graphite-server-in-go.html&#34;&gt;graphite-ng&lt;/a&gt; (graphite technical debt, slow development pace, etc) though it doesn&#39;t go to such extreme lengths and for now focuses on being a robust alternative for the graphite server, api-compatible, trivial to install and with a faster pace of development.
&lt;/li&gt;
&lt;li&gt;
That&#39;s where &lt;a href=&#34;https://github.com/vimeo/graphite-influxdb&#34;&gt;graphite-influxdb&lt;/a&gt; comes in.  It hooks InfluxDB into graphite-api, so that you can query the graphite api, but using data in InfluxDB.
It should also work with the regular graphite, though I&#39;ve never tried.  (I have no incentive to bother with that, because I don&#39;t use the composer.  And I think it makes more sense to move the composer into a separate project anyway).
&lt;/li&gt;
&lt;/ul&gt;

With all these parts in place, I can run our dashboards next to each other - one running on graphite with whisper, one on graphite-api with InfluxDB - and simply look whether the returned data matches up, and which dashboards loads graphs faster.
Later i might do more extensive benchmarking and acceptance testing.
&lt;br/&gt;
&lt;br/&gt;
If all goes well, I can make carbon-relay-ng fully mirror all data, make graphite-api/InfluxDB the primary, and turn our old graphite box into a live &#34;backup&#34;.
We&#39;ll need to come up with something for rollups and deletions of old data (although it looks like by itself influx is already more storage efficient than whisper too), and I&#39;m really looking forward to the InfluxDB team building out the function api, having the same function api available for historical querying as well as realtime pub-sub.  (my goal used to be implementing this in graphite-ng and/or carbon-relay-ng, but if they do this well, I might just abandon graphite-ng)

&lt;br/&gt;
&lt;br/&gt;To be continued..

</description>
    </item>
    
    <item>
      <title>Metrics 2.0 now has its own website!</title>
      <link>http://dieter.plaetinck.be/post/metrics-2-0-own-website/</link>
      <pubDate>Wed, 23 Apr 2014 09:10:32 -0400</pubDate>
      
      <guid>metrics-2-0-own-website</guid>
      <description>&lt;p&gt;
from the website:
&lt;/p&gt;
      &lt;p&gt;
      We have pretty good storage of timeseries data, collection agents, and dashboards.  But the idea of giving timeseries a &#34;name&#34; or a
      &#34;key&#34; is profoundly limiting us.  Especially when they&#39;re not standardized and missing information.
      &lt;br/&gt;Metrics 2.0 aims for &lt;b&gt;self-describing&lt;/b&gt;, &lt;b&gt;standardized&lt;/b&gt; metrics using &lt;b&gt;orthogonal tags&lt;/b&gt; for every dimension.
      &#34;metrics&#34; being the pieces of information that point to, and describe timeseries of data.
      &lt;/p&gt;By adopting metrics 2.0 you can:
      &lt;ul&gt;
        &lt;li&gt;increase compatibility between tools&lt;/li&gt;
        &lt;li&gt;get immediate understanding of metrics&lt;/li&gt;
        &lt;li&gt;build graphs, plots, dashboards and alerting expressions with minimal hassle&lt;/li&gt;
      &lt;/ul&gt;
Read more on &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics20.org&lt;/a&gt;.
</description>
    </item>
    
    <item>
      <title>Introduction talk to metrics 2.0 and Graph-Explorer</title>
      <link>http://dieter.plaetinck.be/post/introduction_talk_to_metrics2-0_and_graph_explorer/</link>
      <pubDate>Sun, 23 Feb 2014 16:20:32 -0400</pubDate>
      
      <guid>introduction_talk_to_metrics2-0_and_graph_explorer</guid>
      <description>&lt;br/&gt;
I could easily talk for hours about this stuff but the talk had to be about 20 minutes so I paraphrased it to be only
about the basic concepts and ideas and some practical use cases and features.  I think it serves as a pretty good introduction
and a showcase of the most commonly used features (graph composition, aggregations and unit conversion), and some new stuff such as alerting and dashboards.

&lt;p&gt;
The talk also briefly covers native metrics 2.0 through your metrics pipeline using &lt;a href=&#34;https://github.com/vimeo/statsdaemon&#34;&gt;statsdaemon&lt;/a&gt; and &lt;a href=&#34;https://github.com/vimeo/carbon-tagger&#34;&gt;carbon-tagger&lt;/a&gt;.  I&#39;m psyched that by formatting metrics at the source a little better and having an aggregation daemon that expresses the performed operations by updating the metric tags, all the foundations are in place for some truly next-gen UI&#39;s and applications (one of them already being implemented: graph-explorer can pretty much generate all graphs I need by just phrasing an information need as a proper query)
&lt;/p&gt;
&lt;p&gt;
The &lt;a href=&#34;http://vimeo.com/87194301&#34;&gt;video&lt;/a&gt; and &lt;a href=&#34;https://www.slideshare.net/Dieterbe/metrics2-0graphexplorer20140218&#34; title=&#34;Metrics 2.0 &amp;amp; Graph-Explorer&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt; are available and also embedded below.
&lt;/p&gt;

&lt;iframe src=&#34;//player.vimeo.com/video/87194301?title=0&amp;amp;byline=0&amp;amp;portrait=0&amp;amp;color=33a352&#34;
width=&#34;500&#34; height=&#34;281&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;iframe src=&#34;http://www.slideshare.net/slideshow/embed_code/31440297&#34; width=&#34;427&#34; height=&#34;356&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;p&gt;
I would love to do another talk that allows me to dive into more of the underlying ideas, the benefits of metrics2.0 for things like metric storage systems, graph renderers, anomaly detection, dashboards, etc.
&lt;br/&gt;
&lt;br/&gt;
Hope you like it!
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Metrics 2.0: a proposal</title>
      <link>http://dieter.plaetinck.be/post/metrics_2_a_proposal/</link>
      <pubDate>Sat, 14 Sep 2013 11:29:32 -0400</pubDate>
      
      <guid>metrics_2_a_proposal</guid>
      <description>&lt;ul&gt;
&lt;li&gt;the metrics are not fully self-describing (they need additional information to become useful for interpretation, e.g. a unit, and information about what they mean)&lt;/li&gt;
&lt;li&gt;they impose needless limitations (no dots allowed, strict ordering)&lt;/li&gt;
&lt;li&gt;aggregators have no way to systematically and consistently express the operation they performed&lt;/li&gt;
&lt;li&gt;we can&#39;t include additional information because that would create a new metric ID&lt;/li&gt;
&lt;li&gt;we run into scale/display/correctness problems because storage rollups, rendering consolidation,
as well as aggregation api functions need to be aware of how the metric
is to be aggregated, and this information is often lacking or incorrect.&lt;/li&gt;
&lt;li&gt;there&#39;s no consistency into what information goes where or how it&#39;s called (which field is the one that has the hostname?)&lt;/li&gt;
&lt;/ul&gt;
&lt;b&gt;We can solve all of this.  In an elegant way, even!&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;
Over the past year, I&#39;ve been working a lot on a graphite dashboard (&lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer&lt;/a&gt;).
It&#39;s fairly unique in the sense that it leverages what I call &#34;structured metrics&#34; to facilitate a powerful way to
build graphs dynamically.  (see also
&lt;a href=&#34;http://dieter.plaetinck.be/a_few_common_graphite_problems_and_how_they_are_already_solved.html&#34;&gt;
&#34;a few common graphite problems and how they are already solved&#34;&lt;/a&gt; and the
&lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer homepage&lt;/a&gt;).
I implemented two solutions. (
&lt;a href=&#34;https://github.com/vimeo/graph-explorer/tree/master/structured_metrics&#34;&gt;structured_metrics&lt;/a&gt;,
&lt;a href=&#34;https://github.com/vimeo/carbon-tagger&#34;&gt;carbon-tagger&lt;/a&gt;) for creating, transporting and using these metrics
on which dashboards like Graph-Explorer can rely.
The former converts graphite metrics into a set of key-value pairs (this is &#34;after the fact&#34; so a little annoying),
carbon-tagger uses a prototype extended carbon (graphite) protocol (called &#34;proto2&#34;) to maintain an elasticsearch tags database on the fly, but
the format proved too restrictive.
&lt;/p&gt;

&lt;p&gt;
Now for &lt;a href=&#34;https://github.com/graphite-ng/graphite-ng&#34;&gt;Graphite-NG&lt;/a&gt; I wanted to rethink the metric, taking the simplicity of Graphite,
the ideas of structured_metrics and carbon_tagger (but less annoying) and OpenTSDB (but more powerful), and propose a new way
to identify and use metrics and organize tags around them.
&lt;/p&gt;
&lt;p&gt;
The proposed ingestion (carbon etc) protocol looks like this:
&lt;pre&gt;
&lt;intrinsic_tags&gt;  &lt;extrinsic_tags&gt; &lt;value&gt; &lt;timestamp&gt;
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
intrinsic_tags and extrinsic_tags are space-separated strings, those strings can be a regular value, or a key=value pair and they describe one
dimension (aspect) of the metric.
    &lt;ul&gt;
    &lt;li&gt;an &lt;b&gt;intrinsic&lt;/b&gt; tag contributes to the identity of the metric. If this section changes, we get a new metric&lt;/li&gt;
    &lt;li&gt;an &lt;b&gt;extrinsic&lt;/b&gt; tag provides additional information about the metric.  changes in this set doesn&#39;t change the metric identity&lt;/li&gt;
    &lt;/ul&gt;
    Internally, the metric ID is nothing more than the string of intrinsic tags you provided (similar to Graphite style).
    When defining a new metric, write down the intrinsic tags/nodes (like you would do with Graphite except you can use &lt;b&gt;any order&lt;/b&gt;), and then keep &#39;em in that order
    (to keep the same ID).  &lt;b&gt;The ordering does not affect your ability to work with the metrics in any way&lt;/b&gt;.
    The backend indexes the tags and you&#39;ll usually rely on those to work with the metrics, and rarely with the ID itself.
&lt;/li&gt;
&lt;li&gt;
A metric in its basic form (without extrinsic tags) would look like:
&lt;pre&gt;
graphite: stats.gauges.db15.mysql.bytes_received
opentsdb: mysql.bytes_received host=db15
proposal: service=mysql server=db15 direction=in unit=B
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
key=val tags are most useful: the key is a name of the aspect, which allows you to express &#34;I want to see my metrics averaged/summed/.. by this
aspect, in different graphs based on another aspect, etc&#34;
(&lt;a href=&#34;https://github.com/vimeo/graph-explorer/wiki/GEQL#special-statements&#34;&gt;e.g. GEQL statements&lt;/a&gt;),
and because you can use them for filtering,
so I highly recommend to take some time to come up with a good key for every tag.
However sometimes it can be hard to come up with a concise, but descriptive key for a tag, hence they are not mandatory.
For regular words without a key, the backend will assign dummy keys (&#39;n1&#39;, &#39;n2&#39;, etc) to facilitate those features without hassle.
Note the two spaces between intrinsic and extrinsic.
With extrinsic tags the example could look like:
&lt;pre&gt;
service=mysql server=db15 direction=in unit=B  src=diamond processed_by_statsd env=prod
&lt;/pre&gt;
to mean that this metric came from diamond, went through statsd, and that the machine is currently in the prod environment
&lt;/li&gt;
&lt;li&gt;Tags can contain any character except whitespace and null.
Specifically: dots (great for metrics that contain an ip, histogram bin, a percentile, etc) and slashes (unit can be &#39;B/s&#39; too)
&lt;/li&gt;
&lt;li&gt;the unit tag is mandatory. It allows dashboards to show the proper label on the Y-axis, and to do conversions
(for example in Graph Explorer, if your metric is an amount of B used on a disk drive, and you request the increase in GB per day.
it will automatically convert (and derive) the data).
We should aim for standardization of units.  I maintain a table of
&lt;a href=&#34;https://github.com/vimeo/graph-explorer/wiki/Units-&amp;-Prefixes&#34;&gt;standardized units &amp; prefixes&lt;/a&gt; which uses SI and IEC as starting point,
and extends it with units commonly used in IT.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Further thoughts/comparisons&lt;/h3&gt;
&lt;h4&gt;General&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
The concept of extrinsic tags is something I haven&#39;t seen before, but I think it makes a lot of sense because we often want to pass extra information
but we couldn&#39;t because it would create a new metric.  It also makes the metrics more self-describing.&lt;/li&gt;
&lt;li&gt;Sometimes it makes sense for tags only to apply to a metric in a certain time frame.  For intrinsic tags this is already the case by design,
for extrinsic tags the database could maintain a from/to timestamp based on when it saw the tag for the given metric&lt;/li&gt;
&lt;li&gt;metric finding: besides the obvious left-to-right auto-complete and pattern matching (which allows searching for substrings in any order), we can now also build
an interface that uses &lt;a href=&#34;http://www.elasticsearch.org/guide/reference/api/search/facets/&#34;&gt;facet searches&lt;/a&gt;
to suggest/auto-complete tags, and filter by toggling on/off tags.
&lt;/li&gt;
&lt;li&gt;Daemons that sit in the pipeline and yield aggregated/computed metrics can do this in a much more useful way.  For example a statsd daemon
that computes a rate per second for metrics with &#39;unit=B&#39; can yield a metric with &#39;unit=B/s&#39;.&lt;/li&gt;
&lt;li&gt;
    We can &lt;a href=&#34;https://github.com/vimeo/graph-explorer/wiki/Consistent-tag-keys-and-values&#34;&gt;standardize tag keys and values&lt;/a&gt;, other than just the unit tag.
    Beyond the obvious compatibility benefits between tools, imagine querying for:
    &lt;ul&gt;
    &lt;li&gt;&#39;unit=(Err|Warn)&#39; and getting all errors and warnings across the entire infrastructure (no matter which tool generated the metric), and grouping/drilling down by tags&lt;/li&gt;
    &lt;li&gt;&#39;$hostname direction=in&#39; and seeing everything coming in to the server, network traffic on the NIC, as well as files being uploaded.&lt;/li&gt;
    &lt;/ul&gt;
    Also, metrics that are summary statistics (i.e. with statsd) will get intrinsic tags like &#39;stat=median&#39; or &#39;stat=upper_90&#39;.
    This has three fantastic consequences:
    &lt;ul&gt;
    &lt;li&gt;aggregation (rollups from higher to lower resolution) knows what to do without configurating aggregation functions, because it can be deduced from the metric itself&lt;/li&gt;
    &lt;li&gt;renderers that have to render &gt;1 datapoints per pixel, will produce more accurate, relevant graphs because they can deduce what the metric is meant to represent&lt;/li&gt;
    &lt;li&gt;API functions such as &#34;cumulative&#34;, &#34;summarize&#34; and &#34;smartSummarize&#34; don&#39;t need to be configured with an explicit aggregation function&lt;/li&gt;
    &lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;From the Graphite perspective, specifically&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;dots and slashes are allowed&lt;/li&gt;
&lt;li&gt;The tree model is slow to navigate, sometimes hard to find your metrics, and makes it really hard to write
queries (target statements) that need metrics in different locations of the tree (because the only way to retrieve multiple metrics in a target is wildcards)&lt;/li&gt;
&lt;li&gt;The tree model causes people to obsess over node ordering to find the optimal tree organization, but no ordering allows all query use cases anyway,
so you&#39;ll be limited no matter how much time you spend organising metrics.&lt;/li&gt;
&lt;li&gt;&lt;i&gt;We do away with the tree entirely.  A multi-dimensional tag database is way more powerful&lt;/i&gt;and allows for great &#34;metric finding&#34; (see above)&lt;/li&gt;
&lt;li&gt;Graphite has no tags support&lt;/li&gt;
&lt;li&gt;you don&#39;t need to configure aggregation functions anymore, less room for errors (&#34;help, my scale is wrong when i zoom out&#34;), better rendering&lt;/li&gt;
&lt;li&gt;when using statsd, you don&#39;t need prefixes like &#34;stats.&#34;.  In fact that whole prefix/postfix/namespacing thing becomes moot&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;From the OpenTSDB perspective, specifically&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;allow dots anywhere&lt;/li&gt;
&lt;li&gt;&#39;http.hits&#39; becomes &#39;http unit=Req&#39; (or &#39;unit=Req http&#39;, as long as you pick one and stick with it)&lt;/li&gt;
&lt;li&gt;probably more, I&#39;m not very familiar with it&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;From the structured_metrics/carbon-tagger perspective, specifically&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;not every tag requires a key (on metric input), but you still get the same benefits&lt;/li&gt;
&lt;li&gt;You&#39;re not forced to order the tags in any way&lt;/li&gt;
&lt;li&gt;sometimes relying on the &#39;plugin&#39; tag is convenient but it&#39;s not really intrinsic to the metric, now we can use it as extrinsic tag&lt;/li&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;h3&gt;Backwards compatibility&lt;/h3&gt;
&lt;!-- These changes may seem a little invasive, but it&#39;s actually not that bad: TODO is that so?--&gt;
&lt;ul&gt;
&lt;li&gt;Sometimes you merely want the ability to copy a &#34;metric id&#34; from the app source code, and paste it in a &#34;/render/?target=&#34; url to see that particular metric.
You can still do this: copy the intrinsic tags string and you have the id.&lt;/li&gt;
&lt;li&gt;if you wanted to, you could easily stay compatible with the official graphite protocol:
for incoming metrics, add a &#39;unit=Unknown&#39; tag and optionally turn the dots into
spaces (so that every node becomes a tag), so you can mix old-style and new style metrics in the same system.
&lt;/li&gt;

</description>
    </item>
    
    <item>
      <title>Graphite-ng: A next-gen graphite server in Go.</title>
      <link>http://dieter.plaetinck.be/post/graphite-ng_a-next-gen-graphite-server-in-go/</link>
      <pubDate>Sat, 07 Sep 2013 20:54:20 -0400</pubDate>
      
      <guid>graphite-ng_a-next-gen-graphite-server-in-go</guid>
      <description>&lt;p&gt;
There&#39;s a few reasons why I decided to start a new project from scratch:
&lt;ul&gt;
&lt;li&gt;With graphite, it&#39;s a whole ordeal to get all components properly installed.  Deploying in production environments is annoying and even more so
when you just want a graphite setup on your personal netbook.&lt;/li&gt;
&lt;li&gt;the graphite development process slows contributors down a lot.  A group of 3rd/4th generation maintainers jointly manages the project, but it&#39;s hard to get big changes through,
because they (understandably) don&#39;t feel authoritative enough to judge those changes and the predecessors have disappeared or are too busy with other things.  And also ...&lt;/li&gt;
&lt;li&gt;there&#39;s a high focus on backwards compatibility, which can be a good thing, but it&#39;s hard to get rid of old design mistakes,
especially when fixing unclear (but arguably broken) early design decisions (or oversights) lead to different outputs&lt;/li&gt;
&lt;li&gt;Graphite suffers feature creep: it has an events system, a PNG renderer, an elaborate composer web UI, etc.
There&#39;s a lot of internal code dependencies holding you back from focusing on a specific problem&lt;/li&gt;
&lt;li&gt;Carbon (the metrics daemon) has a pretty hard performance and scalability ceiling.
&lt;a href=&#34;https://github.com/pcn/carbon/blob/new-sending-mechanism/Why_Spooling.md&#34;&gt;Peter&#39;s article explains this well&lt;/a&gt;; I think we&#39;ll need some form of
rewrite.   Peter suggests some solutions but they are basically workarounds for Python&#39;s shortcomings.  I&#39;m also thinking of using &lt;a href=&#34;http://pypy.org/&#34;&gt;pypy&lt;/a&gt;.
But last time I checked pypy just wasn&#39;t there yet.&lt;/li&gt;
&lt;li&gt;I want to become a good Go programmer&lt;/li&gt;
&lt;/ul&gt;

&lt;i&gt;Note: the Graphite project is still great&lt;/i&gt;, the people managing do good work, but it&#39;s fairly natural for a code base that large and complicated
to end up in this situation.&lt;i&gt;I&#39;m not at all claiming graphite-ng is, or ever will be better&lt;/i&gt; but I need a fresh start to try some disruptive ideas,
using Go means having a runtime very suited for concurrency and parallelism, you can compile the whole thing down into a single executable file,
and its performance looks promising.  Leaving out the non-essentials (see below) allows for an elegant and surprisingly small, hackable code base.
&lt;/p&gt;

&lt;p&gt;
The API server I developed sets up a processing pipeline as directed by your query: every processing function runs in a goroutine
for concurrency and the metrics flow through using Go channels.  It literally compiles a program and executes it.  You can add your own functions
to collect, process, and return metrics by writing simple plugins.
&lt;br/&gt;As for timeseries storage, for now it uses simple text files,
but I&#39;m experimenting and thinking what would be the best metric store(s) that works on small scale
(personal netbook install) to large scale (&#34;I have millions of metrics that need to be distributed across nodes,
the system should be HA and self-healing in failure scenarios, easily maintainable, and highly performant&#34;) and is still easy to deploy, configure and run.
Candidates are &lt;a href=&#34;https://github.com/kisielk/whisper-go&#34;&gt;whisper-go&lt;/a&gt;, &lt;a href=&#34;https://code.google.com/p/kairosdb/&#34;&gt;kairosdb&lt;/a&gt;,
my own &lt;a href=&#34;https://github.com/graphite-ng/graphite-ng/tree/master/carbon-es&#34;&gt;elasticsearch experiment&lt;/a&gt; etc.
&lt;br/&gt;I won&#39;t implement rendering images, because I think client-side rendering using something like &lt;a href=&#34;https://github.com/vimeo/timeserieswidget&#34;&gt;timeserieswidget&lt;/a&gt;
is superior.  I can also leave out events because &lt;a href=&#34;https://github.com/Dieterbe/anthracite/&#34;&gt;anthracite&lt;/a&gt; already does that.
There&#39;s a ton of dashboards out there (&lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;graph-explorer&lt;/a&gt;, &lt;a href=&#34;https://github.com/obfuscurity/descartes&#34;&gt;descartes&lt;/a&gt;, &lt;a href=&#34;http://graphite.readthedocs.org/en/1.0/tools.html&#34;&gt;etc&lt;/a&gt;) so that can be left out as well.
&lt;/p&gt;

For more information, see &lt;a href=&#34;https://github.com/graphite-ng/graphite-ng&#34;&gt;the Graphite-ng homepage&lt;/a&gt;.
&lt;br/&gt;
&lt;br/&gt;PS: props to &lt;a href=&#34;http://felixge.de/&#34;&gt;Felix Geisendorfer&lt;/a&gt; who suggested a graphite clone in Go first,
it seemed like a huge undertaking but the right thing to do, I had some time so I went for it!
</description>
    </item>
    
    <item>
      <title>A few common graphite problems and how they are already solved.</title>
      <link>http://dieter.plaetinck.be/post/a_few_common_graphite_problems_and_how_they_are_already_solved/</link>
      <pubDate>Thu, 04 Apr 2013 08:54:20 -0400</pubDate>
      
      <guid>a_few_common_graphite_problems_and_how_they_are_already_solved</guid>
      <description>&lt;h4&gt;metrics often seem to lack details, such as units and metric types&lt;/h4&gt;
looking at a metric name, it&#39;s often hard to know
&lt;ul&gt;
&lt;li&gt;the unit a metric is measured in (bits, queries per second, jiffies, etc)&lt;/li&gt;
&lt;li&gt;the &#34;type&#34; (a rate, an ever increasing counter, gauge, etc)&lt;/li&gt;
&lt;li&gt;the scale/prefix (absolute, relative, percentage, mega, milli, etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;a href=&#34;#structured_metrics&#34;&gt;structured_metrics&lt;/a&gt; solves this by adding these tags to graphite metrics:
&lt;ul&gt;
&lt;li&gt;&lt;pre&gt;what&lt;/pre&gt;
&lt;i&gt;what is being measured?: bytes, queries, timeouts, jobs, etc&lt;/i&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;target_type&lt;/pre&gt;
&lt;i&gt;must be one of the existing &lt;a href=&#34;https://github.com/vimeo/graph-explorer#enhanced-metrics&#34;&gt;clearly defined target_types&lt;/a&gt; (count, rate, counter, gauge)
&lt;br/&gt;These match statsd metric types (i.e. rate is per second, count is per flushInterval)&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;
In &lt;a href=&#34;#graph_explorer&#34;&gt;Graph-Explorer&lt;/a&gt; these tags are mandatory, so that it can show the unit along with the prefix (i.e. &lt;i&gt;&#39;Gb/s&#39;&lt;/i&gt;) on the axis.  
&lt;br/&gt;
This will also allow you to request graphs in a different unit and the dashboard will know how to convert (say, Mbps to GB/day)

&lt;h4&gt;tree navigation/querying is cumbersome, metrics search is broken.  How do I organize the tree anyway?&lt;/h4&gt;
the tree is a simplistic model. There is simply too much dimensionality that can&#39;t be expressed in a flat tree.
There&#39;s no way you can organize it so that will it satisfy all later needs.
A tag space like &lt;a href=&#34;#structured_metrics&#34;&gt;structured_metrics&lt;/a&gt; makes it &lt;b&gt;obsolete&lt;/b&gt;.
with &lt;a href=&#34;#graph_explorer&#34;&gt;Graph-Explorer&lt;/a&gt; you can do (full-text) search on metric name, by any of their tags, and/or by added metadata.
So practically you can filter by things like server, service, unit (e.g. anything expressed in bits/bytes per second,
or anything denoting errors).  All this irrespective of the source of a metric or the &#34;location in the tree&#34;.

&lt;h4&gt;no interactivity with graphs&lt;/h4&gt;
&lt;a href=&#34;#timeserieswidget&#34;&gt;timeserieswidget&lt;/a&gt; allows you to easily add interactive graphite graph objects to html pages.
You get modern features like togglable/reorderable metrics, realtime switching between lines/stacked,
information popups on hoover, highlighting, smoothing, and (WIP) realtime zooming.
It has a canvas (&lt;a href=&#34;http://www.flotcharts.org/&#34;&gt;flot&lt;/a&gt;) and svg (&lt;a href=&#34;http://code.shutterstock.com/rickshaw/&#34;&gt;rickshaw/d3&lt;/a&gt;) backend.
So it basically provides a simpler api to use these libraries specifically with graphite.
&lt;br/&gt;There&#39;s a bunch of different graphite dashboards with different takes on graph composition/configuration and workflow, but the actual rendering of graphs
usually comes down to plotting some graphite targets with a legend.  timeserieswidget aims to be a drop-in plugin that brings all modern features
so that different dashboards can benefit from a common, shared codebase, because &lt;b&gt;static PNGs are a thing from the past&lt;/b&gt;
&lt;p&gt;
screenshot:
&lt;br/&gt;
&lt;a href=&#34;http://dieter.plaetinck.be/files/timeserieswidget-rickshaw-stacked.png&#34;&gt;&lt;img height=&#34;300&#34; src=&#34;http://dieter.plaetinck.be/files/blog/github/vimeo/timeserieswidget/timeserieswidget-rickshaw-stacked.png&#34; title=&#34;yes I&#39;m aware of the irony of a static screenshot of an interactive widget :)&#34;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h4&gt;events lack text annotations, they are simplistic and badly supported&lt;/h4&gt;
Graphite is a great system for time series metrics.  Not for events.  metrics and events are very different things across the board.  drawAsInFinite() is a bit of a hack.
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#anthracite&#34;&gt;anthracite&lt;/a&gt; is designed specifically to manage events.
&lt;br/&gt;It brings extra features such as different submission scripts, outage annotations, various ways to see events and reports with uptime/MTTR/etc metrics.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#timeserieswidget&#34;&gt;timeserieswidget&lt;/a&gt; displays your events on graphs along with their metadata (which can be just some text or even html code).
&lt;br/&gt;this is where client side rendering &lt;i&gt;shines&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
screenshots:
&lt;br/&gt;
&lt;a href=&#34;https://raw.github.com/Dieterbe/anthracite/master/screenshots/screenshot-table.png&#34;&gt;&lt;img height=&#34;300&#34; src=&#34;http://dieter.plaetinck.be/files/blog/github/Dieterbe/anthracite/screenshot-table.png&#34;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;a href=&#34;https://raw.github.com/vimeo/timeserieswidget/master/screenshots/flot-annotated-event.png&#34;&gt;&lt;img height=&#34;300&#34; src=&#34;http://dieter.plaetinck.be/files/blog/github/vimeo/timeserieswidget/flot-annotated-event.png&#34;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h4&gt;cumbersome to compose graphs&lt;/h4&gt;
There&#39;s basically two approaches:
&lt;ul&gt;
&lt;li&gt;interactive composing: with the graphite composer, you navigate through the tree and apply functions.
This is painfull, dashboards like &lt;a href=&#34;https://github.com/obfuscurity/descartes&#34;&gt;descartes&lt;/a&gt; and
&lt;a href=&#34;https://github.com/paperlesspost/graphiti&#34;&gt;graphiti&lt;/a&gt; can make this easier&lt;/li&gt;
&lt;li&gt;use a dashboard that uses predefined templates (&lt;a href=&#34;https://github.com/ripienaar/gdash&#34;&gt;gdash&lt;/a&gt; and others)
They often impose a strict navigation path to reach pages which may or may not give you the information you need (usually less or way more)&lt;/li&gt;
&lt;/ul&gt;
With both approaches, you usually end up with an ever growing pile of graphs that you created and then keep for reference.
&lt;br/&gt;This becomes unwieldy but is useful for various use cases and needs.
&lt;br/&gt;However, &lt;i&gt;neither approach is convenient for changing information needs&lt;/i&gt;.
&lt;br/&gt;Especially when troubleshooting, one day you might want to compare the rate of increase of open file handles on a set of specific servers to the traffic
on given network switches, the next day it&#39;s something completely different.
&lt;br/&gt;With &lt;a href=&#34;#graph_explorer&#34;&gt;Graph-Explorer&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;GE gives you a query interface on top of &lt;a href=&#34;#structure_metrics&#34;&gt;structured_metric&lt;/a&gt;&#39;s tag space.  this enables a bunch of things (see above)&lt;/li&gt;
&lt;li&gt;you can yield arbitrary targets for each metric, to look at the same thing from a different angle (i.e. as a rate with `derivative()` or as a daily summary),
and you can of course filter by angle&lt;/li&gt;
&lt;li&gt;&lt;b&gt;You can group metrics into graphs by arbitrary tags&lt;/b&gt; (e.g. you can see bytes used of all filesystems on a graph per server, or &lt;i&gt;compare servers on a graph per filesystem&lt;/i&gt;).
&lt;b&gt;This feature always results in the &#34;wow that&#39;s really cool&#34; every time I show it&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;GE includes &#39;what&#39; and &#39;target_type&#39; in the group_by tags by default so basically, if things are in a different unit (B/s vs B vs b etc) it&#39;ll put them in
separate graphs (controllable in query)&lt;/li&gt;
&lt;li&gt;GE automatically generates the graph title and vertical title (always showing the &#39;what&#39; and the unit), and shows all metrics&#39; extra tags.
This also gives you a lot of inspiration to modify or extend your query&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
&lt;a href=&#34;http://dieter.plaetinck.be/files/blog/github/vimeo/graph-explorer/screenshot.png&#34;&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/blog/github/vimeo/graph-explorer/screenshot.png&#34; width=&#34;619&#34; height=&#34;396&#34;/&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h4&gt;limited options to request a specific time range&lt;/h4&gt;
&lt;a href=&#34;https://github.com/vimeo/graph-explorer#query-parsing-and-execution&#34;&gt;GE&#39;s query language&lt;/a&gt; supports freeform `from` and `to` clauses.

&lt;h3&gt;Referenced projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/Dieterbe/anthracite&#34; id=&#34;anthracite&#34;&gt;anthracite&lt;/a&gt;:
&lt;br/&gt;
event/change logging/management with a bunch of ingestion scripts and outage reports
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/vimeo/timeserieswidget&#34; id=&#34;timeserieswidget&#34;&gt;timeserieswidget&lt;/a&gt;:
&lt;br/&gt;
jquery plugin to easily get highly interactive graphite graphs onto html pages (dashboards)
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/vimeo/graph-explorer/tree/master/structured_metrics&#34; id=&#34;structured_metrics&#34;&gt;structured_metrics&lt;/a&gt;:
&lt;br/&gt;
python library to convert graphite metrics tree into a tag space with clearly defined units and target types, and arbitrary metadata.
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/vimeo/graph-explorer&#34; id=&#34;graph_explorer&#34;&gt;graph-explorer&lt;/a&gt;:
&lt;br/&gt;
dashboard that provides a query language so you can easily compose graphs on the fly to satisfy varying information needs.
&lt;/li&gt;
&lt;/ul&gt;
All tools are designed for integration with other tools and each other.
Timeserieswidget gets data from anthracite, graphite and elasticsearch.
Graph-Explorer uses structured_metrics and timeserieswidget.

&lt;h3&gt;Future work&lt;/h3&gt;
There&#39;s a whole lot going on in the monitoring space, but I&#39;d like to highlight a few things I personally want to work more on:
&lt;ul&gt;
&lt;li&gt;
I spoke with Michael Leinartas at Monitorama (and there&#39;s also a &lt;a href=&#34;https://answers.launchpad.net/graphite/+question/223956&#34;&gt;launchpad thread&lt;/a&gt;).
We agreed that native tags in graphite are the way forward.  This will address some of the pain points
I&#39;m already fixing with structured_metrics but in a more native way.
I envision submitting metrics would move from:
&lt;pre&gt;
stats.serverdb123.mysql.queries.selects 895 1234567890
&lt;/pre&gt;
to something more along these lines:
&lt;pre&gt;
host=serverdb123 service=mysql type=select what=queries target_type=rate 895 1234567890
host=serverdb123 service=mysql type=select unit=Queries/s 895 1234567890
h=serverdb123 s=mysql t=select queries r 895 1234567890
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;switch Anthracite backend to ElasticSearch for native integration with logstash data (and allow you to use kibana)&lt;/li&gt;
&lt;/ul&gt;

</description>
    </item>
    
  </channel>
</rss>
