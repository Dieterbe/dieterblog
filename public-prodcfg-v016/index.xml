<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dieter&#39;s blog</title>
    <link>http://dieter.plaetinck.be/</link>
    <description>Recent content on Dieter&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Dec 2016 19:13:03 +0000</lastBuildDate>
    <atom:link href="http://dieter.plaetinck.be/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Practical fault detection: redux. Next-generation alerting now as presentation</title>
      <link>http://dieter.plaetinck.be/post/practical-fault-detection-redux-next-generation-alerting-now-as-presentation/</link>
      <pubDate>Sat, 10 Dec 2016 19:13:03 +0000</pubDate>
      
      <guid>http://dieter.plaetinck.be/post/practical-fault-detection-redux-next-generation-alerting-now-as-presentation/</guid>
      <description>&lt;p&gt;This summer I had the opportunity to present my &lt;a href=&#34;http://dieter.plaetinck.be/post/practical-fault-detection-alerting-dont-need-to-be-data-scientist/&#34;&gt;practical fault detection&lt;/a&gt; concepts and &lt;a href=&#34;http://dieter.plaetinck.be/post/practical-fault-detection-on-timeseries-part-2/&#34;&gt;hands-on approach&lt;/a&gt; as conference presentations.&lt;/p&gt;

&lt;p&gt;First at &lt;a href=&#34;http://conferences.oreilly.com/velocity/vl-ca-2016/public/schedule/detail/49335&#34;&gt;Velocity&lt;/a&gt; and then at &lt;a href=&#34;https://www.usenix.org/conference/srecon16europe/program/presentation/plaetinck&#34;&gt;SRECon16 Europe&lt;/a&gt;.  The latter page also contains the recorded video.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dieter.plaetinck.be/files/poor-mans-fault-detection.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re interested at all in tackling non-trivial timeseries alerting use cases (e.g. working with seasonal or trending data) this video should be useful to you.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s basically me trying to convey in a concrete way why I think the big-data and math-centered algorithmic approaches come with a variety of problems making them unrealistic and unfit,
whereas the real breakthroughs happen when tools recognize the symbiotic relationship between operators and software, and focus on supporting a collaborative, iterative process to managing alerting over time. There should be a harmonious relationship between operator and monitoring tool, leveraging the strengths of both sides, with minimal factors harming the interaction.
From what I can tell, &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun&lt;/a&gt; is pioneering this concept of a modern alerting IDE and is far ahead of other alerting tools in terms of providing high alignment between alerting configuration, the infrastructure being monitored, and individual team members, which are all moving targets, often even fast moving.  In my experience this results in high signal/noise alerts and a happy team.
(according to Kyle, the bosun project leader, my take &lt;a href=&#34;https://twitter.com/kylembrandt/status/804409406846746624&#34;&gt;is a useful one&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;That said, figuring out the tool and using it properly has been, and remains, rather hard.  I know many who rather not fight the learning curve.  Recently the bosun team has been making strides at making it easier for newcomers -
e.g. &lt;a href=&#34;https://github.com/bosun-monitor/bosun/pull/1817&#34;&gt;reloadable configuration&lt;/a&gt; and &lt;a href=&#34;https://grafana.net/plugins/bosun-app&#34;&gt;Grafana integration&lt;/a&gt; - but there is lots more to do.
Part of the reason is that some of the UI tabs aren&amp;rsquo;t implemented for non-opentsdb databases and integrating Graphite for example into the tag-focused system that is bosun, is bound to be a bit weird.  (that&amp;rsquo;s on me)&lt;/p&gt;

&lt;p&gt;For an interesting juxtaposition, we released &lt;a href=&#34;http://docs.grafana.org/guides/whats-new-in-v4/&#34;&gt;Grafana v4 with alerting functionality&lt;/a&gt; which approaches the problem from the complete other side: simplicity and a unified dashboard/alerting workflow first, more advanced alerting methods later.  I&amp;rsquo;m doing what I can to make the ideas of both projects converge, or at least make the projects take inspiration from each other and combine the good parts. (just as I hope to bring the ideas behind &lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;graph-explorer&lt;/a&gt; into Grafana, eventually&amp;hellip;)&lt;/p&gt;

&lt;p&gt;Note:
One thing that somebody correctly pointed out to me, is that I&amp;rsquo;ve been inaccurate with my terminology.
Basically, machine learning and anomaly detection can be as simple or complex as you want to make it. In particular, what we&amp;rsquo;re doing with our alerting software (e.g. bosun) can rightfully also be considered machine learning, since we construct models that learn from data and make predictions.  It may not be what we think of at first, and indeed, even a simple linear regression is a machine learning model.  So most of my critique was more about the big data approach to machine learning, rather than machine learning itself.  As it turns out then the key to applying machine learning successfully is tooling that assists the human operator in every possible way, which is what IDE&amp;rsquo;s like bosun do and how I should have phrased it, rather than presenting it as an alternative to machine learning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Restoring accidental git force push overwrite on GitHub if you don&#39;t have the needed commits locally</title>
      <link>http://dieter.plaetinck.be/post/restoring-accidental-git-force-push-overwrite-on-github-if-dont-have-needed-commits-locally/</link>
      <pubDate>Mon, 14 Nov 2016 11:33:03 +0200</pubDate>
      
      <guid>http://dieter.plaetinck.be/post/restoring-accidental-git-force-push-overwrite-on-github-if-dont-have-needed-commits-locally/</guid>
      <description>&lt;p&gt;I &lt;a href=&#34;http://dieter.plaetinck.be/post/why-rewriting-git-history-and-why-commits-imperative-present-tense/&#34;&gt;like cleaning git history&lt;/a&gt;, in feature branches, at least.
The goal is a set of logical commits without other cruft, that can be cleanly merged into master.  This can be easily achieved with git rebase and force pushing to the feature branch on GitHub.&lt;/p&gt;

&lt;p&gt;Today I had a little accident and found myself in this situation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I accidentally ran &lt;code&gt;git push origin -f&lt;/code&gt; instead of my usual &lt;code&gt;git push origin -f branchname&lt;/code&gt; or &lt;code&gt;git push origin -f HEAD&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;This meant that I not only overwrote the branch I wanted to update, but also by accident a feature branch (called &lt;code&gt;httpRefactor&lt;/code&gt; in this case) to which a colleague had been force pushing various improvements which I did not have on my computer.  And my colleague is on the other side of the world so I didn&amp;rsquo;t want to wait until he wakes up. (if you can talk to someone who has the commits just have him/her re-force-push, that&amp;rsquo;s quite a bit easier than this)
It looked something like so:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;$ git push origin -f
  &amp;lt;here was the force push that succeeded as desired&amp;gt;
+ 92a817d...065bf68 httpRefactor -&amp;gt; httpRefactor (forced update)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Oops!&lt;/strong&gt;
So I wanted to reset the branch on GitHub to what it should be, and also it would be nice to update the local copy on my computer while we&amp;rsquo;re at it.
Note that the commit (or rather the abbreviated hash) on the left refers to the commit that was the latest version in GitHub, i.e. the one I did not have on my computer.
A little strange if you&amp;rsquo;re to accustomed to &lt;code&gt;git diff&lt;/code&gt; and &lt;code&gt;git log&lt;/code&gt; output showing hashes you have in your local repository.&lt;/p&gt;

&lt;p&gt;Normally in a git repository, the objects dangle around until &lt;code&gt;git gc&lt;/code&gt; is run, which clears any commits except those reachable by any branches or tags.
I figured the commit is probably still in the GitHub repo (either cause it&amp;rsquo;s dangling, or perhaps there&amp;rsquo;s a reference to it that&amp;rsquo;s not public such as a remote branch), I just need a way to attach a regular branch to it (either on GitHub, or fetch it somehow to my computer, attach the branch there and re-force-push), so step one is finding it on GitHub.&lt;/p&gt;

&lt;p&gt;The first obstacle is that GitHub wouldn&amp;rsquo;t recognize this abbreviated hash anymore: going to
&lt;code&gt;https://github.com/raintank/metrictank/commit/92a817d&lt;/code&gt; resulted in a 404 commit not found.&lt;/p&gt;

&lt;p&gt;Now, we use CircleCI, so I could see what had been the full commit hash in the CI build log.
Once I had it, I could see that &lt;code&gt;https://github.com/raintank/metrictank/commit/92a817d2ba0b38d3f18b19457f5fe0a706c77370&lt;/code&gt; showed it.
An alternative way of opening a view of the dangling commit we need, is using the reflog syntax.
&lt;a href=&#34;https://git-scm.com/docs/git-reflog&#34;&gt;Git reflog&lt;/a&gt; is a pretty sweet tool that often comes in handy when you made a bit too much of a mess on your local repository,
but also on GitHub it works:  if you navigate to &lt;code&gt;https://github.com/raintank/metrictank/tree/httpRefactor@{1}&lt;/code&gt; you will be presented with the commit
that the branch head was at before the last change, i.e. the missing commit, 92a817d in my case.&lt;/p&gt;

&lt;p&gt;Then follows the problem of re-attaching a branch to it.
Running on my laptop &lt;code&gt;git fetch --all&lt;/code&gt; doesn&amp;rsquo;t seem to fetch dangling objects, so I couldn&amp;rsquo;t bring the object in.&lt;/p&gt;

&lt;p&gt;Then I tried to create a tag for the non-existant object.  I figured, the tag may not reference an object in my repo, but it will on GitHub, so if only I can create the tag, manually if needed (it seems to be just a file containing a commit hash), and push it, I should be good.
So:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~/g/s/g/r/metrictank ❯❯❯ git tag recover 92a817d2ba0b38d3f18b19457f5fe0a706c77370
fatal: cannot update ref &#39;refs/tags/recover&#39;: trying to write ref &#39;refs/tags/recover&#39; with nonexistent object 92a817d2ba0b38d3f18b19457f5fe0a706c77370
~/g/s/g/r/metrictank ❯❯❯ echo 92a817d2ba0b38d3f18b19457f5fe0a706c77370 &amp;gt; .git/refs/tags/recover
~/g/s/g/r/metrictank ❯❯❯ git push origin --tags
error: refs/tags/recover does not point to a valid object!
Everything up-to-date
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So this approach won&amp;rsquo;t work.  I can create the tag, but not push it, even though the object exists on the remote.&lt;/p&gt;

&lt;p&gt;So I was looking for a way to attach a tag or branch to the commit on GitHub, and then I found a way.
While having the view of the needed commit open, click the branch dropdown, which you typically use to switch the view to another branch or tag.
If you type any word in there that does not match any existing branch, it will let you create a branch with that name. So I created &lt;code&gt;recover&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;From then on, it&amp;rsquo;s easy.. on my computer I went into &lt;code&gt;httpRefactor&lt;/code&gt;, backed my version up as httpRefactor-old (so I could diff against my colleague&amp;rsquo;s recent work), deleted httpRefactor, and set it to
the same commit as what origin/recover is pointing to, pushed it out again, and removed the recover branch on GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~/g/s/g/r/metrictank ❯❯❯ git fetch --all
(...)
~/g/s/g/r/metrictank ❯❯❯ git checkout httpRefactor
~/g/s/g/r/metrictank ❯❯❯ git checkout -b httpRefactor-old
Switched to a new branch &#39;httpRefactor-old&#39;
~/g/s/g/r/metrictank ❯❯❯ git branch -D httpRefactor
Deleted branch httpRefactor (was 065bf68).
~/g/s/g/r/metrictank ❯❯❯ git checkout recover
HEAD is now at 92a817d... include response text in error message
~/g/s/g/r/metrictank ❯❯❯ git checkout -b httpRefactor
Switched to a new branch &#39;httpRefactor&#39;
~/g/s/g/r/metrictank ❯❯❯ git push -f origin httpRefactor
Total 0 (delta 0), reused 0 (delta 0)
To github.com:raintank/metrictank.git
 + 065bf68...92a817d httpRefactor -&amp;gt; httpRefactor (forced update)
~/g/s/g/r/metrictank ❯❯❯ git push origin :recover                                                                                                                                            ⏎
To github.com:raintank/metrictank.git
 - [deleted]         recover
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that was that&amp;hellip; If you&amp;rsquo;re ever in this situation and you don&amp;rsquo;t have anyone who can do the force push again, this should help you out.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>25 Graphite, Grafana and statsd gotchas</title>
      <link>http://dieter.plaetinck.be/post/25-graphite-grafana-statsd-gotchas/</link>
      <pubDate>Tue, 15 Mar 2016 16:22:03 +1000</pubDate>
      
      <guid>http://dieter.plaetinck.be/post/25-graphite-grafana-statsd-gotchas/</guid>
      <description>

&lt;p&gt;This is a crosspost of &lt;a href=&#34;https://blog.raintank.io/25-graphite-grafana-and-statsd-gotchas/&#34;&gt;an article I wrote on the raintank.io blog&lt;/a&gt;&lt;br /&gt;
For several years I&amp;rsquo;ve worked with Graphite, Grafana and statsd on a daily basis and have been participating in the community.  All three are fantastic tools and solve very real problems.  Hence my continued use and recommendation.  However, between colleagues, random folks on irc, and personal experience, I&amp;rsquo;ve seen a plethora of often subtle issues, gotchas and insights, which today I&amp;rsquo;d like to share.&lt;/p&gt;

&lt;p&gt;I hope this will prove useful to users while we, open source monitoring developers, work on ironing out these kinks.  At raintank we&amp;rsquo;re addressing a bunch of these as well but we have a long road ahead of us.
&lt;br&gt;
Before we begin, when trying to debug what&amp;rsquo;s going on with your metrics (whether they&amp;rsquo;re going in or out of these tools), don&amp;rsquo;t be afraid to dive into the network or the whisper files. They can often be invaluable in understanding what&amp;rsquo;s up.&lt;/p&gt;

&lt;p&gt;For network sniffing, I almost always use these commands:&lt;br /&gt;
&lt;code&gt;ngrep -d any -W byline port 2003 # carbon traffic
ngrep -d any -W byline port 8125 # statsd traffic&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Getting the json output from Graphite (just append &lt;code&gt;&amp;amp;format=json&lt;/code&gt;) can be very helpful as well. Many dashboards, including &lt;a href=&#34;http://www.grafana.org&#34;&gt;Grafana&lt;/a&gt; already do this, so you can use the browser network inspector to analyze requests. For the whisper files, Graphite comes with various useful utilities such as whisper-info, whisper-dump, whisper-fetch, etc.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;here we go:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;#traffic.drop&#34;&gt;1. OMG! Did all our traffic just drop?&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#null.math.func&#34;&gt;2. Null handling in math functions.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#null.runtime&#34;&gt;3. Null handling during runtime consolidation.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#consolidation.aggregation&#34;&gt;4. No consolidation or aggregation for incoming data.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#limited.aggregation&#34;&gt;5. Limited storage aggregation options.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#runtime.consolidation&#34;&gt;6. Runtime consolidation is detached from storage aggregation.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#grafana.consolidation&#34;&gt;7. Grafana consolidation is detached from storage aggregation and runtime consolidation.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#aggregating.percentiles&#34;&gt;8. Aggregating percentiles.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#deriving.integration&#34;&gt;9. Deriving and integration in Graphite.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#graphite.quantization&#34;&gt;10. Graphite quantization.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#statsd.flush.offset&#34;&gt;11. statsd flush offset depends on when statsd was started.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#time-attributed.metrics&#34;&gt;12. Improperly time-attributed metrics.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#timestamps&#34;&gt;13. The relation between timestamps and the intervals they describe.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#statsd.timing&#34;&gt;14. The statsd timing type is not only for timings.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#metric.keys&#34;&gt;15. The choice of metric keys depends on how you deployed statsd.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#non-blocking.udp.sends&#34;&gt;16. statsd is &amp;ldquo;fire and forget&amp;rdquo;, &amp;ldquo;non-blocking&amp;rdquo; &amp;amp; UDP sends &amp;ldquo;have no overhead&amp;rdquo;.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#statsd.sampling&#34;&gt;17. statsd sampling.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#saturate&#34;&gt;18. As long as my network doesn&amp;rsquo;t saturate, statsd graphs should be accurate.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#gauges&#34;&gt;19. Incrementing/decrementing gauges.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#cant.graph.what.you.havent.seen&#34;&gt;20. You can&amp;rsquo;t graph what you haven&amp;rsquo;t seen&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#nulls.deleteIdleStats&#34;&gt;21. Don&amp;rsquo;t let the data fool you: nulls and deleteIdleStats options.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#keeplastvalue&#34;&gt;22. keepLastValue works&amp;hellip; almost always.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#statsd.counters&#34;&gt;23. statsd counters are not counters in the traditional sense.&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#input&#34;&gt;24. What can I send as input? &lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;#hostnames.ip.addresses&#34;&gt;25. Hostnames and ip addresses in metric keys.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;traffic.drop&#34;&gt;&lt;/span&gt;
1) &lt;a href=&#34;#traffic.drop&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;OMG! Did all our traffic just drop?&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
Probably the most common gotcha, and one i run into periodically. Graphite will return data up until the current time, according to your data schema.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; if your data is stored at 10s resolution, then at 10:02:35, it will show
data up until 10:02:30. Once the clock hits 10:02:40, it will also include that point (10:02:40) in its response.&lt;/p&gt;

&lt;p&gt;It typically takes some time for your services to send data for that timestamp, and for it to be processed by Graphite, so Graphite will typically return a null here for that timestamp. Depending on how you visualize (see for example the &amp;ldquo;null as null/zero&amp;rdquo; option in grafana), or if you use a function such as sumSeries (see below) it may look like a drop in your graph, and cause panic.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can work around this with &amp;ldquo;null as null&amp;rdquo; in Grafana, transformNull() or keepLastValue() in Graphite, or plotting until a few seconds ago instead of now. See below for some other related issues.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;null.math.func&#34;&gt;&lt;/span&gt;
2) &lt;a href=&#34;#null.math.func&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Null handling in math functions.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
Graphite functions don&amp;rsquo;t exactly follow the rules of logic (by design). When you request something like &lt;code&gt;sumSeries(diskspace.server_*.bytes_free)&lt;/code&gt; Graphite returns the sum of all bytes_free&amp;rsquo;s for all your servers, at each point in time. If all of the servers have a null for a given timestamp, the result will be null as well for that time. However, if only some - but not all - of the terms are null, they are counted as 0.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; The sum for a given point in time, 100 + 150 + null + null = 250.&lt;/p&gt;

&lt;p&gt;So when some of the series have the
occasional null, it may look like a drop in the summed series. This especially commonly happens for the last point at each point in time, when your servers don&amp;rsquo;t submit their metrics at the exact same time.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now let&amp;rsquo;s say you work for a major video sharing website that runs many upload and transcoding clusters around the world. And let&amp;rsquo;s also say you build this really cool traffic routing system that sends user video uploads to the most appropriate cluster, by taking into account network speeds, transcoding queue wait times, storage cluster capacity (summed over all servers), infrastructure cost and so forth.  Building this on top of Graphite would make a lot of sense, and in all testing everything works fine, but in production the decisions made by the algorithm seem nonsensical, because the storage capacity seems to fluctuate all over the place.   Let&amp;rsquo;s just say that&amp;rsquo;s  how a certain Belgian engineer became intimately familiar with Graphite&amp;rsquo;s implementation of functions such as sumSeries.  Lesson learned.  No videos were hurt.&lt;/p&gt;

&lt;p&gt;Graphite could be changed to strictly follow the rules of logic, meaning as soon as there&amp;rsquo;s a null in an equation, emit a null as output. But a low number of nulls in a large sum is usually not a big deal and having a  result with the nulls counted as 0 can be more useful than no result at all. Especially when averaging, since those points can typically be safely excluded  without impact on the output. Graphite has the xFilesFactor option in storage-aggregation.conf, which lets you configure the minimum fraction of non-null values, which it uses during storage aggregation (rollups) to determine whether the input is sufficiently known for output to be known, otherwise the output value will be null.&lt;/p&gt;

&lt;p&gt;It would be nice if these on-demand math requests would take an xFilesFactor argument to do the same thing. But for now, just be aware of it, not using the last point is usually all you need if you&amp;rsquo;re only looking at the most recent data. Or get the timing of your agents tighter and only request data from Graphite until now=-5s or so.&lt;/p&gt;

&lt;div class=&#34;intermezzo&#34;&gt;
&lt;strong&gt;INTERMEZZO:&lt;/strong&gt;
At this point we should describe the many forms of consolidation; a good understanding of this is required for the next points.
&lt;br/&gt;&lt;br/&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;storage consolidation aka aggregation, aka rollups&lt;/strong&gt;: data aggregated to optimize the cost of historical archives and make it faster to return large timeframes of data. Driven by storage-aggregation.conf&lt;/li&gt;
&lt;br/&gt;
&lt;li&gt;&lt;strong&gt;runtime consolidation&lt;/strong&gt;: when you want to render a graph but there are more datapoints than pixels for the graph, or more than the maxDataPoints setting. Graphite will reduce the data in the http response. Averages by default, but can be overridden through consolidateBy()&lt;/li&gt;
&lt;br/&gt;
&lt;li&gt;&lt;strong&gt;Grafana consolidation&lt;/strong&gt;: Grafana can visualize stats such as min/max/avg which it computes on the data returned by your timeseries system.&lt;/li&gt;
&lt;br/&gt;
&lt;li&gt;&lt;strong&gt;statsd consolidation&lt;/strong&gt;: statsd is also commonly used to consolidate usually very high rates of irregular incoming data in properly spaced timeframes, before it goes into Graphite.&lt;/li&gt;
&lt;br/&gt;
&lt;li&gt;Unlike all above mechanisms which work per series,
the &lt;a href=&#34;http://graphite.readthedocs.org/en/1.0/functions.html#graphite.render.functions.sumSeries&#34;&gt;sumSeries()&lt;/a&gt;, &lt;a href=&#34;http://graphite.readthedocs.org/en/1.0/functions.html#graphite.render.functions.averageSeries&#34;&gt;saverageSeries()&lt;/a&gt;, &lt;a href=&#34;http://graphite.readthedocs.org/en/1.0/functions.html#graphite.render.functions.groupByNode&#34;&gt;groupByNode()&lt;/a&gt; Graphite functions work
across series. They merge multiple series into a single or fewer series by aggregating  
points from different series at each point in time.&lt;/li&gt;
&lt;br/&gt;
&lt;li&gt;And then there&#39;s &lt;a href=&#34;http://graphite.readthedocs.org/en/latest/carbon-daemons.html#carbon-aggregator-py&#34;&gt;carbon-aggregator&lt;/a&gt; and &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt; which operate on your metrics stream and can aggregate on per-series level, as well as across series, as well as a combination thereof.&lt;/li&gt;

&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;span id=&#34;null.runtime&#34;&gt;&lt;/span&gt;
3) &lt;a href=&#34;#null.runtime&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Null handling during runtime consolidation&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
Similar to above, when Graphite performs runtime consolidation it simply ignores null values. Imagine a series that measures throughput with points 10, 12, 11, null, null, 10, 11, 12, 10. Let&amp;rsquo;s say it needs to aggregate every 3 points with sum. this would return 33, 10, 33. This will visually look like a drop in throughput, even though there probably was none.&lt;/p&gt;

&lt;p&gt;For some functions like avg, a missing value amongst several valid values is usually not a big deal, but the likeliness of it becoming a big deal increases with the amount of nulls, especially with sums. For runtime consolidation, Graphite needs something similar to the xFilesFactor setting for rollups.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;consolidation.aggregation&#34;&gt;&lt;/span&gt;
4) &lt;a href=&#34;#consolidation.aggregation&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;No consolidation or aggregation for incoming data.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
If you submit multiple values into Graphite for the same interval, the last one overwrites any previous values, no consolidation/aggregation happens in this scenario.&lt;/p&gt;

&lt;p&gt;Never send multiple values for the same interval. However, carbon-aggregator or carbon-relay-ng may be of help (see above)&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;limited.aggregation&#34;&gt;&lt;/span&gt;
5) &lt;a href=&#34;#limited.aggregation&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Limited storage aggregation options.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
In storage-aggregation.conf you configure which function to use for historical rollups. It can be limiting that you can only choose one. Often you may want to retain both the max values as well as the averages for example.(very useful for throughput). This feature exists in &lt;a href=&#34;http://oss.oetiker.ch/rrdtool/&#34;&gt;RRDtool&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another issue with this is that often these functions will be misconfigured. Make sure to think about all the possible metrics you&amp;rsquo;ll be sending (e.g. min and max values through statsd) and set up the right configuration for them.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.librato.com/posts/time-series-data&#34;&gt;Proprietary systems tend to be more flexible&lt;/a&gt; and I&amp;rsquo;m sure it will make a come back in the Graphite stack as well. (more on that later)&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;runtime.consolidation&#34;&gt;&lt;/span&gt;
6) &lt;a href=&#34;#runtime.consolidation&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Runtime consolidation is detached from storage aggregation.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
The function chosen in storage-aggregation.conf is only used for rollups.&lt;/p&gt;

&lt;p&gt;If Graphite performs any runtime consolidation it will always use average unless told otherwise through consolidateBy.  This means it&amp;rsquo;s easy to run into cases where data is rolled up (in whisper) using, say, max or count,  but then accidentally with average while creating your visualization, resulting in incorrect information and nonsensical charts. Beware!&lt;/p&gt;

&lt;p&gt;It would be nice if the configured roll-up function would also apply here (and also the xFilesFactor, as above),
For now, just be careful :)&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;grafana.consolidation&#34;&gt;&lt;/span&gt;
7) &lt;a href=&#34;#grafana.consolidation&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Grafana consolidation is detached from storage aggregation and runtime consolidation.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
Like mentioned earlier, Grafana can provide min/max/avg/&amp;hellip; values from the received data. For now, it just computes these from the received data, which may already have been consolidated (twice: in storage and in runtime) using different functions, so these results may not be always representative. (&lt;a href=&#34;http://play.grafana.org/dashboard/db/ultimate-graphite-query-guide&#34;&gt;more info&lt;/a&gt;) We will make this mechanism more powerful and accurate.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;aggregating.percentiles&#34;&gt;&lt;/span&gt;
8) &lt;a href=&#34;#aggregating.percentiles&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Aggregating percentiles.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
The pet peeve of many, it has been written about a lot: If you have percentiles, such as those collected by statsd, (e.g. 95th percentile response time for each server) there is in theory no proper way to aggregate those numbers.&lt;/p&gt;

&lt;p&gt;This issue appears when rolling up the data in the storage layer, as well when doing runtime consolidation, or when  trying to combine the data for all servers (multiple series) together. There is real math behind this, and I&amp;rsquo;m not going into it because in practice it actually doesn&amp;rsquo;t seem to matter much. If you&amp;rsquo;re trying to spot issues or alert on outliers you&amp;rsquo;ll get quite far with averaging the data together or taking the max value seen. This is especially true when the amount of requests, represented by each latency measurement, is in the same order of magnitude.  E.g. You&amp;rsquo;re averaging per-server latency percentiles and your servers get a similar load or you&amp;rsquo;re averaging multiple points in time for one server, but there was a similar load at each point.  You can always set up a separate alerting rule for unbalanced servers or drops in throughput.  As we&amp;rsquo;ll see in some of the other points, taking sensible shortcuts instead of obsessing over math is often the better way to accomplish your job of operating your infrastructure or software.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;deriving.integration&#34;&gt;&lt;/span&gt;
9) &lt;a href=&#34;#deriving.integration&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Deriving and integration in Graphite.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
The Graphite documentation for &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.15/functions.html#graphite.render.functions.derivative&#34;&gt;derivative()&lt;/a&gt; hints at this already (&amp;ldquo;This function does not normalize for periods of time, as a true derivative would.&amp;rdquo;), but to be  entirely clear:
&lt;ul&gt;
&lt;li&gt;Graphite&amp;rsquo;s &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.15/functions.html#graphite.render.functions.derivative&#34;&gt;derivative&lt;/a&gt; is not a derivative. A &lt;a href=&#34;https://en.wikipedia.org/wiki/Derivative&#34;&gt;derivative&lt;/a&gt; divides difference in value by difference in time.  Graphite&amp;rsquo;s derivative just returns the value deltas. Similar for &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.15/functions.html#graphite.render.functions.nonNegativeDerivative&#34;&gt;nonNegativeDerivative()&lt;/a&gt;.  If you want an actual derivative, use the somewhat awkwardly named &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.15/functions.html#graphite.render.functions.perSecond&#34;&gt;perSecond()&lt;/a&gt; function.&lt;/li&gt;
&lt;li&gt;Graphite&amp;rsquo;s integral is not an integral either. An &lt;a href=&#34;https://en.wikipedia.org/wiki/Integral&#34;&gt;integral sums the multiplications of the value difference with the time span&lt;/a&gt;.  &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.15/functions.html#graphite.render.functions.integral&#34;&gt;Graphite&amp;rsquo;s integral&lt;/a&gt; just adds up the value differences.  To my knowledge there&amp;rsquo;s no proper way to do an actual integral, but luckily this is an uncommon operation anyway. Note also that Graphite&amp;rsquo;s integral just skips null values, so a value at each  point in time can be lower than it should, due to nulls that preceded it.&lt;/li&gt;
&lt;/ul&gt;
Maybe for a future stable Graphite release this can be reworked.  for now, just something to be aware of.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;graphite.quantization&#34;&gt;&lt;/span&gt;
10) &lt;a href=&#34;#graphite.quantization&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Graphite quantization.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
We already saw in the consolidation paragraph that for multiple points per interval, last write wins. But you should also know that any data point submitted gets the timestamp rounded down.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: You record points every 10 seconds but submit a point with timestamp at 10:02:59, in Graphite this will be stored at 10:02:50. To be more precise if you submit points at 10:02:52, 10:02:55 and 10:02:59, and have 10s resolution, Graphite will pick the point from 10:02:59 but store it at 10:02:50. So it&amp;rsquo;s important that you make sure to submit points at consistent intervals, aligned to your Graphite retention intervals (e.g. if every 10s, submit on timestamps divisible by 10)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span id=&#34;statsd.flush.offset&#34;&gt;&lt;/span&gt;
11) &lt;a href=&#34;#statsd.flush.offset&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;statsd flush offset depends on when statsd was started.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
Statsd lets you configure a flushInterval, i.e. how often it should compute the output stats and submit them to your backend. However, the exact timing is pretty arbitrary and depends on when statsd is  started.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: If you start statsd at 10:02:00 with a flushInterval of 10, then it
will emit values with timestamps at 10:02:10, 10:02:20, etc (this is what you want), but if you happened to start it at 10:02:09 then it will submit values with timestamps 10:02:19, 10:02:29, etc. So typically, the values sent by statsd are subject to Graphite&amp;rsquo;s quantization and are moved into the past.  In the latter example, by 9 seconds.  This can make troubleshooting harder, especially when comparing statsd metrics to metrics from a different service.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Note: &lt;a href=&#34;https://github.com/vimeo/statsdaemon&#34;&gt;vimeo/statsdaemon&lt;/a&gt; (and possibly other servers as well) will always submit values at quantized intervals  so that it&amp;rsquo;s guaranteed to map exactly to Graphite&amp;rsquo;s timestamps as long as the interval is correct.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;time-attributed.metrics&#34;&gt;&lt;/span&gt;
12) &lt;a href=&#34;#time-attributed.metrics&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Improperly time-attributed metrics.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
You don&amp;rsquo;t tell statsd the timestamps of when things happened.  Statsd applies its own timestamp when it flushes the data. So this is prone to various (mostly network) delays. This could result in a metric being generated in a certain interval only arriving in statsd after the next interval has started. But it can get worse.  Let&amp;rsquo;s say you measure how long it takes to execute a query on a database server. Typically it takes 100ms but let&amp;rsquo;s say now many queries are taking 60s, and you have a flushInterval of 10s. Note that the metric is only sent after the full query has completed. So during the full minute where queries were slow, there are no  metrics (or only some metrics that look good, they came through cause they were part of a group of queries that managed to execute timely), and only after a minute do you get the stats that reflect queries  spawned a minute ago. The higher a timing value, the higher the attribution error, the more into the past the values it represents and the longer the issue will go undetected or invisible.&lt;/p&gt;

&lt;p&gt;Keep in mind that other things, such as garbage collection cycles or paused goroutines under cpu saturation may also delay your metrics reporting. Also watch out for the queries aborting all together, causing the metrics never to be sent and these faults to be invisble! Make sure you properly monitor throughput and the functioning (timeouts, errors, etc) of the service from the client perspective, to get a more accurate picture.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note that many instrumentation libraries have similar issues.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;timestamps&#34;&gt;&lt;/span&gt;
13) &lt;a href=&#34;#timestamps&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;The relation between timestamps and the intervals they describe.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
When I look at a point at a graph that represents a spike in latency, a drop in throughput, or anything interesting really, I always wonder whether it describes the timeframe before, or after it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: With points every minute, and a spike at 10:17:00, does it mean the spike happened in the timeframe between 10:16 and 10:17, or between 10:17 and 10:18?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I don&amp;rsquo;t know if a terminology for this exists already, but I&amp;rsquo;m going to call the former &lt;strong&gt;&lt;em&gt;premarking&lt;/em&gt;&lt;/strong&gt; (data described by a timestamp that precedes the observation period) and the latter &lt;strong&gt;&lt;em&gt;postmarking&lt;/em&gt;&lt;/strong&gt; (data described by a timestamp at the end). As we already saw statsd postmarks, and many tools seem to do this, but some, including Graphite, premark.&lt;/p&gt;

&lt;p&gt;We saw above that any data received by Graphite for a point in between an interval is adjusted to get the timestamp at the beginning of the interval. Furthermore, during aggregation (say, aggregating sets of 10 minutely points into 10min points), each 10 minutes taken together get assigned the timestamp that precedes those 10 intervals. (i.e. the timestamp of the first point)&lt;/p&gt;

&lt;p&gt;So essentially, Graphite likes to show metric values before they actually happened, especially after aggregation, whereas other tools rather use a timestamp in the future of the event than in the past. As a monitoring community, we should probably standardize on an approach. I personally favor postmarking because measurements being late is fairly intuitive, predicting the future not so much.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;statsd.timing&#34;&gt;&lt;/span&gt;
14) &lt;a href=&#34;#statsd.timing&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;The statsd timing type is not only for timings.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
The naming is a bit confusing, but anything you want to compute summary statistics (min, max, mean, percentiles, etc) for (for example message or packet sizes) can be submitted as a timing metric. You&amp;rsquo;ll get your  summary stats just fine. The type is just named &amp;ldquo;timing&amp;rdquo; because that was the original (and still most  common) use case. Note that if you want to time an operation that happens at consistent intervals,  you may just as well simply use a statsd gauge for it. (or write directly to Graphite)&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;metric.keys&#34;&gt;&lt;/span&gt;
15) &lt;a href=&#34;#metric.keys&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;The choice of metric keys depends on how you deployed statsd.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
It&amp;rsquo;s common to deploy a statsd server per machine or per cluster. This is a convenient way to assure you have a lot of processing capacity. However, if multiple statsd servers receive the same metric, they will do their own independent computations and emit the same output metric, overwriting each other. So if you run a statsd server per host, you should include the host in the metrics you&amp;rsquo;re sending into statsd, or into the prefixStats (or similar option) for your statsd server, so that statsd itself differentiates the metrics.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: there&amp;rsquo;s some other statsd options to diversify metrics but the global
prefix is the most simple and common one used.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;non-blocking.udp.sends&#34;&gt;&lt;/span&gt;
16) &lt;a href=&#34;#non-blocking.udp.sends&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;statsd is &amp;ldquo;fire and forget&amp;rdquo;, &amp;ldquo;non-blocking&amp;rdquo; &amp;amp; UDP sends &amp;ldquo;have no overhead&amp;rdquo;.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This probably stems from this snippet in the &lt;a href=&#34;https://codeascraft.com/2011/02/15/measure-anything-measure-everything/&#34;&gt;original statsd announcement&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;q class=&#34;cite&#34;&gt;&amp;ldquo;[UDP is] fast — you don’t want to slow your application down in order to track its performance — but also sending a UDP packet is fire-and-forget.&amp;rdquo;&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;UDP sends do have an overhead that can slow your application down. I think a more accurate description is that the overhead is insignificant either if you&amp;rsquo;re lucky, or if you&amp;rsquo;ve spent a lot of attention to the details. In specific, it depends on the following factors:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Your network stack.&lt;/strong&gt;  UDP sends are not asynchronous, which is a common assumption.  (i.e. they are not &amp;ldquo;non-blocking&amp;rdquo;) Udp write calls from userspace will block until the kernel has moved the data  through the networking stack, firewall rules, through the network driver, onto the network.
I found this out the hard way once when an application I was testing on my laptop ran much slower than expected due to my consumer grade WiFi adapter, which was holding back my program by hanging on statsd calls. Using this &lt;a href&#34;http://play.golang.org/p/1YOZ1oxB-d&#34;&gt;simple go program&lt;/a&gt; I found that I could do 115 k/s UDP sends to localhost (where the kernel can bypass some of the network stack), but only 2k/s to a remote host (where it needs to go through all layers, including your NIC).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;other code running, and its performance.&lt;/strong&gt;  If you have a PHP code base with some statsd calls amongst slow / data-intensive code to generate complex pages, there won&amp;rsquo;t be much difference. This is a common, and the original setting, for statsd instrumentation calls. But I have conducted cpu profiling of high-performance Golang
applications, both at Vimeo and raintank, and when statsd was used, the apps were often spending most of their time in UDP writes caused by statsd invocations. Typically they were using a simple statsd client, where each statsd call  corresponds to a UDP write.  There are some statsd clients that buffer messages and send batches of data to statsd, reducing the UDP writes, which is a lot more efficient. (see below). You can also use sampling to lower the volume.  This does come with some problems however, see the gotcha below.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;The performance of client code itself can wildly vary as well.&lt;/strong&gt; Besides how it maps metric updates to UDP writes (see above) the invocations themselves can carry an overhead, which as you can see through &lt;a href=&#34;https://github.com/Dieterbe/statsdbench&#34;&gt;these benchmarks&lt;/a&gt; of  all Go statsd clients I&amp;rsquo;m aware of, vary anywhere between 15 microseconds to 0.4 microseconds. &lt;br /&gt;&lt;br/&gt;
When I saw this, &lt;a href=&#34;https://github.com/alexcesaro/statsd&#34;&gt; Alex Cesaro&amp;rsquo;s statsd client&lt;/a&gt; promptly became my favorite statsd library for Go.  It has zero-allocation logic and various performance tweaks, such as the much needed client side buffering. Another interesting one is &lt;a href=&#34;https://github.com/quipo/statsd&#34;&gt;quipo&amp;rsquo;s&lt;/a&gt;, which offloads some of the computations into the client to reduce traffic and workload for the server.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Finally, it should be noted that sending to non-listening destinations may
make your application slower.&lt;/strong&gt;
This is because destination hosts receiving UDP traffic for a closed socket, will return ICMP rejects to the sender, which the sender then has to process. I&amp;rsquo;ve not seen this with my own eyes, but the people on the #go-nuts channel who  were helping me out definitely seemed to know what they&amp;rsquo;re talking about. Sometimes &amp;ldquo;disabling&amp;rdquo; statsd sends is implemented by pointing to a random host or port that&amp;rsquo;s not listening (been there done that) and is hence not the best idea!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Takeaway: If you use statsd in high-performance situations, use optimized client libraries . Or use client libraries such as go-metrics, but be prepared to pay a processing tax on all of your servers. Be aware that slow NICs and sending to non-listening destinations will slow your app down.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;statsd.sampling&#34;&gt;&lt;/span&gt;
17) &lt;a href=&#34;#statsd.sampling&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;statsd sampling.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
The sampling feature was part of the original statsd, so it can be seen in pretty much every client and server alternative.  The idea sounds simple enough: If a certain statsd invocation causes too much statsd network traffic or  overloads the server, sample down the messages. However, there&amp;rsquo;s several gotchas with this.  A common invocation with a typical  statsd client looks something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;statsd.Increment(&amp;quot;requests.$backend.$http_response&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this code, we have 1 statement, and it increments a counter to track http response codes for different backends.  It works for different values of backend and http_response, tracking the counts for each. However, let&amp;rsquo;s say the backend &amp;ldquo;default&amp;rdquo; and http_response 200 are causing too much statsd traffic. It&amp;rsquo;s easy to just sample down and change the line to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;statsd.Increment(&amp;quot;requests.$backend.$http_response&amp;quot;, 0.1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s 3 problems here though:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;How do you select a good sampling ratio? It&amp;rsquo;s usually based on a gut feeling or some very quick math at best to come up with a reasonable number, without any attention to the impact on statistical significance, so you may be hurting the quality of your data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;While you mainly wanted to target the highest volume metric (something like &amp;ldquo;requests.default.200&amp;rdquo;), your sampling setting affects the metrics for all values of backend and http_response.  Those variable combinations that are less  frequent (for a backend that&amp;rsquo;s rarely used or a response code not commonly seen)  will be too drastically sampled down, resulting in an inaccurate view of those cases. Worst case, you won&amp;rsquo;t see any data over the course over a longer time period, even though there were points at every interval. You can add some code to use different invocations with different sample rates based on the values of the variables, but that can get kludgy.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Volumes of metric calls tend to constantly evolve over time.  It&amp;rsquo;s simply not feasible to be constantly manually adjusting sample rates.  And given the above, it&amp;rsquo;s not a good solution anyway.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One of the ideas I wanted to implement in &lt;a href=&#34;https://github.com/vimeo/statsdaemon&#34;&gt;https://github.com/vimeo/statsdaemon&lt;/a&gt; was a feedback mechanism where the server would maintain counts of received messages for each key. Based on performance criteria and standard deviation measurements, it would periodically update &amp;ldquo;recommended sampling rates&amp;rdquo; for every single key, which then could be fed back to the clients (by compiling into a PHP config file for example, or over a TCP connection).   Ultimately I decided it was just too complicated and deployed a statsdaemon server on every single machine so I could sample as gently as possible.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Takeaway: Sampling can be used to lower load, but be aware of the trade offs.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;saturate&#34;&gt;&lt;/span&gt;
18) &lt;a href=&#34;#saturate&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;As long as my network doesn&amp;rsquo;t saturate, statsd graphs should be accurate.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
We all know that graphs from statsd are not guaranteed to be accurate, because it uses UDP, so is prone to data loss and inaccurate graphs. And we&amp;rsquo;re OK with that trade  off, because the convenience is worth the loss in accuracy. However, it&amp;rsquo;s commonly believed that the only, or main reason of inaccurate  graphs is UDP data loss on the network. In my experience, I&amp;rsquo;ve seen a lot more message loss due to the UDP buffer overflowing on the statsd server.&lt;/p&gt;

&lt;p&gt;A typical statsd server is constantly receiving a flood of metrics. For a network socket, the Linux kernel maintains a buffer in which it stores newly received network packets, while the listening application (statsd) reads  them out of the buffer.  But if for any reason the application can&amp;rsquo;t read from the buffer fast enough and it fills up, the kernel has to drop incoming traffic. The most common case are the statsd flushes, which compute all summary statistics and submit them each flushInterval. Typically this operation takes between 50ms and a few seconds, depending mostly on how many timing metrics were received, because those are the most computationally expensive.  Usually statsd&amp;rsquo;s cpu is maxed out during this operation, but you won&amp;rsquo;t see it on  your cpu graphs because your cpu measurements are averages taken at intervals that typically don&amp;rsquo;t exactly coincide with the statsd flush-induced spikes. During this time, it temporarily cannot read from the UDP buffer, and it is very  common for the UDP buffer to rapidly fill up with new packets, at which point the Linux kernel is forced to drop incoming network packets for the UDP socket. In my experience this is very common and often even goes undetected, because  only a portion of the metrics are lost, and the graphs are not obviously wrong. Needless to say, there&amp;rsquo;s also plenty of other scenarios for the buffer to fill up and cause traffic drops other than statsd flushing.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s two things to be done here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Luckily we can tune the size of the incoming UDP buffer with the net.core.rmem_max and net.core.rmem_default sysctl&amp;rsquo;s. I recommend setting it to the &lt;a href=&#34;http://stackoverflow.com/questions/2090850/specifying-udp-receive-buffer-size-at-runtime-in-linux&#34;&gt;max values&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The kernel increments a counter every time it drops a UDP packet. You can see UDP packet drops with &lt;code&gt;netstat -s &amp;ndash;udp&lt;/code&gt; on the command  line. I highly encourage you to monitor this with something like collectd, visualize  it over time, and make sure to set an alert on it.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Note also that the larger of a buffer you need, the more delay between receiving a metric and including it in the output data, possibly skewing data into the future.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&#34;intermezzo&#34;&gt;
&lt;strong&gt;INTERMEZZO:&lt;/strong&gt;
There&#39;s a lot of good statsd servers out there with various feature  sets. Most of them provide improved performance (upstream statsd is single threaded JavaScript), but some come with additional interesting features. My favorites include &lt;a href=&#34;https://github.com/armon/statsite&#34;&gt;statsite&lt;/a&gt;, &lt;a href=&#34;http://githubengineering.com/brubeck/&#34;&gt;brubeck&lt;/a&gt; and of course &lt;a href=&#34;https://github.com/vimeo/statsdaemon&#34;&gt;vimeo&#39;s statsdaemon version&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;span id=&#34;gauges&#34;&gt;&lt;/span&gt;
19) &lt;a href=&#34;#gauges&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Incrementing/decrementing gauges.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
Statsd supports a syntax to &lt;a href=&#34;https://github.com/etsy/statsd/blob/master/docs/metric_types.md#gauges&#34;&gt; increment and decrement gauges&lt;/a&gt;. However, as we&amp;rsquo;ve seen, any message can be lost. If you do this and a message gets dropped, your gauge value will be incorrect forever (or until you set it explicitly again). Also by sending increment/decrement values you can confuse a statsd server if it was just started. For this reason, I highly recommend not using this particular feature, and always setting gauge values explicitly.  In fact, some statsd servers don&amp;rsquo;t support this syntax for this reason.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;cant.graph.what.you.havent.seen&#34;&gt;&lt;/span&gt;
20) &lt;a href=&#34;#cant.graph.what.you.havent.seen&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;You can&amp;rsquo;t graph what you haven&amp;rsquo;t seen&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
Taking the earlier example again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;statsd.Increment(&amp;quot;requests.$backend.$http_response&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The metrics will only be created after the values have been sent. Sounds obvious, but this can get surprisingly annoying. Let&amp;rsquo;s say you want to make a graph of http 500&amp;rsquo;s, but they haven&amp;rsquo;t happened yet. The metrics won&amp;rsquo;t exist and with many  dashboarding tools, you won&amp;rsquo;t be able to create the graph yet, or at least need extra  work.&lt;/p&gt;

&lt;p&gt;I see this very commonly with all kinds of error metrics.  I think with Grafana it will become easier to deal with this over time,  but for now I write code to just send each metric once at startup.  For counts you can just send a 0, for gauges and timers it&amp;rsquo;s of course harder/weirder to come up with a fake value but it&amp;rsquo;s still a  reasonable approach.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;nulls.deleteIdleStats&#34;&gt;&lt;/span&gt;
21) &lt;a href=&#34;#nulls.deleteIdleStats&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Don&amp;rsquo;t let the data fool you: nulls and deleteIdleStats options.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
statsd has the deleteIdleStats, deleteGauges, and similar options. By default they are disabled, meaning statsd will keep sending data for metrics it&amp;rsquo;s no longer seeing. So for example, it&amp;rsquo;ll keep sending the last gauge value even if it didn&amp;rsquo;t get an update. I have three issues with this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Your graphs are misleading.&lt;/strong&gt; Your service may be dead but you can&amp;rsquo;t tell from
the gauge graphs cause they will still &amp;ldquo;look good&amp;rdquo;. If you have something like  counters for requests handled, you can&amp;rsquo;t tell the difference between your  service being dead or it being up but not getting incoming requests.  I rather have no data show up if there is none, combined with something else (alerting, different graph) to deduce whether the service is up or not&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Any alerting rules defined on the metrics have the same fate as humans and
can&amp;rsquo;t reliably do their work&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Tougher to phase out metrics.&lt;/strong&gt;  If you decommission servers or services, it&amp;rsquo;s nice that the metrics stop updating so you can clean them up later&lt;/li&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My recommendation is, enable deleteIdleStats.  I went as far as hard coding that behavior into &lt;a  href=&#34;https://github.com/vimeo/statsdaemon&#34;&gt;vimeo/statsdaemon&lt;/a&gt; and not making it configurable.&lt;/p&gt;

&lt;p&gt;To visualize it, make sure in Grafana to set the &amp;ldquo;null as null&amp;rdquo; option, though &amp;ldquo;null as zero&amp;rdquo; can make sense for counters. You can also use the &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.15/functions.html#graphite.render.functions.transformNull&#34;&gt;transformNull()&lt;/a&gt; or &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.15/render_api.html#drawnullaszero&#34;&gt;drawNullAsZero&lt;/a&gt; options in Graphite for those  cases where you want to explicitly get 0&amp;rsquo;s instead of nulls, which I typically  do in some of my bosun alerting rules, so I can treat null counts as no traffic received, while having a separate alerting rule to make sure my service is up  and running, based on a different metric, while using null as null for visualization.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;keeplastvalue&#34;&gt;&lt;/span&gt;
22) &lt;a href=&#34;#keeplastvalue&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;keepLastValue works&amp;hellip; almost always.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
Another Graphite function to change the semantics of nulls is &lt;a href=&#34;http://graphite.readthedocs.org/en/latest/functions.html#graphite.render.functions.keepLastValue&#34;&gt; keepLastValue&lt;/a&gt;, which causes null values to be represented by the last known value that precedes them. However, that known value must be included in the requested time range.  This is probably a rare case that you may never encounter, but if you have scripts that infrequently update a metric and you use this function, it may result in a graph sometimes showing no data at all if the last known value becomes too old.  Especially confusing to newcomers, if the graph does &amp;ldquo;work&amp;rdquo; at other times.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;statsd.counters&#34;&gt;&lt;/span&gt;
23) &lt;a href=&#34;#statsd.counters&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;statsd counters are not counters in the traditional sense.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
You may know counters such as switch traffic/packet counters, that just keep increasing over time. You can see their rates per second by deriving the data. Statsd counters however, are typically either stored as the number of hits per flushInterval, or per second, or both (perhaps because of Graphite&amp;rsquo;s derivative issue?). This in itself is not a huge problem, however this is more vulnerable to loss of data.  If your statsd server has trouble flushing some data to Graphite and some data gets lost.  If it were using a traditional counter, you could still derive the data and average out the gap across the nulls. In this case however this is not possible and you have no idea what the numbers were.  In practice this doesn&amp;rsquo;t happen often though.  Many statsd implementations can buffer a writequeue or use something like &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt; carbon-relay-ng&lt;/a&gt; as a write queue.
Other disadvantages of this approach is that you need to know the flushInterval
value to make sense of the count values, and the rounding that happens when
computing the rates per second, which is also slightly lossy.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;input&#34;&gt;&lt;/span&gt;
24) &lt;a href=&#34;#input&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;What can I send as input?&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
Neither &lt;a href=&#34;http://graphite.readthedocs.org/en/latest/feeding-carbon.html&#34;&gt;Graphite&lt;/a&gt;, nor &lt;a href=&#34;https://github.com/etsy/statsd/blob/master/docs/metric_types.md&#34;&gt;statsd&lt;/a&gt; does a great job specifying exactly what they allow as input, which can be frustrating.  Graphite timestamps are 32bit unix timestamp integers, while values for both Graphite and statsd can be integers or floats, up to float 64bit precision. For statsd, see the &lt;a href&#34;https://github.com/b/statsd_spec&#34;&gt;statsd_spec&lt;/a&gt; project for more details.&lt;/p&gt;

&lt;p&gt;As for what characters can be included in the metric keys. Generally, Graphite is somewhat forgiving and may alter your metric keys: it converts slashes to dots (which can be confusing), subsequent dots become single dots, prefix dots get removed, postfix dots will get it confused a bit though and create an extra hierarchy with an empty node at the end if you&amp;rsquo;re using whisper. Non-alphanumeric characters such as &lt;a href=&#34;https://github.com/graphite-project/graphite-web/issues/242&#34;&gt;parenthesis&lt;/a&gt;, &lt;a href=&#34;https://github.com/graphite-project/graphite web/issues/604&#34;&gt;colons&lt;/a&gt;, or &lt;a href=&#34;https://github.com/brutasse/graphite-api/issues/57&#34;&gt; equals signs&lt;/a&gt; often don&amp;rsquo;t work well or at all.&lt;/p&gt;

&lt;p&gt;Graphite as a policy does not go far in validating incoming data, citing performance in large-throughput systems as the major reason. It&amp;rsquo;s up to the senders to send data in proper form, with non-empty nodes (words between dots) separated by single dots.  You can use alphanumeric characters, hyphens and underscores, but straying from that will probably not work well. You may want to use &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt; carbon-relay-ng&lt;/a&gt; which provides metric validation. See also &lt;a href=&#34;https://github.com/graphite-project/carbon/issues/417&#34;&gt;this issue&lt;/a&gt; which aims to formalize what is, and isn&amp;rsquo;t allowed.&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;hostnames.ip.addresses&#34;&gt;&lt;/span&gt;
25) &lt;a href=&#34;#hostnames.ip.addresses&#34; class=&#34;anchor-link&#34;&gt;&lt;strong&gt;Hostnames and ip addresses in metric keys.&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
The last gotcha relates to the previous one but is so common it deserves its own spot. Hostnames, especially FQDN&amp;rsquo;s, and IP addresses (due to their dots), when included in metric keys, will be interpreted by Graphite as separate pieces. Typically, people replace all dots in hostnames and IP addresses with hyphens or underscores to combat this.&lt;/p&gt;

&lt;h3 id=&#34;closing-thoughts&#34;&gt;Closing thoughts:&lt;/h3&gt;

&lt;p&gt;Graphite, Grafana and statsd are great tools, but there&amp;rsquo;s some things to watch out for. When setting them up, make sure you configure Graphite and statsd to play well together. Make sure to set your roll-up functions and data retentions properly, and  whichever statsd version you decide to use, make sure it flushes at the same  interval as your Graphite schema configuration. &lt;a  href=&#34;https://github.com/etsy/statsd/blob/master/docs/graphite.md&#34;&gt;More tips if you use the nodejs version&lt;/a&gt;. Furthermore, I hope this is an extensive list of gotchas and will serve to improve our practices at first, and our tools themselves, down to road. We are definitely working on making things better and have some announcements coming up&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interview with Matt Reiferson, creator of NSQ</title>
      <link>http://dieter.plaetinck.be/post/interview-matt-reiferson-nsq/</link>
      <pubDate>Fri, 02 Oct 2015 10:25:02 +0200</pubDate>
      
      <guid>http://dieter.plaetinck.be/post/interview-matt-reiferson-nsq/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m a fan of the &lt;a href=&#34;http://nsq.io/&#34;&gt;NSQ&lt;/a&gt; message processing system written in golang.
I&amp;rsquo;ve studied the code, &lt;a href=&#34;http://dieter.plaetinck.be/post/transplanting-go-packages-for-fun-and-profit/&#34;&gt;transplanted its diskqueue code&lt;/a&gt; into another project, and have used NSQ by itself.
The code is well thought out, organized and written.&lt;/p&gt;

&lt;p&gt;Inspired by the book &lt;a href=&#34;http://codersatwork.com/&#34;&gt;coders at work&lt;/a&gt; and the &lt;a href=&#34;http://systemslive.org/&#34;&gt;systems live&lt;/a&gt; podcast,
I wanted to try something I&amp;rsquo;ve never done before: spend an hour talking to &lt;a href=&#34;https://github.com/mreiferson&#34;&gt;Matt Reiferson&lt;/a&gt; - the main author of NSQ - about software design and Go programming patterns,
and post the video online for whomever might be interested.&lt;/p&gt;

&lt;p&gt;We talked about Matt&amp;rsquo;s background, starting the NSQ project at Bitly as his first (!) Go project,
(code) design patterns in NSQ and the nsqd diskqueue in particular and the new &lt;a href=&#34;https://github.com/nsqio/nsq/pull/625&#34;&gt;WAL&lt;/a&gt; (write-ahead-log) approach in terms of design and functionality.&lt;/p&gt;

&lt;p&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/-X73gfrt8Qk&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
You can watch it &lt;a href=&#34;https://www.youtube.com/watch?v=-X73gfrt8Qk&#34;&gt;on youtube&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, the video got cut a bit short.
But basically in the cut off part i asked about the new go internals convention that prevents importing packages that are in an internals subdirectory.
Matt wants to make it very clear that certain implementation details are not supported (by the NSQ team) and may change, whereas my take was that it&amp;rsquo;s annoying
when i want to reuse code some I find in a project.
We ultimately both agreed that while a bit clunky, it gets the job done, and is probably a bit crude because there is also no proper package management yet.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ld like to occasionally interview other programmers in a similar way and post on my site later.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transplanting Go packages for fun and profit</title>
      <link>http://dieter.plaetinck.be/post/transplanting-go-packages-for-fun-and-profit/</link>
      <pubDate>Wed, 02 Sep 2015 19:25:02 +0300</pubDate>
      
      <guid>http://dieter.plaetinck.be/post/transplanting-go-packages-for-fun-and-profit/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://blog.raintank.io/content/images/2015/09/transplant_blog.jpg&#34; alt=&#34;crazy Gopher scientist&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A while back I read &lt;a href=&#34;http://codersatwork.com/&#34;&gt;coders at work&lt;/a&gt;, which is a book of interviews with some great computer scientists who earned their stripes, the questions just as thoughtful as the answers.
For one thing, it re-ignited my interest in functional programming, for another I got interested in &lt;a href=&#34;https://en.wikipedia.org/wiki/Literate_programming&#34;&gt;literate programming&lt;/a&gt; but most of all, it struck me how common of a recommendation it was to read other people&amp;rsquo;s code as a means to become a better programmer.
(It also has a good section of &lt;a href=&#34;http://bradfitz.com&#34;&gt;Brad Fitzpatrick&lt;/a&gt; describing his dislike for programming languages, and dreaming about his ideal language. This must have been shortly before Go came about and he became a maintainer.)&lt;/p&gt;

&lt;p&gt;I hadn&amp;rsquo;t been doing a good job reading/studying other code out of fear that inferior patterns/style would rub off on me.  But I soon realized that was an irrational, perhaps slightly absurd excuse. So I made the decision to change. Contrary to my presumption I found that by reading code that looks bad you can challenge and re-evaluate your mindset and get out with a more nuanced understanding and awareness of the pros and cons of various approaches.&lt;/p&gt;

&lt;p&gt;I also realized if code is proving too hard to get into or is of too low quality, you can switch to another code base with negligible effort and end up spending almost all of your time reading code that is worthwhile and has plenty of learnings to offer.  There is a lot of high quality Go code, easy to find through sites like Github or &lt;a href=&#34;http://golangweekly.com/&#34;&gt;Golang weekly&lt;/a&gt;, just follow your interests and pick a project to start reading.&lt;/p&gt;

&lt;p&gt;It gets really interesting though once you find bodies of code that are not only a nice learning resource, but can be transplanted into your code with minimal work to solve a problem you&amp;rsquo;re having, but in a different context then the author of the code originally designed it for.  Components often grow and mature in the context of an application without being promoted as reusable libraries, but you can often use them as if they were.  I would like to share 2 such success cases below.&lt;/p&gt;

&lt;h1 id=&#34;nsq-s-diskqueue-code&#34;&gt;Nsq&amp;rsquo;s diskqueue code&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;ve always had an interest in code that manages the same binary data both in memory and on a block device.  Think filesystems, databases, etc.  There&amp;rsquo;s some interesting concerns like robustness in light of failures combined with optimizing for performance (infrequent syncs to disk, maintaining the hot subset of data in memory, etc), combined with optimizing for various access patterns, this can be a daunting topic to get into.&lt;/p&gt;

&lt;p&gt;Luckily there&amp;rsquo;s a use case that I see all the time in my domain (telemetry systems) and that covers just enough of the problems to be interesting and fun, but not enough to be overwhelming.  And that is: for each step in a monitoring data pipeline, you want to be able to buffer data if the endpoint goes down, in memory and to disk if the amount of data gets too much. Especially to disk if you&amp;rsquo;re also concerned with your software crashing or the machine power cycling.&lt;/p&gt;

&lt;p&gt;This is such a common problem that applies to all metrics agents, relays, etc that I was longing for a library that just takes care of spooling data to disk for you without really affecting much of the rest of your software.  All it needs to do is sequentially write pieces of data to disk and have a sequential reader catching up and read newer data as it finishes processing the older.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://nsq.io/&#34;&gt;NSQ&lt;/a&gt; is a messaging platform from bitly, and it has &lt;a href=&#34;https://github.com/bitly/nsq/blob/master/nsqd/diskqueue.go&#34;&gt;diskqueue code&lt;/a&gt; that does exactly that. And it does so oh so elegantly.
I had previously found a beautiful pattern in bitly&amp;rsquo;s go code that I &lt;a href=&#34;http://dieter.plaetinck.be/post/beautiful_go_patterns_for_concurrent_access_to_shared_resources_and_coordinating_responses/&#34;&gt;blogged about&lt;/a&gt; and again I found a nice and elegant design that builds further on this pattern, with concurrent access to data protected via a single instance of a for loop running a select block which assures only one piece of code can make changes to data at the same time (see bottom of the file), not unlike ioloops in other languages.  And method calls such as &lt;a href=&#34;https://github.com/bitly/nsq/blob/fe4198b648499375651b7fece0b8489ea07d029f/nsqd/diskqueue.go#L120-L130&#34;&gt;Put()&lt;/a&gt; provide a clean external interface, though their implementation simply hooks into the internal select loop that runs the code that does the bulk of the work.  Genius.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (d *diskQueue) Put(data []byte) error {
  // some details
  d.writeChan &amp;lt;- data
  return &amp;lt;-d.writeResponseChan
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition the package came with &lt;a href=&#34;https://github.com/bitly/nsq/blob/master/nsqd/diskqueue_test.go&#34;&gt;extensive tests and benchmarks&lt;/a&gt; out of the box.&lt;/p&gt;

&lt;p&gt;After finding and familiarizing myself with this diskqueue code about a year ago I had an easy time introducing disk spooling to &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;Carbon-relay-ng&lt;/a&gt;, by transplanting the code into it. The only change I had to make was &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng/commit/4d6ebb37451ce6e05b606ea4ba6221611d367f71&#34;&gt;capitalizing the Diskqueue type to export it outside of the package&lt;/a&gt;.  It has proven a great fit, enabling a critical feature through little work of transplanting mature, battle-tested code into a context that original authors probably never thought of.&lt;/p&gt;

&lt;p&gt;Note also how the data unit here is the &lt;code&gt;[]byte&lt;/code&gt;, the queue does not deal with the higher level &lt;code&gt;nsq.Message&lt;/code&gt; (!).  The authors had the foresight of keeping this generic, enabling code reuse and rightfully shot down &lt;a href=&#34;https://github.com/bitly/nsq/pull/626&#34;&gt;a PR of mine&lt;/a&gt; that had a side effect of making the queue aware of the Message type.   In NSQ you&amp;rsquo;ll find thoughtful and deliberate api design and pretty sound code all around. Also, they went pretty far in &lt;a href=&#34;http://nsq.io/overview/internals.html&#34;&gt;detailing some lessons learned and providing concrete advice&lt;/a&gt;, a very interesting read, especially around managing goroutines &amp;amp; synchronizing their exits, and performance optimizations.  At Raintank, we had a need for a messaging solution for metrics so we will so be rolling out &lt;a href=&#34;https://github.com/raintank/raintank-metric/issues/11&#34;&gt;NSQ as part of the raintank stack&lt;/a&gt;.  This is an interesting case where my past experience with the NSQ code and ideas helped to adopt the full solution.&lt;/p&gt;

&lt;h1 id=&#34;bosun-expression-package&#34;&gt;Bosun expression package&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m a &lt;a href=&#34;http://dieter.plaetinck.be/post/practical-fault-detection-alerting-dont-need-to-be-data-scientist/&#34;&gt;fan of the bosun alerting system&lt;/a&gt; which came out of Stack Exchange.  It&amp;rsquo;s a full-featured alerting system that solves a few problems like no other tool I&amp;rsquo;ve seen does (see my linked post), and timeseries data storage aside, comes with basically everything built in to the one program.  I&amp;rsquo;ve used it with success. However, for &lt;a href=&#34;http://www.raintank.io/litmus/&#34;&gt;litmus&lt;/a&gt; I needed an alerting handler that integrated well into the Grafana backend.   I needed the ability to do arbitrarily complex computations. Graphite&amp;rsquo;s api only takes you so far. We also needed (desired) reduction functions, boolean logic, etc.  This is where &lt;a href=&#34;http://bosun.org/expressions.html&#34;&gt;bosun&amp;rsquo;s expression language&lt;/a&gt; is really strong.  I found the &lt;a href=&#34;https://github.com/bosun-monitor/bosun/tree/master/cmd/bosun/expr&#34;&gt;expression package&lt;/a&gt; quite interesting, they basically built their own DSL for metrics processing.  so it deals with expression parsing, constructing AST&amp;rsquo;s, executing them, dealing with types (potentially mixed types in the same expression), etc.&lt;/p&gt;

&lt;p&gt;But bosun also has incident management, contacts, escalations, etc.  Stuff that we either already had in place, or didn&amp;rsquo;t want to worry about just yet.  So we could run bosun standalone and talk to it as a service via its API which I found too loosely coupled and risky, hook all its code into our binary at once - which seemed overkill - or the strategy I chose: gradually familiarize ourself and adopt pieces of Bosun on a case by case basis, making sure there&amp;rsquo;s a tight fit and without ever building up so much technical debt that it would become a pain to move away from the transplanted code if it becomes clear it&amp;rsquo;s not/no longer well suited. For the foreseeable future we only need one piece, the expression package. Potentially ultimately we&amp;rsquo;ll adopt the entire thing, but without the upfront commitment and investment.&lt;/p&gt;

&lt;p&gt;So practically, our code now simply has &lt;a href=&#34;https://github.com/raintank/grafana/blob/9cfa14a2a6ea079b9dd5bc0164aced942190a33a/pkg/alerting/eval.go#L47&#34;&gt;one line&lt;/a&gt; where we create a bosun expression object from a string, and &lt;a href=&#34;https://github.com/raintank/grafana/blob/9cfa14a2a6ea079b9dd5bc0164aced942190a33a/pkg/alerting/eval.go#L77&#34;&gt;another&lt;/a&gt; where we ask bosun to execute the expression for us, which takes care of parsing the expression, querying for the data, evaluating and processing the results and distilling everything down into a final result.  We get all the language features (reduction functions, boolean logic, nested expressions, &amp;hellip;) for free.&lt;/p&gt;

&lt;p&gt;This transplantation was again probably not something the bosun authors expected, but for us it was tremendously liberating.  We got a lot of power for free.  The only thing I had to do was spend some time reading code, and learning in the process.  And I knew the code was well tested so we had zero issues using it.&lt;/p&gt;

&lt;p&gt;Much akin to the NSQ example above, there was another reason the transplantation went so smoothly: the expression package is not tangled into other stuff.  It just needs  a string expression and a graphite instance.  To be precise, any struct instance that satisfies the &lt;a href=&#34;https://github.com/bosun-monitor/bosun/blob/master/graphite/graphite.go#L124&#34;&gt;graphiteContext interface&lt;/a&gt; that is handily defined in the bosun code. While the bosun design aims to make its various clients (graphite, opentsdb, &amp;hellip;) applicable for other projects, it also happens to let us do opposite: reuse some of its core code - the expression package - and pass in a custom graphite Context, such as &lt;a href=&#34;https://github.com/raintank/grafana/blob/12d42c9715bbbd62063df37e37e89bfc77f64626/pkg/graphite/graphite.go#L113-L129&#34;&gt;our implementation&lt;/a&gt; which has extensive instrumentation. This lets us use the bosun expression package as a &amp;ldquo;black box&amp;rdquo; and still inject our own custom logic into the part that queries data from graphite.  Of course, once we want to change the logic of anything else in the black box, we will need come up with something else, perhaps fork the package, but it doesn&amp;rsquo;t seem like we&amp;rsquo;ll need that any time soon.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;If you want to become a better programmer I highly recommend you go read some code.  There&amp;rsquo;s plenty of good code out there.  Pick something that deals with a topic that is of interest to you and looks mature.  You typically won&amp;rsquo;t know if code is good before you start reading but you&amp;rsquo;ll find out really fast, and you might be pleasantly surprised, as was I, several times.  You will learn a bunch, possibly pretty fast.  However, don&amp;rsquo;t go for the most advanced, complex code straight away.  Pick projects and topics that are out of your comfort zone and do things that are new to you, but nothing too crazy.  Once you truly grok those, proceed to other, possibly more advanced stuff.&lt;/p&gt;

&lt;p&gt;Often you&amp;rsquo;ll read reusable libraries that are built to be reused, or you might find ways to transplant smaller portions of code into your own projects.  Either way is a great way to tinker and learn, and solve real problems.  Just make sure the code actually fits in so you don&amp;rsquo;t end up with the software version of Frankenstein&amp;rsquo;s monster.  It is also helpful to have the authors available to chat if you need help or have issues understanding something, though they might be surprised if you&amp;rsquo;re using their code in a way they didn&amp;rsquo;t envision and might not be very inclined to provide support to what they consider internal implementation details.  So that could be a hit or miss.  Luckily the people behind both nsq and bosun were supportive of my endeavors but I also made sure to try to figure out things by myself before bothering them.  Another reason why it&amp;rsquo;s good to pick mature, documented projects.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.raintank.io/content/images/2015/09/gopher_frank_monst-1.jpg&#34; alt=&#34;Gopher frankenstein&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Part of the original meaning of hacking, extended into open source, is a mindset and practice of seeing how others solve a problem, discussion and building on top of it.  We&amp;rsquo;ve gotten used to - and fairly good at - doing this on a project and library level but forgot about it on the level of code, &lt;a href=&#34;nil&#34; title=&#34;We do share and discuss code patterns but typically via blog posts which use contrived/theoretical examples.  Not in their natural setting of the real code where they are used.&#34;&gt;code patterns&lt;/a&gt; and ideas.  I want to see these practices come back to life.&lt;/p&gt;

&lt;p&gt;We also apply this at &lt;a href=&#34;http://raintank.io&#34;&gt;Raintank&lt;/a&gt;: not only are we trying to build the best open source monitoring platform by reusing (and often contributing to) existing open source tools and working with different communities, we realize it&amp;rsquo;s vital to work on a more granular level, get to know the people and practice cross-pollination of ideas and code.&lt;/p&gt;

&lt;p&gt;Next stuff I want to read and possibly implement or transplant parts of: &lt;a href=&#34;https://github.com/dgryski/go-trigram&#34;&gt;dgryski/go-trigram&lt;/a&gt;, &lt;a href=&#34;github.com/armon/go-radix&#34;&gt;armon/go-radix&lt;/a&gt;, especially as used in the &lt;a href=&#34;https://github.com/dgryski/carbonmem&#34;&gt;dgryski/carbonmem&lt;/a&gt; server to search through Graphite metrics.  Other fun stuff by dgryski: an implementation of the &lt;a href=&#34;https://github.com/dgryski/go-arc/&#34;&gt;ARC caching algorithm&lt;/a&gt; and &lt;a href=&#34;https://github.com/dgryski/go-bloomf&#34;&gt;bloom filters&lt;/a&gt;. (you might want to get used to reading Wikipedia pages also). And &lt;a href=&#34;https://github.com/mreiferson/wal&#34;&gt;mreiferson/wal&lt;/a&gt;, a write ahead log by one of the nsqd authors, which looks like it&amp;rsquo;ll become the successor of the beloved diskqueue code.&lt;/p&gt;

&lt;p&gt;Go forth and transplant!&lt;/p&gt;

&lt;p&gt;Also posted on the &lt;a href=&#34;https://blog.raintank.io/transplanting-go-packages-for-fun-and-profit/&#34;&gt;Raintank blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Focusing on open source monitoring.  Joining raintank.</title>
      <link>http://dieter.plaetinck.be/post/focusing-on-open-source-monitoring-joining-raintank/</link>
      <pubDate>Fri, 03 Jul 2015 09:22:02 -0700</pubDate>
      
      <guid>http://dieter.plaetinck.be/post/focusing-on-open-source-monitoring-joining-raintank/</guid>
      <description>&lt;h2&gt;Goodbye Vimeo&lt;/h2&gt;
&lt;p&gt;
It&#39;s never been as hard saying goodbye to the people and the work environment as it is now.
&lt;br/&gt;
Vimeo was created by dedicated film creators and enthusiasts, just over 10 years ago, and today it still shows.
From the quirky, playful office culture, &lt;a href=&#34;https://vimeo.com/staff&#34;&gt;the staff created short films&lt;/a&gt;,
to the &lt;a href=&#34;https://vimeo.com/categories&#34;&gt;tremendous curation effort&lt;/a&gt; and &lt;a href=&#34;https://vimeo.com/channels/staffpicks/videos&#34;&gt;staff picks&lt;/a&gt; including &lt;a href=&#34;http://websta.me/p/797128421285501516_12986477&#34;&gt;monthly staff screenings&lt;/a&gt; where we get to see the best of the best videos on the Internet each month,
to the dedication towards building the best platform and community on the web to enjoy videos and the uncompromising commitment to supporting movie creators and working in their best interest.
&lt;br/&gt;Engineering wise, there has been plenty of opportunity to make an impact and learn.
&lt;br/&gt;Nonetheless, I have to leave and I&#39;ll explain why.  First I want to mention a few more things.
&lt;/p&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/bye_vimeo.jpg&#34; alt=&#34;vimeo goodbye drink&#34; /&gt;
&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;
In Belgium I used to hitchhike to and from work so that each day brought me opportunities to have conversations with a diverse, fantastic assortment of people.  I still fondly remember some of those memories. (and it was also usually faster than taking the bus!)
&lt;br/&gt;Here in NYC this isn&#39;t really feasible, so I tried the next best thing.  A mission to have lunch with every single person in the company, starting with those I don&#39;t typically interact with.  I managed to have lunch with 95 people, get to know them a bit, find some gems of personalities and anecdotes, and have conversations on a tremendous variety of subjects, some light-hearted, some deep and profound.  It was fun and I hope to be able to keep doing such social experiments in my new environment.

&lt;/p&gt;
&lt;p&gt;
	Vimeo is also part of my life in an unusually personal way.  When I came to New York (my first ever visit to the US) in 2011 to interview, I also met a pretty fantastic woman in a random bar in Williamsburg. We ended up traveling together in Europe, I decided to move the US and we moved in together.  I&#39;ve had the pleasure of being submerged in both American and Greek culture for the last few years, but the best part is that today we are engaged and I feel like the luckiest guy in the world.  While I&#39;ve tried to keep work and personal life somewhat separate, Vimeo has made an undeniable ever lasting impact on my life that I&#39;m very grateful for.
&lt;/p&gt;
&lt;p&gt;
At Vimeo I found an area where a bunch of my interests converge: operational best practices, high performance systems, number crunching, statistics and open source software.  Specifically, timeseries metrics processing in the context of monitoring.  While I have enjoyed my opportunity &lt;a href=&#34;http://dieter.plaetinck.be/tags/monitoring/&#34;&gt;to make contributions in this space&lt;/a&gt; to help our teams and other companies who end up using my tools, I want to move out of the cost center of the company, I want to be in the department that creates the value.  If I want to focus on open source monitoring, I should align my incentives with those of my employer.  Both for my and their sake.
&lt;b&gt;I want to make more profound contributions to the space.  The time has come for me to join a company for which the main focus is making open source monitoring better.&lt;/b&gt;
&lt;/p&gt;
&lt;h2&gt;Hello raintank!&lt;/h2&gt;
Over the past two years or so I&#39;ve talked to many people in the industry about monitoring, many of them trying to bring me into their team.
I never found a perfect fit but as we transitioned from 2014 into 2015, the stars seemingly aligned for me.
Here&#39;s why I&#39;m very excited to join the &lt;a href=&#34;http://www.raintank.io/&#34;&gt;raintank&lt;/a&gt; crew:
&lt;ul&gt;
&lt;li&gt;I&#39;m a strong believer in open source.  I believe fundamental infrastructure tooling should be modifiable and in your control.  It&#39;s partially a philosophical argument, but also what I believe will separate long lasting business from short term successes.
&lt;br/&gt;SaaS is great, but not if it&#39;s built to lock you in.  In this day and age, I think you should &#34;lock in&#34; your customers by providing a great experience they can&#39;t say no to, not by having them build technical debt as they integrate into your stack.
&lt;br/&gt;That said, integrating with open source also incurs technical debt, and some closed source service providers are so good that the debt is worth it.  But I don&#39;t believe this lasts long term, especially given the required pricing models.  I think you can entice customers more by lowering the debt they build up (i.e. use standardized protocols, tooling and making it easy for them to leave) as they adopt your service.
&lt;!-- I was really interested in datadog for example.  I liked the team, I liked the product they are/were building and I saw they would become successfull, but it&#39;s not the kind of business I want to build towards long term. --&gt;
&lt;br/&gt;raintank&#39;s commitment to open source is not a gimmick but stems from a fundamental conviction that 100% open source is the right thing to do.  That by providing freedom, you get loyalty.
I think they came up with a good business model that combines the benefits of SaaS with those of open source.  At least it&#39;s a bet I want to take.
It&#39;s not an easy feat but I think we got a good formula. (basically openSAAS, the stack is open source, integrates in the wider ecosystem, you can run it yourself, you own your data, you can use our SAAS, or create a mixed setup.)&lt;/li&gt;
&lt;br&gt;
&lt;iframe src=&#34;https://player.vimeo.com/video/131790481&#34; width=&#34;500&#34; height=&#34;281&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;li&gt;Something I value a lot in coworkers, besides &#34;technical&#34; abilities is emotional intelligence and professional maturity.  Part of me thinks it&#39;s a vastly underestimated quality in general, although perhaps I just happen to find it more important than average.  Either way, the new team seems solid on both fronts.  I&#39;m mostly impressed by the founder/CEO Raj Dutt who has shown a very personal and graceful approach.
	Strong high-stakes relationships with people in general, and coworkers in particular are a very worthy pursuit and two months in I can still vouch for the level of alignment and maturity that I haven&#39;t experienced before.
	&lt;br&gt;
	&lt;br&gt;
	&lt;img src=&#34;http://dieter.plaetinck.be/files/20150429_194521_HDR.jpg&#34; alt=&#34;Yours truly and Raj Dutt&#34; /&gt;
	&lt;br&gt;
	&lt;br&gt;
	&lt;li&gt;I (and my fiancee) want to see the world. The common conception that significant leisure travel can&#39;t be combined with hard, or even normal amounts of work seems so backwards to me.  Today more than ever, we have globalization, we are developing a sharing economy (airbnb, lyft, ...).  There&#39;s no reason we can&#39;t do great work while enjoying our personal time to the fullest.  Working remotely, and/or with remote colleagues requires more discipline but I simply only want to work with disciplined people.  Working on a fixed schedule, in a fixed office location is needlessly arcane, constrained and inefficient. It puts a damper on life.
		Almost all software companies I know see open office plans as a great thing, but it&#39;s usually an attempt at compensating for employee&#39;s poor communication skills, forcing them
		to talk to the detriment of people who need focus and get distracted.  Isolation of workers does not preclude healthy communication and collaboration.
		I get a lot more done in isolation, with sync-ups and face-to-face when appropriate, especially if it&#39;s on my schedule. Working remote is a great way to facilitate this and I&#39;m happy that raintank not only agrees with the idea, but actually encourages travel, and encourages me to follow whatever time allocation works best for me.
		I work hard but we travel where ever we want, we&#39;ll have plenty of time to spend with our families in Belgium and Cyprus.  And yet I don&#39;t think I ever worked this closely with anyone.&lt;/li&gt;
	&lt;br&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/office_shasta.jpg&#34; alt=&#34;office mount shasta&#34; /&gt;
	&lt;br&gt;
	&lt;br&gt;
&lt;li&gt;I&#39;ve always wanted to be in a company from the start and experience the &#34;true&#34; start-up feel.  I have a lot of opinions on organization, culture, and product and so I&#39;m glad to have the opportunity to make that kind of impact as well.
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;OK, so what am I really up to?&lt;/h2&gt;
&lt;p&gt;
	Grafana is &lt;a href=&#34;http://play.grafana.org/dashboard/db/stats&#34;&gt;pretty much the leading open source metrics dashboard&lt;/a&gt; right now.
So it only makes sense that raintank is a heavy Grafana user and contributor.
My work, logically, revolves around codifying some of the experience and ideas I have, and making
them accessible through the polished interface that is Grafana, which now also has a full time UX designer working on it.
Since according to the &lt;a href=&#34;https://infogr.am/grafana_user_survey_mar2015&#34;&gt;Grafana user survey&lt;/a&gt; &lt;b&gt;alerting is the most sorely missed non-feature of Grafana&lt;/b&gt;,
we are working hard on rectifying this and it is my full-time focus.  If you&#39;ve followed my blog you know I have some thoughts on where the sweet spot lies in clever alerting.
In short, take the claims of anomaly detection via machine learning with a big grain of salt, and instead, focus on enabling operators to express complex logic simply, quickly,
and in an agile way.  My latest favorite project, &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun&lt;/a&gt; exemplifies this approach (highly recommend giving this a close look).
&lt;/p&gt;
&lt;p&gt;
The way I&#39;m thinking of it now, the priorities (and sequence of focus) for alerting within Grafana will probably be something like this:
&lt;ul&gt;
	&lt;li&gt;cover the low hanging fruit: simple threshold checks with email notifications actually go a long way&lt;/li&gt;
	&lt;li&gt;gradually provide more power and sophistication (reduction functions, boolean logic, etc), while keeping things in a nice UI&lt;/li&gt;
	&lt;li&gt;provide full-on integration with advanced alerting systems such as Bosun. Iterative workflow, Signal/noise analysis, etc&lt;/li&gt;
&lt;/ul&gt;

There&#39;s a lot of thought work, UX and implementation details around this topic,
I&#39;ve created a &lt;a href=&#34;https://github.com/grafana/grafana/issues/2209&#34;&gt;github ticket&lt;/a&gt; to kick off a discussion and am curious to hear your thoughts.

Finally, if any of this sounds interesting to you, you can sign up to the &lt;a href=&#34;http://grafana.org/&#34;&gt;grafana newsletter&lt;/a&gt; or the &lt;a href=&#34;http://raintank.io/&#34;&gt;raintank newsletter&lt;/a&gt; which will get you info on the open source platform as well
as the SaaS product.  Both are fairly low volume.

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/office_sausolito.jpg&#34; alt=&#34;office sausolito&#34; /&gt;
&lt;i&gt;It may look like I&#39;m not doing much from my temporary Mill Valley office, but trust me, cool stuff is coming!&lt;/i&gt;



&lt;!--
some reminisic about meeting up with torkel and talking buziness models in pdx
 how i see myself integrating in the team:
the way I see it, there&#39;s some good, next-gen ideas and implementation in some of the things i&#39;ve built and/or have experience with (metrics2.0, graph-explorer, bosun) but clearly some of these projects (esp the former two) have been handicapped by a steep learning curve and weak UX.  Part of the reason I&#39;m excited in joining in RT is that over time I will be able to gently infuse some of these re-envisioned ideas into the grafana package, with a much higher focus on user friendlyness.
nyc skin issues
alternative funding? crowd etc


https://vimeo.com/blog/post:702
grown as backend engineer, carbon-relay-ng most fun
my various projects
contributor to graphite, influxdb, bosun, diamond, statsd, graphite-api,
https://github.com/brutasse/graphite-api
https://github.com/Dieterbe/timeserieswidget
https://github.com/Dieterbe/profile-process
http://vimeo.github.io/graph-explorer/
https://github.com/vimeo/graphite-influxdb
https://github.com/vimeo/carbon-tagger
https://github.com/vimeo/statsdaemon
https://github.com/vimeo/graphite-api-influxdb-docker
https://github.com/vimeo/whisper-to-influxdb
https://github.com/vimeo/smoketcp
https://github.com/vimeo/timeserieswidget
https://github.com/vimeo/simple-black-box
https://groups.google.com/forum/#!forum/it-telemetry

https://github.com/python-diamond
https://github.com/bosun-monitor

https://github.com/Dieterbe/anthracite
https://github.com/Dieterbe/influx-cli

https://github.com/graphite-ng/graphite-ng
https://github.com/graphite-ng/carbon-relay-ng
http://metrics20.org/
humbled by big players who saw my work and invited me to work with them
basically 2 big camps.


it seems like every week a monitoring startup launches somewhere.  There is, and will be, more and more competition.  It&#39;s almost ludicrous to start or join a new one.
--&gt;
</description>
    </item>
    
    <item>
      <title>Moved blog to hugo, fastly and comma</title>
      <link>http://dieter.plaetinck.be/post/moved-blog-to-hugo-fastly-comma/</link>
      <pubDate>Thu, 02 Jul 2015 16:35:02 -0700</pubDate>
      
      <guid>http://dieter.plaetinck.be/post/moved-blog-to-hugo-fastly-comma/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I noticed what a disservice I was doing my readers when I started monitoring my site using &lt;a href=&#34;http://www.raintank.io/litmus/&#34;&gt;litmus&lt;/a&gt;.
A dynamic website in python on a cheap linode&amp;hellip; What do you expect?  So I now serve through &lt;a href=&#34;https://www.fastly.com/&#34;&gt;fastly&lt;/a&gt; and use a static site generator.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://pyblosxom.github.io/&#34;&gt;pyblosxom&lt;/a&gt; was decent while it lasted.
It can generate sites statically, but the project never got a lot of traction and is slowly fading out.  There were a bit too many moving parts, so &amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I now use the &lt;a href=&#34;http://gohugo.io/&#34;&gt;hugo&lt;/a&gt; static site generator, which is powerful, quite complete and gaining momentum.
Fast and simple to use.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Should also keep an eye on the &lt;a href=&#34;https://caddyserver.com/&#34;&gt;caddy&lt;/a&gt; webserver since it has some nice things such as &lt;a href=&#34;https://caddyserver.com/docs/git&#34;&gt;git integration&lt;/a&gt; which should work well with hugo.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Trying to get disqus going was frustrating.
Self hosted options like &lt;a href=&#34;https://github.com/talkatv/talkatv&#34;&gt;talkatv&lt;/a&gt; and &lt;a href=&#34;https://github.com/posativ/isso&#34;&gt;isso&lt;/a&gt; were too complex, and &lt;a href=&#34;https://github.com/spf13/kaiju&#34;&gt;kaiju&lt;/a&gt; is just not there yet and also pretty complex.
I wrote &lt;a href=&#34;https://github.com/Dieterbe/comma&#34;&gt;comma&lt;/a&gt; which is a simple comment server in Go.
Everything I need in 100 lines of Go and 50 lines of javascript! Let me know if you see anything funky.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Dieterbe/dieterblog/blob/master/pyblosxom-to-hugo.py&#34;&gt;pyblosxom-to-hugo.py&lt;/a&gt; migrated all content.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Talks</title>
      <link>http://dieter.plaetinck.be/talks/</link>
      <pubDate>Sun, 03 May 2015 23:01:07 -0400</pubDate>
      
      <guid>11 at http://dieter.plaetinck.be</guid>
      <description>&lt;h2&gt;Talks&lt;/h2&gt;

&lt;table&gt;

&lt;tbody&gt;
&lt;tr&gt;

    &lt;td&gt;July 12, 2016&lt;/td&gt;
    &lt;td&gt;Next-generation alerting and fault detection&lt;/td&gt;
    &lt;td&gt;SRECon Europe&lt;/td&gt;
    &lt;td&gt;Dublin&lt;/td&gt;
    &lt;td&gt;
				&lt;a href=&#34;https://www.usenix.org/conference/srecon16europe/program/presentation/plaetinck&#34;&gt;details and video&lt;/a&gt;
				&lt;a href=&#34;http://www.slideshare.net/Dieterbe/next-generation-alerting-and-fault-detection-srecon-europe-2016&#34;&gt;slides&lt;/a&gt;
    &lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;June 6, 2016&lt;/td&gt;
    &lt;td&gt;Next-generation alerting and fault detection&lt;/td&gt;
    &lt;td&gt;Velocity&lt;/td&gt;
    &lt;td&gt;Santa Clara&lt;/td&gt;
    &lt;td&gt;
				&lt;a href=&#34;http://conferences.oreilly.com/velocity/vl-ca-2016/public/schedule/detail/49335&#34;&gt;details&lt;/a&gt;
				See talk above for updated version
    &lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Oct 15, 2015&lt;/td&gt;
    &lt;td&gt;Alerting in Grafana&lt;/td&gt;
    &lt;td&gt;GrafanaCon 2015&lt;/td&gt;
		&lt;td&gt;NYC&lt;/td&gt;
    &lt;td&gt;
				&lt;a href=&#34;https://www.youtube.com/watch?v=C_H2ew8e5OM&#34;&gt;video (with embedded slides)&lt;/a&gt;
				&lt;a href=&#34;http://www.slideshare.net/Dieterbe/alerting-in-grafana-grafanacon-2015&#34;&gt;slides&lt;/a&gt;
    &lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Nov 12, 2014&lt;/td&gt;

    &lt;td&gt;Rethinking Metrics: Metrics 2.0&lt;/td&gt;

    &lt;td&gt;LISA 2014&lt;/td&gt;

    &lt;td&gt;Seattle, Washington&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/plaetinck&#34;&gt;slides, audio, video&lt;/a&gt;

    &lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Nov 04, 2014&lt;/td&gt;

    &lt;td&gt;Rethinking Metrics: Metrics 2.0&lt;/td&gt;

    &lt;td&gt;Monitoring NYC&lt;/td&gt;

    &lt;td&gt;NYC&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;http://www.meetup.com/Monitoring-NYC/events/214589632/&#34;&gt;details&lt;/a&gt;
        &lt;a href=&#34;http://www.slideshare.net/Dieterbe/rethinking-metrics-metrics-20&#34;&gt;slides&lt;/a&gt;

    &lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;May 5, 2014&lt;/td&gt;

    &lt;td&gt;Metrics 2.0&lt;/td&gt;

    &lt;td&gt;Monitorama&lt;/td&gt;

    &lt;td&gt;Portland, Oregon&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;http://metrics20.org/media/&#34;&gt;slides, video&lt;/a&gt;,

        &lt;a href=&#34;http://dietertest.plaetinck.be/monitorama-pdx-metrics20.html&#34;&gt;blogpost&lt;/a&gt;

    &lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Apr 3, 2014&lt;/td&gt;

    &lt;td&gt;Metrics stack 2.0&lt;/td&gt;

    &lt;td&gt;nycdevops meetup&lt;/td&gt;

    &lt;td&gt;NYC&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://metrics20.org/media/&#34;&gt;slides&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Feb 18, 2014&lt;/td&gt;

    &lt;td&gt;Metrics 2.0 &amp; Graph-Explorer&lt;/td&gt;

    &lt;td&gt;FullStack engineering meetup&lt;/td&gt;

    &lt;td&gt;NYC&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://metrics20.org/media/&#34;&gt;slides, video&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Jan 18, 2013&lt;/td&gt;

    &lt;td&gt;Simple Black Box&lt;/td&gt;

    &lt;td&gt;Devopsdays&lt;/td&gt;

    &lt;td&gt;NYC&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;http://devopsdays.org/events/2012-newyork/proposals/SimpleBlackBox/&#34;&gt;devopsdays page&lt;/a&gt;,

        &lt;a href=&#34;http://dieter.plaetinck.be/profiling_and_behavior_testing_processes_daemons_devopsdays_nyc.html&#34;&gt;blog&lt;/a&gt;,

        &lt;a href=&#34;https://twitter.com/Dieter_be/status/293377294679027713&#34;&gt;slides&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Feb 6, 2011&lt;/td&gt;

    &lt;td&gt;Can we build a simple, cross-distribution installation framework?&lt;/td&gt;

    &lt;td&gt;Fosdem&lt;/td&gt;

    &lt;td&gt;Brussels, Belgium&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;https://archive.fosdem.org/2011/schedule/event/distro_crossinstall&#34;&gt;fosdem page&lt;/a&gt;,

        &lt;a href=&#34;http://dieter.plaetinck.be/can_we_build_a_simple_cross-distribution_installation_framework.html&#34;&gt;blog, slides, video&lt;/a&gt;

    &lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Jul 23, 2010&lt;/td&gt;

    &lt;td&gt;Uzbl - web interface tools which adhere to the unix philosophy&lt;/td&gt;

    &lt;td&gt;Archcon&lt;/td&gt;

    &lt;td&gt;Toronto, Canada&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://dieter.plaetinck.be/back_from_canada_archcon.html&#34;&gt;archcon blogpost, slides, video&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Jul 22, 2010&lt;/td&gt;

    &lt;td&gt;AIF: The Arch Installation Framework&lt;/td&gt;

    &lt;td&gt;Archcon&lt;/td&gt;

    &lt;td&gt;Toronto, Canada&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://dieter.plaetinck.be/back_from_canada_archcon.html&#34;&gt;archcon blogpost, slides, video&lt;/a&gt;&lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;Feb 6, 2010&lt;/td&gt;

    &lt;td&gt;Uzbl lightning talk&lt;/td&gt;

    &lt;td&gt;Fosdem&lt;/td&gt;

    &lt;td&gt;Brussels, Belgium&lt;/td&gt;

    &lt;td&gt;

        &lt;a href=&#34;https://archive.fosdem.org/2010/schedule/events/uzbl&#34;&gt;fosdem page&lt;/a&gt;,

        &lt;a href=&#34;http://dieter.plaetinck.be/uzbl_monitoring_aif_talks.html&#34;&gt;blog post&lt;/a&gt;

    &lt;/td&gt;

&lt;/tr&gt;

&lt;tr&gt;

    &lt;td&gt;???, 2009&lt;/td&gt;

    &lt;td&gt;Overview of monitoring software&lt;/td&gt;

    &lt;td&gt;Kangaroot showcase&lt;/td&gt;

    &lt;td&gt;Vilvoorde, Belgium&lt;/td&gt;

    &lt;td&gt;&lt;a href=&#34;http://dieter.plaetinck.be/uzbl_monitoring_aif_talks.html&#34;&gt;blog post&lt;/a&gt;. video/slides unavailable&lt;/td&gt;

&lt;/tr&gt;

&lt;/tbody&gt;

&lt;/table&gt;



&lt;h2&gt;Other fun public performances&lt;/h2&gt;

&lt;ul&gt;

&lt;li&gt;&lt;a href=&#34;http://dieter.plaetinck.be/vimeo_holiday_special_and_other_great_videos.html&#34;&gt;Vimeo holiday special 2013&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://vimeo.com/59629623&#34;&gt;trailer&lt;/a&gt; and &lt;a href=&#34;https://vimeo.com/59740798&#34;&gt;full version&lt;/a&gt; of a live metal show with me on drums.  Summer of 2012&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;https://vimeo.com/89865779&#34;&gt;another metal music video&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&#34;http://www.osnews.com/story/22692/Arch_Linux_Team&#34;&gt;Arch Linux team interview&lt;/a&gt;. Jan 11, 2010&lt;/li&gt;

&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Practical fault detection on timeseries part 2: first macros and templates</title>
      <link>http://dieter.plaetinck.be/post/practical-fault-detection-on-timeseries-part-2/</link>
      <pubDate>Mon, 27 Apr 2015 09:05:02 -0400</pubDate>
      
      <guid>practical-fault-detection-on-timeseries-part-2</guid>
      <description>In the &lt;a href=&#34;http://dieter.plaetinck.be/practical-fault-detection-alerting-dont-need-to-be-data-scientist.html&#34;&gt;previous fault detection article&lt;/a&gt;, we saw how we can cover a lot of ground in fault detection with simple methods and technology that is available today.
It had an example of a simple but effective approach to find sudden spikes (peaks and drops) within fluctuating time series.
This post explains the continuation of that work and provides you the means to implement this yourself with minimal effort.
I&#39;m sharing with you:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://bosun.org&#34;&gt;Bosun&lt;/a&gt; macros which detect our most common not-trivially-detectable symptoms of problems&lt;/li&gt;
&lt;li&gt;Bosun notification template which provides a decent amount of information&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.grafana.org&#34;&gt;Grafana&lt;/a&gt; and &lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer&lt;/a&gt; dashboards and integration for further troubleshooting&lt;/li&gt;
&lt;/ul&gt;
We reuse this stuff for a variety of cases where the data behaves similarly and I suspect that you will be able to apply this to a bunch of your monitoring targets as well.
&lt;!--more--&gt;
&lt;h2&gt;Target use case&lt;/h2&gt;
As in the previous article, we focus on the specific category of timeseries metrics driven by user activity.
Those series are expected to fluctuate in at least some kind of (usually daily) pattern, but is expected to have a certain smoothness to it. Think web requests per second or uploads per minute.   There are a few characteristics that are considered faulty or at least worth our attention:
&lt;br/&gt;
&lt;table&gt;
&lt;tr&gt;
    &lt;td style=&#34;padding:0px;&#34; width=&#34;160px;&#34;&gt;&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-good.png&#34;&gt;&lt;img width=&#34;160px;&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-good.png&#34;/&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&#34;padding:0px;&#34; width=&#34;160px;&#34;&gt;&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-spikes.png&#34;&gt;&lt;img width=&#34;160px;&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-spikes.png&#34;/&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&#34;padding:0px;&#34; width=&#34;160px;&#34;&gt;&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-erratic.png&#34;&gt;&lt;img width=&#34;160px;&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-erratic.png&#34;/&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&#34;padding:0px;&#34; width=&#34;160px;&#34;&gt;&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-timeseries-median-drop.png&#34;&gt;&lt;img width=&#34;160px;&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeseries-bad-timeseries-median-drop.png&#34;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;&lt;b style=&#34;color: green;&#34;&gt;looks good&lt;/b&gt;&lt;br/&gt;consistent pattern&lt;br/&gt;consistent smoothness&lt;/td&gt;
  &lt;td&gt;&lt;b style=&#34;color: red;&#34;&gt;sudden deviation (spike)&lt;/b&gt;&lt;br/&gt;Almost always something broke or choked.&lt;br/&gt;could also be pointing up. ~ peaks and valleys&lt;/td&gt;
  &lt;td&gt;&lt;b style=&#34;color: red;&#34;&gt;increased erraticness&lt;/b&gt;&lt;br/&gt;Sometimes natural&lt;br/&gt;often result of performance issues&lt;/td&gt;
  &lt;td&gt;&lt;b style=&#34;color: red;&#34;&gt;lower values than usual&lt;/b&gt; (in the third cycle)&lt;br/&gt;Often caused by changes in code or config, sometimes innocent.  But best to alert operator in any case [*]&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;br/&gt;
[*] Note that some regular patterns can look like this as well. For example weekend traffic lower than weekdays, etc.  We see this a lot.
&lt;br/&gt;The illustrations don&#39;t portray this for simplicity.   But the alerting logic below supports this just fine by comparing to same day last week instead of yesterday, etc.


&lt;h2&gt;Introducing the new approach&lt;/h2&gt;

The &lt;a href=&#34;http://dieter.plaetinck.be/practical-fault-detection-alerting-dont-need-to-be-data-scientist.html&#34;&gt;previous article&lt;/a&gt; demonstrated using graphite to compute standard deviation.
This let us alert on the erraticness of the series in general and as a particularly interesting side-effect, on spikes up and down.
The new approach is more refined and concrete by leveraging some of bosun&#39;s and Grafana&#39;s strengths.  We can&#39;t always detect the last case above via erraticness checking (a lower amount may be introduced gradually, not via a sudden drop) so now we monitor for that as well, covering all cases above.

We use 
&lt;ul&gt;
&lt;li&gt;Bosun macros which encapsulate all the querying and processing&lt;/li&gt;
&lt;li&gt;Bosun template for notifications&lt;/li&gt;
&lt;li&gt;A generic Grafana dashboard which aids in troubleshooting&lt;/li&gt;
&lt;/ul&gt;
We can then leverage this for various use cases, as long as the expectations of the data are as outlined above.
We use this for web traffic, volume of log messages, uploads, telemetry traffic, etc.
For each case we simply define the graphite queries and some parameters and leverage the existing mentioned Bosun and Grafana configuration.
&lt;br/&gt;
&lt;p&gt;
The best way to introduce this is probably by showing how a notification looks like:
&lt;br/&gt;
&lt;center&gt;
&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-dm-notification.png&#34;&gt;&lt;img height=&#34;600px&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-dm-notification.png&#34;/&gt;&lt;/a&gt;
&lt;br/&gt;
(image redacted to hide confidential information
&lt;br/&gt;the numbers are not accurate and for demonstration purposes only)
&lt;/center&gt;
&lt;p&gt;
As you can tell by the sections, we look at some global data (for example &#34;all web traffic&#34;, &#34;all log messages&#34;, etc), and also
by data segregated by a particular dimension (for example web traffic by country, log messages by key, etc)
&lt;br/&gt;
To cover all problematic cases outlined above, we do 3 different checks:
(note, everything is parametrized so you can tune it, see further down)
&lt;ul&gt;
&lt;li&gt;Global volume: comparing the median value of the last 60 minutes or so against the corresponding 60 minutes last week and expressing it as a &#34;strength ratio&#34;.  Anything below a given threshold such as 0.8 is alerted on&lt;/li&gt;
&lt;li&gt;Global erraticness. To find all forms of erraticness (increased deviation), we use a refined formula.  See details below.  A graph of the input data is included so you can visually verify the series&lt;/li&gt;
&lt;li&gt;On the segregated data: compare current (hour or so) median against median derived from the corresponding hours during the past few weeks, and only allow a certain amount of standard deviations difference&lt;/li&gt;
&lt;/ul&gt;

If any, or multiple of these conditions are in warning or critical state, we get 1 alert that gives us all the information we need.
&lt;br/&gt;
Note the various links to GE (Graph-Explorer) and Grafana for timeshifts.
The Graph-Explorer links are just standard GEQL queries, I usually use this if i want to be easily manage what I&#39;m viewing (compare against other countries, adjust time interval, etc) because that&#39;s what GE is really good at.
The timeshift view is a Grafana dashboard that takes in a Graphite expression as a template variable, and can hence be set via a GET parameter by using the url  &lt;pre&gt;http://grafana/#/dashboard/db/templatetimeshift?var-patt=expression&lt;/pre&gt;
It shows the current past week as red dots, and the past weeks before that as timeshifts in various shades of blue representing the age of the data. (darker is older).
&lt;br/&gt;
&lt;br/&gt;

&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-screenshot-template-timeshift.png&#34;&gt;&lt;img src=&#34;http://dieter.plaetinck.be/files/practical-alerting-screenshot-template-timeshift.png&#34; /&gt;&lt;/a&gt;
&lt;br/&gt;
This allows us to easily spot when traffic becomes too low, overly erratic, etc as this example shows:
&lt;br/&gt;
&lt;br/&gt;

&lt;a href=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeshift-use.png&#34;&gt;&lt;img src=&#34;http://dieter.plaetinck.be/files/practical-alerting-timeshift-use.png&#34; /&gt;&lt;/a&gt;
&lt;br/&gt;

&lt;h2&gt;Getting started&lt;/h2&gt;

Note: I Won&#39;t explain the details of the bosun configuration.  Familiarity with bosun is assumed.  The &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun documentation&lt;/a&gt; is pretty complete.
&lt;br/&gt;
&lt;br/&gt;
&lt;a href=&#34;https://gist.github.com/Dieterbe/d1892fa0b4454b892216&#34;&gt;Gist with bosun macro, template, example use, and Grafana dashboard definition&lt;/a&gt;.  Load the bosun stuff in your bosun.conf and import the dashboard in Grafana.
&lt;br/&gt;
&lt;br/&gt;
The pieces fit together like so:

&lt;ul&gt;
&lt;li&gt;The alert is where we define the graphite queries, the name of the dimension segregated by (used in template), how long the periods are, what the various thresholds are and the expressions to be fed into Grafana and Graph-Explorer.
&lt;br/&gt;
It also lets you set an importance which controls the sorting of the segregated entries in the notification (see screenshot).  By default it is based on the historical median of the values but you could override this.  For example for a particular alert we maintain a lookup table with custom importance values.&lt;/li&gt;
&lt;li&gt;The macros are split in two:
&lt;ol&gt;
&lt;li&gt;dm-load loads all the initial data based on your queries and computes a bunch of the numbers.&lt;/li&gt;
&lt;li&gt;dm-logic does some final computations and evaluates the warning and critical state expressions.&lt;/li&gt;
&lt;/ol&gt;
They are split so that your alerting rule can leverage the returned tags from the queries in dm-load to use a lookup table to set the importance variable or other thresholds, such as s_min_med_diff on a case-by-case basis, before calling dm-logic.
&lt;br/&gt;
We warn if one or more segregated items didn&#39;t meet their median requirements, and if erraticness exceeds its threshold (note that the latter can be disabled).
&lt;br&gt;Critical is when more than the specified number of segregated items didn&#39;t meet their median requirements, the global volume didn&#39;t meet the strength ratio, or if erraticness is enabled and above the critical threshold.
&lt;/li&gt;
&lt;li&gt;The template is evaluated and generates the notification like shown above&lt;/li&gt;
&lt;li&gt;Links to Grafana (timeshift) and GE are generated in the notification to make it easy to start troubleshooting&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Erraticness formula refinements&lt;/h2&gt;
You may notice that the formula has changed to
&lt;pre&gt;
(deviation-now * median-historical) /
((deviation-historical * median-now) + 0.01)
&lt;/pre&gt;
&lt;img style=&#34;float: right; margin: 45px;&#34; width=&#34;40%&#34; src=&#34;http://dieter.plaetinck.be/files/practical-alerting-notes-cleaned-small.jpg&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;Current deviation is compared to an automatically chosen historical deviation value (so no more need to manually set this)&lt;/li&gt;
&lt;li&gt;Accounts for difference in volume: for example if traffic at any point is much higher, we can also expect the deviation to be higher.  With the previous formula we would have cases where in the past the numbers were very low, and naturally the deviation then was low and not a reasonable standard to be held against when traffic is higher, resulting in trigger happy alerting with false positives.
&lt;br/&gt;Now we give a fair weight to the deviation ratio by making it inversely proportional to the median ratio&lt;/li&gt;
&lt;li&gt;The + 0.01 is to avoid division by zero&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
streak covers cases where values are very low so that stdev is in the same order (like low volume logs) and we can&#39;t properly use the erraticness or x-deviations. for example logs with very little traffic, datapoints representing healthy traffic can look like (1, 2, 0, 3, 1, ..)
although i think the ratio of medians (median_now/median_then) should work just as well as streak

&lt;img src=&#34;http://dieter.plaetinck.be/files/practical-alerting-low-values-zeroes.png&#34; /&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/practical-alerting-low-values-low.png&#34; /&gt;
--&gt;

&lt;h2&gt;Still far from perfect&lt;/h2&gt;
While this has been very helpful to us, I want to highlight a few things that could be improved.
&lt;ul&gt;
&lt;li&gt;With these alerts, you&#39;ll find yourself wanting to iteratively fine tune the various parameters and validate the result of your changes by comparing the status-over-time timeline before and after the change.  While Bosun already makes iterative development easier and lets you &lt;a href=&#34;http://bosun.org/public/ss_rule_timeline.png&#34;&gt;run test rules against old data and look at a the status over time&lt;/a&gt;, the interface could be improved by
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bosun-monitor/bosun/issues/636&#34;&gt;showing timeseries (with event markers where relevant) alongside the status visualization&lt;/a&gt;, so you have context to interpret the status timeline&lt;/li&gt;
&lt;li&gt;routinely building &lt;a href=&#34;https://github.com/grafana/grafana/pull/1569&#34;&gt;a knowledge base of time ranges annotated with a given state for a given alerting concern, which would help in validating the generated status timeline, both visually and in code.  We could compute percentage of issues found, missed, etc&lt;/a&gt;. &#34;unit tests for alerting&#34; my boss called it.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Template could be prettier.  In particular the plots often don&#39;t render very well.  We&#39;re looking into closer Grafana-Bosun integration so I think that will be resolved at some point.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bosun-monitor/bosun/issues/719&#34;&gt;Current logic doesn&#39;t take past outages into account. &#34;just taking enough periods in graphiteBand()&#34; helps alleviate it mostly, but it&#39;s not very robust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See that drop in the screenshot a bit higher up? That one was preceded by a code deploy event in anthracite which made some changes where a drop in traffic was actually expected.  Would love to be able to mark stuff like this in deploys (like putting in the commit message something like &#34;expect 20-50 drop&#34; and have the monitoring system leverage that.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;In conclusion&lt;/h2&gt;
I know many people are struggling with poor alerting rules (static thresholds?)
&lt;br/&gt;As I explained in the previous article I fondly believe that the commonly cited solutions (anomaly detection via machine learning) are a very difficult endeavor and results can be achieved much quicker and simpler.
&lt;br/&gt;While this only focuses on one class of timeseries (it won&#39;t work on diskspace metrics for example) I found this class to be in the most dire need of better fault detection.  Hopefully this is useful to you. Good luck and let me know how it goes!

</description>
    </item>
    
    <item>
      <title>Practical fault detection &amp; alerting.  You don&#39;t need to be a data scientist</title>
      <link>http://dieter.plaetinck.be/post/practical-fault-detection-alerting-dont-need-to-be-data-scientist/</link>
      <pubDate>Thu, 29 Jan 2015 09:08:02 -0400</pubDate>
      
      <guid>practical-fault-detection-alerting-dont-need-to-be-data-scientist</guid>
      <description>&lt;br/&gt;
As we try to retain visibility into our increasingly complicated applications and infrastructure, we&#39;re building out more advanced monitoring systems.
Specifically, a lot of work is being done on alerting via fault and anomaly detection.
This post covers some common notions around these new approaches, debunks some of the myths that ask for over-complicated solutions, and provides some practical pointers that any programmer or sysadmin can implement that don&#39;t require becoming a data scientist.
&lt;!--more--&gt;
&lt;br/&gt;
&lt;br/&gt;

&lt;h2&gt;It&#39;s not all about math&lt;/h2&gt;
&lt;p&gt;
I&#39;ve seen smart people who are good programmers decide to tackle anomaly detection on their timeseries metrics.
(anomaly detection is about building algorithms which spot &#34;unusual&#34; values in data, via statistical frameworks).  This is a good reason to brush up on statistics, so you can apply some of those concepts.
But ironically, in doing so, they often seem to think that they are now only allowed to implement algebraic mathematical formulas. No more if/else, only standard deviations of numbers.  No more for loops, only moving averages. And so on.
&lt;br/&gt;When going from thresholds to something (&lt;i&gt;anything&lt;/i&gt;) more advanced, suddenly people only want to work with mathematical formula&#39;s.  Meanwhile we have entire Turing-complete programming languages available, which allow us to execute any logic, as simple or as rich as we can imagine.  Using only math massively reduces our options in implementing an algorithm.
&lt;br/&gt;
&lt;br/&gt;For example I&#39;ve seen several presentations in which authors demonstrate how they try to fine-tune moving average algorithms and try to get a robust base signal to check against but which is also not affected too much by previous outliers, which raise the moving average and might mask subsequent spikes).
&lt;br/&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/fault-detection-moving-average.png&#34;&gt;
from &lt;a href=&#34;https://speakerdeck.com/astanway/a-deep-dive-into-monitoring-with-skyline&#34;&gt;A Deep Dive into Monitoring with Skyline&lt;/a&gt;
&lt;br/&gt;
&lt;br/&gt;
But you can&#39;t optimize both, because a mathematical formula at any given point can&#39;t make the distinction between past data that represents &#34;good times&#34; versus &#34;faulty times&#34;.
&lt;br/&gt;However: we wrap the output of any such algorithm with some code that decides what is a fault (or &#34;anomaly&#34; as labeled here) and alerts against it, so why would we hold ourselves back in feeding this useful information back into the algorithm?
&lt;br/&gt;I.e. &lt;b&gt;assist the math with logic&lt;/b&gt; by writing some code to make it work better for us:  In this example, we could modify the code to just retain the old moving average from before the time-frame we consider to be faulty.  That way, when the anomaly passes, we resume &#34;where we left off&#34;.  For timeseries that exhibit seasonality and a trend, we need to do a bit more, but the idea stays the same.   Restricting ourselves to only math and statistics cripples our ability to detect actual &lt;b&gt;faults&lt;/b&gt; (problems).
&lt;/p&gt;
&lt;p&gt;
Another example: During his &lt;a href=&#34;https://coderanger.net/talks/echo/&#34;&gt;Monitorama talk&lt;/a&gt;, Noah Kantrowitz made the interesting and thought provoking observation that Nagios flap detection is basically a low-pass filter.  A few people suggested re-implementing flap detection as a low-pass filter.  This seems backwards to me because reducing the problem to a pure mathematical formula loses information.  The current code has the high-resolution view of above/below threshold and can visualize as such.  Why throw that away and limit your visibility?
&lt;/p&gt;

&lt;h2&gt;Unsupervised machine learning... let&#39;s not get ahead of ourselves.&lt;/h2&gt;
&lt;a href=&#34;https://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;Etsy&#39;s Kale&lt;/a&gt; has ambitious goals: you configure a set of algorithms, and those algorithms get applied to &lt;b&gt;all&lt;/b&gt; of your timeseries.  Out of that should come insights into what&#39;s going wrong.  The premise is that the found anomalies are relevant and indicative of faults that require our attention.
&lt;br/&gt;I have quite a variety amongst my metrics.  For example diskspace metrics exhibit a sawtooth pattern (due to constant growth and periodic cleanup),
crontabs cause (by definition) periodic spikes in activity, user activity causes a fairly smooth graph which is characterized by its daily pattern and often some seasonality and a long-term trend.
&lt;br/&gt;
&lt;br/&gt;
&lt;img width=&#34;70%&#34; src=&#34;http://dieter.plaetinck.be/files/anomaly-detection-cases.png&#34;&gt;
&lt;br/&gt;
&lt;br/&gt;Because they look differently, anomalies and faults look different too.  In fact, within each category there are multiple problematic scenarios. (e.g. user activity based timeseries should not suddenly drop, but also not be significantly lower than other days, even if the signal stays smooth and follows the daily rhythm)
&lt;br/&gt;
&lt;br/&gt;I have a hard time believing that running the same algorithms on all of that data, and doing minimal configuration on them, will produce meaningful results. At least I expect a very low signal/noise ratio.  Unfortunately, of the people who I&#39;ve asked about their experiences with Kale/Skyline, the only cases where it&#39;s been useful is where skyline input has been restricted to a certain category of metrics - it&#39;s up to you do this filtering (perhaps via carbon-relay rules), potentially running multiple skyline instances - and sufficient time is required hand-selecting the appropriate algorithms to match the data.  This reduces the utility.
&lt;br/&gt;&#34;Minimal configuration&#34; sounds great but this doesn&#39;t seem to work.
&lt;br/&gt;
Instead, something like &lt;a href=&#34;http://bosun.org/&#34;&gt;Bosun&lt;/a&gt; (see further down) where you can visualize your series, experiment with algorithms and see the results in place on current and historical data, to manage alerting rules seems more practical.
&lt;br/&gt;
&lt;br/&gt;Some companies (all proprietary) take it a step further and pay tens of engineers to work on algorithms that inspect all of your series, classify them into categories, &#34;learn&#34; them and automatically configure algorithms that will do anomaly detection, so it can alert anytime something looks unusual (though not necessarily faulty).
This probably works fairly well, but has a high cost, still can&#39;t know everything there is to know about your timeseries, is of no help if your timeseries is behaving faulty from the start and still alerts on anomalous, but irrelevant outliers.
&lt;br/&gt;
&lt;br/&gt;

I&#39;m &lt;b&gt;suggesting we don&#39;t need to make it that fancy&lt;/b&gt; and we can do much better by &lt;b&gt;injecting some domain knowledge&lt;/b&gt; into our monitoring system:
&lt;ul&gt;
&lt;li&gt;using minimal work of classifying metrics via metric meta-data or rules that parse metric id&#39;s, we can automatically infer knowledge of how the series is supposed to behave (e.g. assume that disk_mb_used looks like sawtooth, frontend_requests_per_s daily seasonal, etc) and apply fitting processing accordingly.
&lt;br/&gt;Any sysadmin or programmer can do this, it&#39;s a bit of work but should make a hands-off automatic system such as Kale more accurate.
&lt;br/&gt;Of course, adopting &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics 2.0&lt;/a&gt; will help with this as well. Another problem with machine learning is they would have to infer how metrics relate against each other, whereas with metric metadata this can easily be inferred (e.g.: what are the metrics for different machines in the same cluster, etc)&lt;/li&gt;
&lt;li&gt;hooking into service/configuration management: you probably already have a service, tool, or file that knows how your infrastructure looks like and which services run where.  We know where user-facing apps run, where crontabs run, where we store log files, where and when we run cleanup jobs.  We know in what ratios traffic is balanced across which nodes, and so on.  Alerting systems can leverage this information to apply better suited fault detection rules.  And you don&#39;t need a large machine learning infrastructure for it. (as an aside: I have a lot more ideas on cloud-monitoring integration)&lt;/li&gt;
&lt;li&gt;Many scientists are working on algorithms that find cause and effect when different series exhibit anomalies, so they can send more useful alerts.  But again here, a simple model of the infrastructure gives you service dependencies in a much easier way.&lt;/li&gt;
&lt;li&gt;hook into your event tracking. If you have something like &lt;a href=&#34;https://github.com/Dieterbe/anthracite/&#34;&gt;anthracite&lt;/a&gt; that lists upcoming press releases, then your monitoring system knows not to alert if suddenly traffic is a bit higher.  In fact, you might want to alert if your announcement did not create a sudden increase in traffic.  If you have a large scale infrastructure, you might go as far as tagging upcoming maintenance windows with metadata so the monitoring knows which services or hosts will be affected (and which shouldn&#39;t).
&lt;/ul&gt;
&lt;br/&gt;
Anomaly detection is useful if you don&#39;t know what you&#39;re looking for, or providing an extra watching eye on your log data.  Which is why it&#39;s commonly used for detecting fraud in security logs and such.
For operational metrics of which admins know what they mean, should and should not look like, and how they relate to each other, we can build more simple and more effective solutions.


&lt;h2&gt;The trap of complex event processing... no need to abandon familiar tools&lt;/h2&gt;
On your quest into better alerting, you soon read and hear about real-time stream processing, and CEP (complex event processing) systems.
It&#39;s not hard to be convinced on their merits:  who wouldn&#39;t want real-time as-soon-as-the-data-arrives-you-can-execute-logic-and-fire-alerts?
&lt;br/&gt;They also come with a fairly extensive and flexible language that lets you program or compose monitoring rules using your domain knowledge.
I believe I&#39;ve heard of &lt;a href=&#34;https://storm.apache.org/&#34;&gt;storm&lt;/a&gt; for monitoring, but &lt;a href=&#34;http://riemann.io/&#34;&gt;Riemann&lt;/a&gt; is the best known of these tools that focus on open source monitoring.
It is a nice, powerful tool and probably the easiest of the CEP tools to adopt.  It can also produce very useful dashboards.
However, these tools come with their own API or language, and programming against real-time streams is quite a paradigm shift which can be hard to justify.  And while their architecture and domain specificity works well for large scale situations, these benefits aren&#39;t worth it for most (medium and small) shops I know:  it&#39;s a lot easier (albeit less efficient) to just query a datastore over and over and program in the language you&#39;re used to.  With a decent timeseries store (or one written to hold the most recent data in memory such as &lt;a href=&#34;https://github.com/dgryski/carbonmem&#34;&gt;carbonmem&lt;/a&gt;) this is not an issue, and the difference in timeliness of alerts becomes negligible!


&lt;h2&gt;An example: finding spikes&lt;/h2&gt;
Like many places, we were stuck with static thresholds, which don&#39;t cover some common failure scenarios.
So I started asking myself some questions:
&lt;br&gt;
&lt;br&gt;
&lt;center&gt;
    &lt;i&gt;which behavioral categories of timeseries do we have, what kind of issues can arise in each category,
        &lt;br/&gt;how does that look like in the data, and what&#39;s the simplest way I can detect each scenario?&lt;/i&gt;
&lt;/center&gt;
&lt;br/&gt;
Our most important data falls within the user-driven category from above where various timeseries from across the stack are driven by, and reflect user activity.  And within this category, the most common problem (at least in my experience) is spikes in the data.  I.e. a sudden drop in requests/s or a sudden spike in response time.  As it turned out, this is much easier to detect than one might think:
&lt;br/&gt;
&lt;img style=&#34;float:left; margin:15px;&#34; src=&#34;http://dieter.plaetinck.be/files/poor-mans-fault-detection.png&#34;&gt;
&lt;br/&gt;
In this example I just track the standard deviation of a moving window of 10 points.  &lt;a href=&#34;http://en.wikipedia.org/wiki/Standard_deviation&#34;&gt;Standard deviation&lt;/a&gt; is simply a measure of how much numerical values differ from each other.  Any sudden spike bumps the standard deviation.   We can then simply set a threshold on the deviation.  Fairly trivial to set up, but has been highly effective for us.
&lt;br/&gt;
&lt;br/&gt;You do need to manually declare what is an acceptable standard deviation value to be compared against, which you will typically deduce from previous data.  This can be annoying until you build an interface to speed up, or a tool to automate this step.
&lt;br/&gt;In fact, it&#39;s trivial to collect previous deviation data (e.g. from the same time of the day, yesterday, or the same time of the week, last week) and automatically use that to guide a threshold.  (&lt;a href=&#34;http://bosun.org/&#34;&gt;Bosun&lt;/a&gt; - see the following section - has &#34;band&#34; and &#34;graphiteBand&#34; functions to assist with this).  This is susceptible to previous outliers, but you can easily select multiple previous timeframes to minimize this issue in practice.
&lt;br/&gt;
&lt;a href=&#34;https://groups.google.com/forum/#!topic/it-telemetry/Zb2H4DP6qtk&#34;&gt;it-telemetry thread&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
So without requiring fancy anomaly detection, machine learning, advanced math, or event processing, we are able to reliably detect faults using simple, familiar tools.  Some basic statistical concepts (standard deviation, moving average, etc) are a must, but nothing that requires getting a PhD.  In this case I&#39;ve been using &lt;a href=&#34;http://graphite.readthedocs.org/en/0.9.10/functions.html#graphite.render.functions.stdev&#34;&gt;Graphite&#39;s stdev function&lt;/a&gt; and &lt;a href=&#34;http://vimeo.github.io/graph-explorer/&#34;&gt;Graph-Explorer&#39;s&lt;/a&gt; alerting feature to manage these kinds of rules, but it doesn&#39;t allow for a very practical iterative workflow, so the non-trivial rules will be going into &lt;a href=&#34;http://bosun.org/&#34;&gt;Bosun&lt;/a&gt;.
&lt;br/&gt;BTW, you can also &lt;a href=&#34;http://obfuscurity.com/2012/05/Polling-Graphite-with-Nagios&#34;&gt;use a script to query Graphite from a Nagios check and do your logic&lt;/a&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;

&lt;!--
divideSeries(stdev(avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)),10),avg(keepLastValue(collectd.dfvimeopweb*.cpu.*.cpu.idle)))

why keepLastValue?
* sumSeries -&gt; none counts as 0, so you can experience big drops which would trigger anomaly detection or failover
* averageSeries -&gt; effectively ignores none values, so your accuracy can drop a lot in light of none values.

of course this masks when your monitoring breaks, so you still need something else to detect anomalies in the &#34;out-of-date-ness&#34; of your points.
--&gt;

&lt;h2&gt;Workflow is key.  A closer look at bosun&lt;/h2&gt;
One of the reasons we&#39;ve been chasing self-learning algorithms is that we have lost faith in the feasibility of a more direct approach.  We can no longer imagine building and maintaining alerting rules because we have no system that provides powerful alerting, helps us keep oversight and streamlines the process of maintaining and iteratively developing alerting.
&lt;br/&gt;I recently discovered &lt;a href=&#34;http://bosun.org/&#34;&gt;bosun&lt;/a&gt;, an alerting frontend (&#34;IDE&#34;) by Stack Exchange, &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/brandt&#34;&gt;presented at Lisa14&lt;/a&gt;.  I highly recommend watching the video.  They have identified various issues that made alerting a pain, and built a solution that makes human-controlled alerting much more doable.  We&#39;ve been using it for a month now with good results (I also gave it support for Graphite).

I&#39;ll explain its merits, and it&#39;ll also become apparent how this ties into some of the things I brought up above:
&lt;img style=&#34;float:left; margin:35px;&#34; src=&#34;http://bosun.org/public/ss_rule_timeline.png&#34; width=&#34;15%&#34;&gt;
&lt;ul&gt;
&lt;li&gt;in each rule you can query any data you need from any of your datasources (currently graphite, openTSDB, and elasticsearch).  You can call various &lt;a href=&#34;http://bosun.org/configuration.html&#34;&gt;functions, boolean logic, and math&lt;/a&gt;.  Although it doesn&#39;t expose you a full programming language, the bosun language as it stands is fairly complete, and can be extended to cover
new needs.  You choose your own alerting granularity (it can automatically instantiate alerts for every host/service/$your_dimension/... it finds within your metrics, but you can also trivially aggregate across dimensions, or both).  This makes it easy to create advanced alerts that cover a lot of ground, making sure you don&#39;t get overloaded by multiple smaller alerts.  And you can incorporate data of other entities within the system, to simply make better alerting decisions.&lt;/li&gt;
&lt;li&gt;you can define your own templates for alert emails, which can contain any html code.  You can trivially plot graphs, build tables, use colors and so on.  Clear, context-rich alerts which contain all information you need!&lt;/li&gt;
&lt;li&gt;As alluded to, the bosun authors spent a lot of time &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/brandt&#34;&gt;thinking about, and solving&lt;/a&gt; the workflow of alerting.  As you work on advanced fault detection and alerting rules you need to be able to see the value of all data (including intermediate computations) and visualize it.  Over time, you will iteratively adjust the rules to become better and more precise.  Bosun supports all of this.  You can execute your rules on historical data and see exactly how the rule performs over time, by displaying the status in a timeline view and providing intermediate values.  And finally, you can see how the alert emails will be rendered &lt;i&gt;as you work on the rule and the templates&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;

The &lt;a href=&#34;http://bosun.org/examples.html&#34;&gt;examples&lt;/a&gt; section gives you an idea of the things you can do.
&lt;br/&gt;I haven&#39;t seen anything solve a pragmatic alerting workflow like bosun (hence their name &#34;alerting IDE&#34;), and its ability to not hold you back as you work on your alerts is refreshing. Furthermore, the built-in processing functions are very &lt;b&gt;complimentary to graphite&lt;/b&gt;:
Graphite has a decent API which works well at aggregating and transforming one or more series into one new series; the bosun language is great at reducing series to single numbers, providing boolean logic, and so on, which you need to declare alerting expressions.  This makes them a great combination.
&lt;br/&gt;Of course Bosun isn&#39;t perfect either.  Plenty of things can be done to make it (and alerting in general) better.  But it does exemplify many of my points, and it&#39;s a nice leap forward in our monitoring toolkit.

&lt;h2&gt;Conclusion&lt;/h2&gt;
Many of us aren&#39;t ready for some of the new technologies, and some of the technology isn&#39;t - and perhaps never will be - ready for us.
As an end-user investigating your options, it&#39;s easy to get lured in a direction that promotes over-complication and stimulates your inner scientist but just isn&#39;t realistic.
&lt;br/&gt;Taking a step back, it becomes apparent we &lt;b&gt;can&lt;/b&gt; setup automated fault detection.  But instead of using machine learning, use metadata, instead of trying to come up with all-encompassing holy grail of math, use several rules of code that you prototype and iterate over, then reuse for similar cases.  Instead of requiring a paradigm shift, use a language you&#39;re familiar with.  Especially by polishing up the workflow, we can make many &#34;manual&#34; tasks much easier and quicker.  I believe we can keep polishing up the workflow, distilling common patterns into macros or functions that can be reused, leveraging metric metadata and other sources of truth to configure fault detection, and perhaps even introducing &#34;metrics coverage&#34;, akin to &#34;code coverage&#34;: verify how much, and which of the metrics are adequately represented in alerting rules, so we can easily spot which metrics have yet to be included in alerting rules.  I think there&#39;s a lot of things we can do to make fault detection work better for us, but we have to look in the right direction.

&lt;h2&gt;PS: leveraging metrics 2.0 for anomaly detection&lt;/h2&gt;
In my last &lt;a href=&#34;https://www.usenix.org/conference/lisa14/conference-program/presentation/plaetinck&#34;&gt;metrics 2.0 talk, at LISA14&lt;/a&gt; I explored a few ideas on leveraging metrics 2.0 metadata for alerting and fault detection, such as automatically discovering error metrics across the stack, getting high level insights via tags, correlation, etc. If you&#39;re interested, it&#39;s in the video from 24:55 until 29:40
&lt;br/&gt;
&lt;br/&gt;
&lt;center&gt;
&lt;img src=&#34;http://dieter.plaetinck.be/files/metrics20-alerting.png&#34; width=&#34;50%&#34;&gt;
&lt;/center&gt;

&lt;!--
It&#39;s not about alerts anyway.

alerts are an immensely crude approach to raising operator awareness.
They are basically boolean: either they interrupt your workflow or they don&#39;t.  There&#39;s no in between.
Yes, you can just check your alert emails &#34;once in a while&#34;, but then realize that after an email or text is sent,
there is no way to update them with new information.  Which is really limiting once you start thinking about it.
Updates have to be provided via new &#34;alerts&#34;, or they are available in the monitoring interface but there&#39;s no way to tell
by just glancing at your alert overview.  You might be looking at very out of date information.
-&gt; only sent alerts for critical things.
--&gt;
</description>
    </item>
    
    <item>
      <title>IT-Telemetry Google group.  Trying to foster more collaboration around operational insights.</title>
      <link>http://dieter.plaetinck.be/post/it-telemetry-google-group-collaboration-operational-insights/</link>
      <pubDate>Sat, 06 Dec 2014 16:01:02 -0400</pubDate>
      
      <guid>it-telemetry-google-group-collaboration-operational-insights</guid>
      <description>The discipline of collecting infrastructure &amp; application performance metrics, aggregation, storage, visualizations and alerting has many terms associated with it...  Telemetry. Insights engineering.  Operational visibility.
I&#39;ve seen a bunch of people present their work in advancing the state of the art in this domain:  
&lt;br/&gt;from &lt;a href=&#34;http://mabrek.github.io/&#34;&gt;Anton Lebedevich&#39;s statistics for monitoring series&lt;/a&gt;, &lt;a href=&#34;https://vimeo.com/95069158&#34;&gt;Toufic Boubez&#39; talks on anomaly detection&lt;/a&gt; and Twitter&#39;s work on &lt;a href=&#34;https://blog.twitter.com/2014/breakout-detection-in-the-wild&#34;&gt;detecting mean shifts&lt;/a&gt; to projects such as &lt;a href=&#34;http://flapjack.io/&#34;&gt;flapjack&lt;/a&gt; (which aims to offload the alerting responsibility from your monitoring apps), the &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics 2.0 standardization effort&lt;/a&gt; or &lt;a href=&#34;https://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;Etsy&#39;s Kale stack&lt;/a&gt; which tries to bring interesting changes in timeseries to your attention with minimal configuration.
&lt;br/&gt;
&lt;br/&gt;

Much of this work is being shared via conference talks and blog posts, especially around anomaly and fault detection, and I couldn&#39;t find a location for collaboration, quicker feedback and discussions on more abstract (algorithmic/mathematical) topics or those that cross project boundaries.  So I created the &lt;a href=&#34;https://groups.google.com/forum/#!forum/it-telemetry&#34;&gt;IT-telemetry&lt;/a&gt; Google group.  If I missed something existing, let me know.  I can shut this down and point to whatever already exists.  Either way I hope this kind of avenue proves useful to people working on these kinds of problems.
</description>
    </item>
    
    <item>
      <title>A real whisper-to-InfluxDB program.</title>
      <link>http://dieter.plaetinck.be/post/a-real-whisper-to-influxdb-program/</link>
      <pubDate>Tue, 30 Sep 2014 08:37:48 -0400</pubDate>
      
      <guid>a-real-whisper-to-influxdb-program</guid>
      <description>The &lt;a href=&#34;http://dieter.plaetinck.be/graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay.html&#34;&gt;whisper-to-influxdb migration script&lt;/a&gt; I posted earlier is pretty bad.  A shell script, without concurrency, and an undiagnosed performance issue.
I hinted that one could write a Go program using the unofficial &lt;a href=&#34;https://github.com/kisielk/whisper-go&#34;&gt;whisper-go&lt;/a&gt; bindings and the &lt;a href=&#34;https://github.com/influxdb/influxdb/tree/master/client&#34;&gt;influxdb Go client library&lt;/a&gt;.
That&#39;s what I did now, it&#39;s at &lt;a href=&#34;https://github.com/vimeo/whisper-to-influxdb&#34;&gt;github.com/vimeo/whisper-to-influxdb&lt;/a&gt;.
It uses configurable amounts of workers for both whisper fetches and InfluxDB commits,
but it&#39;s still a bit naive in the sense that it commits to InfluxDB one serie at a time, irrespective of how many records are in it.
My series, and hence my commits have at most 60k records, and presumably InfluxDB could handle a lot more per commit, so we might leverage better batching later.  Either way, this way I can consistently commit about 100k series every 2.5 hours (or 10/s), where each serie has a few thousand points on average, with peaks up to 60k points. I usually play with 1 to 30 InfluxDB workers. 
Even though I&#39;ve hit a few &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/985&#34;&gt;InfluxDB&lt;/a&gt; &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/970&#34;&gt;issues&lt;/a&gt;, this tool has enabled me to fill in gaps after outages and to do a restore from whisper after a complete database wipe.

</description>
    </item>
    
    <item>
      <title>InfluxDB as a graphite backend, part 2</title>
      <link>http://dieter.plaetinck.be/post/influxdb-as-graphite-backend-part2/</link>
      <pubDate>Wed, 24 Sep 2014 07:56:01 -0400</pubDate>
      
      <guid>influxdb-as-graphite-backend-part2</guid>
      <description>&lt;br&gt;
&lt;br/&gt;Updated oct 1, 2014 with a new &lt;i&gt;Disk space efficiency&lt;/i&gt; section which fixes some mistakes and adds more clarity.
&lt;br/&gt;

&lt;p&gt;
The &lt;i&gt;Graphite + InfluxDB&lt;/i&gt; series continues.
&lt;ul&gt;
&lt;li&gt;In part 1, &lt;a href=&#34;http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html&#34;&gt;&#34;On Graphite, Whisper and InfluxDB&#34;&lt;/a&gt; I described the problems of Graphite&#39;s whisper and ceres, why I disagree with common graphite clustering advice as being the right path forward, what a great timeseries storage system would mean to me, why InfluxDB - despite being the youngest project - is my main interest right now, and introduced my approach for combining both and leveraging their respective strengths: InfluxDB as an ingestion and storage backend (and at some point, realtime processing and pub-sub) and graphite for its renown data processing-on-retrieval functionality.
Furthermore, I introduced some tooling: &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt; to easily route streams of carbon data (metrics datapoints) to storage backends, allowing me to send production data to Carbon+whisper as well as InfluxDB in parallel, &lt;a href=&#34;https://github.com/brutasse/graphite-api&#34;&gt;graphite-api&lt;/a&gt;, the simpler Graphite API server, with &lt;a href=&#34;https://github.com/vimeo/graphite-influxdb&#34;&gt;graphite-influxdb&lt;/a&gt; to fetch data from InfluxDB.
&lt;/li&gt;
&lt;li&gt;Not Graphite related, but I wrote &lt;a href=&#34;https://github.com/Dieterbe/influx-cli&#34;&gt;influx-cli&lt;/a&gt; which I introduced &lt;a href=&#34;http://dieter.plaetinck.be/influx-cli_a_commandline_interface_to_influxdb.html&#34;&gt;here&lt;/a&gt;.  It allows to easily interface with InfluxDB and measure the duration of operations, which will become useful for this article.&lt;/li&gt;
&lt;li&gt;In the &lt;a href=&#34;graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay.html&#34;&gt;Graphite &amp;amp; Influxdb intermezzo&lt;/a&gt; I shared a script to import whisper data into InfluxDB and noted some write performance issues I was seeing, but the better part of the article described the various improvements done to &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt;, which is becoming an increasingly versatile and useful tool.&lt;/li&gt;
&lt;li&gt;In &lt;a href=&#34;http://dieter.plaetinck.be/using-influxdb-as-graphite-backend-part2.html&#34;&gt;part 2&lt;/a&gt;, which you are reading now, I&#39;m going to describe recent progress, share more info about my setup, testing results, state of affairs, and ideas for future work&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;

&lt;h4&gt;Progress made&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;InfluxDB saw two major releases:
&lt;ul&gt;
&lt;li&gt;0.7 (and followups), which was mostly about some needed features and bug fixes&lt;/li&gt;
&lt;li&gt;0.8 was all about bringing some major refactorings in the hands of early adopters/testers: support for multiple storage engines, configurable shard spaces, rollups and retention schemes. There was some other useful stuff like speed and robustness improvements for the graphite input plugin (by yours truly) and various things like regex filtering for &#39;list series&#39;.  Note that a bunch of older bugs remained open throughout this release (most notably the broken &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/334&#34;&gt;derivative aggregator&lt;/a&gt;), and a bunch of new ones appeared. Maybe this is why the release was mostly in the dark.  In this context, it&#39;s not so bad, because we let graphite-api do all the processing, but if you want to query InfluxDB directly you might hit some roadblocks.&lt;/li&gt;
&lt;li&gt;An older fix, but worth mentioning: series names can now also contain any character, which means you can easily use &lt;a href=&#34;http://metrics20.org/&#34;&gt;metrics2.0&lt;/a&gt; identifiers.  This is a welcome relief after having struggled with Graphite&#39;s restrictions on metric keys.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://graphite-api.readthedocs.org&#34;&gt;graphite-api&lt;/a&gt; received various bug fixes and support for templating, statsd instrumentation and caching.
&lt;br/&gt;Much of this was driven by graphite-influxdb: the caching allows us to cache metadata and the statsd integration gives us insights into the performance of the steps it goes through of building a graph (getting metadata from InfluxDB, querying InfluxDB, interacting with cache, post processing data, etc).&lt;/li&gt;
&lt;li&gt;the progress on InfluxDB and graphite-api in turn enabled &lt;a href=&#34;https://github.com/vimeo/graphite-influxdb&#34;&gt;graphite-influxdb&lt;/a&gt; to become faster and simpler (note: graphite-influxdb requires InfluxDB 0.8).  Furthermore you can now configure series resolutions (but different retentions per serie is on the roadmap, see &lt;i&gt;State of affairs and what&#39;s coming&lt;/i&gt;), and of course it also got a bunch of bugfixes.&lt;/li&gt;
&lt;/ul&gt;
Because of all these improvements, all involved components are now ready for serious use.

&lt;h4&gt;Putting it all together, with docker&lt;/h4&gt;
&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; probably needs no introduction, it&#39;s a nifty tool to build an environment with given software installed, and allows to easily deploy it and run it in isolation.
&lt;a href=&#34;https://github.com/vimeo/graphite-api-influxdb-docker&#34;&gt;graphite-api-influxdb-docker&lt;/a&gt; is a very creatively named project that generates the - also very creatively named - docker image &lt;a href=&#34;https://registry.hub.docker.com/u/vimeo/graphite-api-influxdb/&#34;&gt;graphite-api-influxdb&lt;/a&gt;, which contains graphite-api and graphite-influxdb, making it easy to hook in a customized configuration and get it up and running quickly.  This is the recommended way to set this up, and this is what we run in production.

&lt;h4&gt;The setup&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;a server running InfluxDB and graphite-api with graphite-influxdb via the docker approach described above:
&lt;pre&gt;
dell PowerEdge R610
24 x Intel(R) Xeon(R) X5660  @ 2.80GHz
96GB RAM
perc raid h700
6x600GB seagate 10k rpm drives in raid10 = 1.6 TB, Adaptive Read Ahead, Write Back, 64 kB blocks, no read caching
no sharding/shard spaces, compiled from git just before 0.8, using LevelDB (not rocksdb, which is now the default)
LevelDB max-open-files = 10000 (lsof shows about 30k open files total for the InfluxDB process), LRU 4096m, everything else is default I think.
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;a server running graphite-web, carbon, and whisper:
&lt;pre&gt;
dell PowerEdge R710
16 x Intel(R) Xeon(R) E5640  @ 2.67GHz
96GB RAM
perc raid h700
8x150GB seagate 15k rm in raid5 = 952 GB, Read Ahead, Write Back, 64 kB blocks, no read caching
MAX_UPDATES_PER_SECOND = 1000  # to sequentialize writes
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;a relay server running carbon-relay-ng that sends the same production load into both.  (about 2500 metrics/s, or 150k minutely)&lt;/li&gt;
&lt;/ul&gt;
As you can tell, on both machines RAM is vastly over provisioned, and they have lots of cpu available (the difference in cores should be negligible), but the difference in RAID level is important to note: RAID 5 comes with a write penalty. Even though the whisper machine has more, and faster disks, it probably has a disadvantage for writes.  Maybe.  Haven&#39;t done raid stuff in a long time, and I haven&#39;t it measured it out.
&lt;br/&gt;&lt;b&gt;Clearly you&#39;ll need to take the results with a grain of salt, as unfortunately I do not have 2 systems available with the same configuration and their baseline (raw) performance is unknown.&lt;/b&gt;.
&lt;br/&gt;Note: no InfluxDB clustering, see &lt;i&gt;State of affairs and what&#39;s coming&lt;/i&gt;.

&lt;h4&gt;The empirical validation &amp;amp; migration&lt;/h4&gt;
Once everything was setup and I could confidently send 100% of traffic to InfluxDB via carbon-relay-ng, it was trivial to run our dashboards with a flag deciding which server to go to.
This way I have literally been running our graphite dashboards next to each other, allowing us to compare both stacks on:
&lt;ul&gt;
&lt;li&gt;visual differences: after a bunch of work and bug fixing, we got to a point where both dashboards looked almost exactly the same.  (note that graphite-api&#39;s implementation of certain functions can behave slightly different, see for example this &lt;a href=&#34;https://github.com/brutasse/graphite-api/issues/66&#34;&gt;divideSeries bug&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;speed differences by simply refreshing both pages and watching the PNGs load, with some assistance from firebug&#39;s network requests profiler.  The difference here was big: graphs served up by graphite-api + InfluxDB loaded considerably faster.  A page with 40 graphs or so would load in a few seconds instead of 20-30 seconds (on both first, as well as subsequent hits).  This is for our default, 6-hour timeframe views.  When cranking the timeframes up to a couple of weeks, graphite-api + InfluxDB was still faster.&lt;/li&gt;
&lt;/ul&gt;
Soon enough my colleagues started asking to make graphite-api + InfluxDB the default, as it was much faster in all common cases.  I flipped the switch and everybody has been happy.
&lt;br/&gt;
&lt;br/&gt;
When loading a page with many dashboards, the InfluxDB machine will occasionally spike up to 500% cpu, though I rarely get to see any iowait (!), even after syncing the block cache (i just realized it&#39;ll probably still use the cache for reads after sync?)
&lt;br/&gt;The carbon/whisper machine, on the other hand, is always fighting iowait, which could be caused by the raid 5 write amplification but the random io due to the whisper format probably has more to do with it.  Via the MAX_UPDATES_PER_SECOND I&#39;ve tried to linearize writes, with mixed success.  But I&#39;ve never gone to deep into it.  So basically &lt;b&gt;comparing write performance would be unfair in these circumstances, I am only comparing reads in these tests&lt;/b&gt;.  Despite the different storage setups, the Linux block cache should make things fair for reads.   Whisper&#39;s iowait will handicap the reads, but I always did successive runs with fully loaded PNGs to make sure the block cache was warm for reads.

&lt;h4&gt;A &#34;slightly more professional&#34; benchmark&lt;/h4&gt;
I could have stopped here, but the validation above was not very scientific.  I wanted to do a somewhat more formal benchmark, to measure read speeds (though I did not have much time so it had to be quick and easy).
&lt;br/&gt;I wanted to compare InfluxDB vs whisper, and specifically how performance scales as you play with parameters such as number of series, points per series, and time range fetched (i.e. amount of points).  I &lt;a href=&#34;https://groups.google.com/forum/#!topic/influxdb/0VeUQCqzgVg&#34;&gt;posted the benchmark on the InfluxDB mailing list&lt;/a&gt;.  Look there for all information. I just want to reiterate the conclusion here:  I was surprised.  Because of the results above, I had assumed that InfluxDB would perform reads noticeably quicker than whisper but this is not the case.  (maybe because whisper reads are nicely sequential - it&#39;s mostly writes that suffer from the whisper format)
&lt;br/&gt;This very much contrasts my earlier findings where the graphite-api+InfluxDB powered dashboards clearly take the lead.  I have yet to figure out why this is.  Maybe something to do with the performance of graphite-web vs graphite-api itself, gunicorn vs apache, worker configuration, or maybe InfluxDB only starts outperforming whisper as concurrency increases.  Some more investigation is definitely needed!

&lt;h4&gt;Future benchmarks&lt;/h4&gt;
The simple benchmark above was very simple to execute, as it only requires influx-cli and whisper-fetch (so you can easily check for yourself), but clearly there is a need to test more realistic scenarios with concurrent reads, and doing some write benchmarks would be nice too.
&lt;br/&gt;We should also look into cpu and memory usage.  I have had the luxury of being able to completely ignore memory usage, but others seem to notice excessive InfluxDB memory usage.
&lt;br/&gt;conclusion: many tests and benchmarks should happen, but I don&#39;t really have time to conduct them.  Hopefully other people in the community will take this on.

&lt;h4&gt;Disk space efficiency&lt;/h4&gt;
Last time I checked, using LevelDB I was pretty close to 24B per record (which makes sense because time, seq_no and value are all 64bit values, and each record has those 3 fields).  (this was with snappy compression enabled, so it didn&#39;t seem to give much benefit).
&lt;br/&gt;Whisper seems to consume 12 Bytes per record - a 32bit timestamp and a 64bit float value - making it considerably more storage efficient than InfluxDB/levelDB for now.
&lt;br/&gt;Some notes on this though:
&lt;ul&gt;
&lt;li&gt;whisper explicitly encodes None values, with InfluxDB those are implied (and require no space).  We have some clusters of metrics that have very sparse data, so whisper gives us a lot of overhead here, but this is different for everyone.  (note: Ceres should also be better at handling sparse data)&lt;/li&gt;
&lt;li&gt;Whisper and Influxdb both explictly encode the timestamp for every record.  Influxdb uses 64bit so you can do very high resolution (up to microseconds), whisper is limited to per-second data.  Ceres AFAIK doesn&#39;t explicitly encode the timestamp at every record, which should also give it a space advantage.&lt;/li&gt;
&lt;li&gt;I&#39;ve been using a data format in InfluxDB where every record is timestamp-sequence_number-value.  It currently works best overall, and so that&#39;s how the graphite ingestion plugin stores it and the graphite-influxdb plugin queries for it.  But it exacerbates the overhead of the timestamp and sequence number.
&lt;br/&gt;We could technically use a row format where we use more variables as part of the record, storing them as columns instead of separate series, which would improve this dynamic (but currently comes with a big tradeoff in performance characteristics - see the &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/582&#34;&gt;column indexes&lt;/a&gt; ticket).
&lt;br/&gt;Another thing is that we could technically come up with a storage format for InfluxDB that is optimized for even-spaced metrics, it wouldn&#39;t need sequence numbers, and timestamps could be implicit instead of explicit, saving a lot of space.  We could even go further and introduce types (int, etc) for values which would consume even less space.
&lt;/ul&gt;
&lt;br/&gt;
It would be great if somebody with more Ceres experience could chip in here, as - in the context of space efficiency - it looks like a neat little format.
Also, I&#39;m probably not making proper use of the compression features that InfluxDB&#39;s storage engines support.  This also requires some more looking into.


&lt;h4&gt;State of affairs and what&#39;s coming&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;InfluxDB typically performs pretty well, but not in all cases.  More validation is needed. It wouldn&#39;t surprise me at this point if tools like hbase/Cassandra/riak clearly outperform InfluxDB, as long as we keep in mind that InfluxDB is a young project.  A year, or two, from now, it&#39;ll probably perform much better. (and then again, it&#39;s not all about raw performance.  InfluxDB&#39;s has other strengths)&lt;/li&gt;
&lt;li&gt;A long time goal which is now a reality:  &lt;b&gt;You can use any Graphite dashboard on top of InfluxDB, as long as the data is stored in a graphite-compatible format.&lt;/b&gt;.  Again, the easiest to get running is via &lt;a href=&#34;https://github.com/vimeo/graphite-api-influxdb-docker&#34;&gt;graphite-api-influxdb-docker&lt;/a&gt;.  There are two issues to be mentioned, though:
&lt;ul&gt;
&lt;li&gt;graphite-influxdb needs to query InfluxDB for metadata, and this &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/884&#34;&gt;can be slow&lt;/a&gt;.  If you have millions of metrics, it can take tens of seconds before querying for the data even starts.  I am trying to work with the InfluxDB people on a solution.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/brutasse/graphite-api/issues/57&#34;&gt;graphite-api doesn&#39;t work with metric id&#39;s that have equals signs in them&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;li&gt;With the 0.8 release out the door, the shard spaces/rollups/retention intervals feature will start stabilizing, so we can start supporting multiple retention intervals per metric&lt;/li&gt;
&lt;li&gt;Because InfluxDB clustering is &lt;a href=&#34;https://github.com/influxdb/influxdb/pull/903&#34;&gt;undergoing major changes&lt;/a&gt;, and because clustering is not a high priority for me, I haven&#39;t needed to worry about this.  I&#39;ll probably only start looking at clustering somewhere in 2015 because I have more pressing issues.&lt;/li&gt;
&lt;li&gt;Once the new clustering system and the storage subsystem have matured (sounds like a v1.0 ~ v1.2 to me) we&#39;ll get more speed improvements and robustness.  Most of the integration work is done, it&#39;s just a matter of doing smaller improvements, bug fixes and waiting for InfluxDB to become better.  Maintaining this stack aside, I personally will start focusing more on:
    &lt;ul&gt;
    &lt;li&gt;per-second resolution in our data feeds, and potentially storage&lt;/li&gt;
    &lt;li&gt;realtime (but basic) anomaly detection, realtime graphs for some key timeseries.  Adrian Cockcroft had an inspirational piece in his &lt;a href=&#34;https://vimeo.com/95064249&#34;&gt;Monitorama keynote&lt;/a&gt; about how alerts from timeseries should trigger within seconds.&lt;/li&gt;
    &lt;li&gt;Mozilla&#39;s awesome &lt;a href=&#34;http://hekad.readthedocs.org&#34;&gt;heka&lt;/a&gt; project (this &lt;a href=&#34;https://vimeo.com/98689689&#34;&gt;heka video&lt;/a&gt; is great), which should help a lot with the above.  Also looking at &lt;a href=&#34;http://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;Etsy&#39;s kale stack&lt;/a&gt; for anomaly detection&lt;/li&gt;
    &lt;li&gt;metrics 2.0 and making sure metrics 2.0 works well with InfluxDB.  Up to now I find the series / columns as a data model too limiting and arbitrary, it could be so much more powerful, ditto for the query language.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can we do anything else to make InfluxDB (+graphite) faster? Yes!
&lt;ul&gt;
&lt;li&gt;Long term, of course, InfluxDB should have powerful enough processing functions and query syntax, so that we don&#39;t even need a graphite layer anymore.&lt;/li&gt;
&lt;li&gt;A storage engine optimized for fixed intervals would probably help, timestamps and sequence numbers currently consume 2/3 of the record... and there&#39;s no reason to explicitly store either one in this use case.  I&#39;ve even rarely seen people make use of the sequence number in any other InfluxDB use case.  See all the remarks in the &lt;i&gt;Disk space efficiency&lt;/i&gt; section above.  Finally we could have InfluxDB have fill in None values without it doing &#34;group by&#34; (timeframe consolidation), which would shave off runtime overhead.&lt;/li&gt;
&lt;li&gt;Then of course, there are projects to replace graphite-web/graphite-api with a Go codebase: &lt;a href=&#34;https://github.com/graphite-ng/graphite-ng&#34;&gt;graphite-ng&lt;/a&gt; and &lt;a href=&#34;https://github.com/dgryski/carbonapi&#34;&gt;carbonapi&lt;/a&gt;.  the latter is more production ready, but depends on some custom tooling and io using protobufs.  But it performs an order of magnitude better than the python api server!  I haven&#39;t touched graphite-ng in a while, but hopefully at some point I can take it up again&lt;/li&gt;
&lt;/ul&gt;
&lt;li&gt;Another thing to keep in mind when switching to graphite-api + InfluxDB: you loose the graphite composer.  I have a few people relying on this, so I can either patch it to talk to graphite-api (meh), separate it out (meh) or replace it with a nicer dashboard like tessera, grafana or descartes.  (or Graph-Explorer, but it can be a bit too much of a paradigm shift).&lt;/li&gt;
&lt;li&gt;some more InfluxDB stuff I&#39;m looking forward to:
&lt;ul&gt;
&lt;li&gt;binary protocol and result streaming (faster communication and responses!) (the latter might not get implemented though)&lt;/li&gt;
&lt;li&gt;&#34;list series&#34; speed improvements (if metadata querying gets fast enough, we won&#39;t need ES anymore for metrics2.0 index)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/influxdb/influxdb/pull/635&#34;&gt;InfluxDB instrumentation&lt;/a&gt; so we actually start getting an idea of what&#39;s going on in the system, a lot of the testing and troubleshooting is still in the dark.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tracking exceptions in graphite-api is &lt;a href=&#34;https://github.com/brutasse/graphite-api/search?q=exception&amp;type=Issues&amp;utf8=%E2%9C%93&#34;&gt;much harder than it should be&lt;/a&gt;.  Currently there&#39;s no way to display exceptions to the user (in the http response) or to even log them.  So sometimes you&#39;ll get http 500 responses and don&#39;t know why.  You can use the &lt;a href=&#34;http://graphite-api.readthedocs.org/en/latest/configuration.html#extra-sections&#34;&gt;sentry integration&lt;/a&gt; which works all right, but is clunky.  Hopefully this will be addressed soon.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Conclusion&lt;/h4&gt;
The graphite-influxdb stack works and is ready for general consumption.  It&#39;s easy to install and operate, and performs well.
It is expected that InfluxDB will over time mature and ultimately meet all my &lt;a href=&#34;http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html&#34;&gt;requirements of the ideal backend&lt;/a&gt;.  It definitely has a long way to go.  More benchmarks and tests are needed.  Keep in mind that we&#39;re not doing large volumes of metrics. For small/medium shops this solution should work well, but on larger scales you will definitely run into issues.  You might conclude that InfluxDB is not for you (yet) (there are alternative projects, after all).
&lt;br/&gt;
&lt;br/&gt;
Finally, a closing thought:
&lt;br/&gt;&lt;i&gt;Having graphs and dashboards that look nice and load fast is a good thing to have, but keep in mind that graphs and dashboards should be a last resort.  It&#39;s a solution if all else fails.  The fewer graphs you need, the better you&#39;re doing.
&lt;br/&gt;How can you avoid needing graphs?  Automatic alerting on your data.
&lt;br/&gt;
&lt;br/&gt;I see graphs as a temporary measure: they provide headroom while you develop an understanding of the operational behavior of your infrastructure, conceive a model of it, and implement the alerting you need to do troubleshooting and capacity planning.  Of course, this process consumes more resources (time and otherwise), and these expenses are not always justifiable, but I think this is the ideal case we should be working towards.&lt;/i&gt;

&lt;br/&gt;
&lt;br/&gt;
Either way, good luck and have fun!
</description>
    </item>
    
    <item>
      <title>Graphite &amp; Influxdb intermezzo: migrating old data and a more powerful carbon relay</title>
      <link>http://dieter.plaetinck.be/post/graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay/</link>
      <pubDate>Sat, 20 Sep 2014 15:18:32 -0400</pubDate>
      
      <guid>graphite-influxdb-intermezzo-migrating-old-data-and-a-more-powerful-carbon-relay</guid>
      <description>&lt;!--more--&gt;

&lt;h4&gt;Migrating data from whisper into InfluxDB&lt;/h4&gt;

&lt;i&gt;&#34;How do i migrate whisper data to influxdb&#34;&lt;/i&gt; is a question that comes up regularly, and I&#39;ve always replied it should be easy to write a tool
to do this.  I personally had no need for this, until a recent small influxdb outage where I wanted to sync data from our backup server (running graphite + whisper) to influxdb, so I wrote a script:

&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;#!/bin/bash&lt;/span&gt;
&lt;span style=&#34;color: #408080; font-style: italic&#34;&gt;# whisper dir without trailing slash.&lt;/span&gt;
&lt;span style=&#34;color: #19177C&#34;&gt;wsp_dir&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;/opt/graphite/storage/whisper
&lt;span style=&#34;color: #19177C&#34;&gt;start&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;$(&lt;/span&gt;date -d &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;sep 17 6am&amp;#39;&lt;/span&gt; +%s&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;)&lt;/span&gt;
&lt;span style=&#34;color: #19177C&#34;&gt;end&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;$(&lt;/span&gt;date -d &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;sep 17 12pm&amp;#39;&lt;/span&gt; +%s&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;)&lt;/span&gt;
&lt;span style=&#34;color: #19177C&#34;&gt;db&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;graphite
&lt;span style=&#34;color: #19177C&#34;&gt;pipe_path&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;$(&lt;/span&gt;mktemp -u&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;)&lt;/span&gt;
mkfifo &lt;span style=&#34;color: #19177C&#34;&gt;$pipe_path&lt;/span&gt;
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;function&lt;/span&gt; influx_updater&lt;span style=&#34;color: #666666&#34;&gt;()&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;{&lt;/span&gt;
    influx-cli -db &lt;span style=&#34;color: #19177C&#34;&gt;$db&lt;/span&gt; -async &amp;lt; &lt;span style=&#34;color: #19177C&#34;&gt;$pipe_path&lt;/span&gt;
&lt;span style=&#34;color: #666666&#34;&gt;}&lt;/span&gt;
influx_updater &amp;amp;
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color: #008000&#34;&gt;read&lt;/span&gt; wsp; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;do&lt;/span&gt;
  &lt;span style=&#34;color: #19177C&#34;&gt;series&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;$(&lt;/span&gt;basename &lt;span style=&#34;color: #BB6688; font-weight: bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;wsp&lt;/span&gt;//&lt;span style=&#34;color: #BB6622; font-weight: bold&#34;&gt;\/&lt;/span&gt;/.&lt;span style=&#34;color: #BB6688; font-weight: bold&#34;&gt;}&lt;/span&gt; .wsp&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;)&lt;/span&gt;
  &lt;span style=&#34;color: #008000&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;updating &lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$series&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt; ...&amp;quot;&lt;/span&gt;
  whisper-fetch.py --from&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$start&lt;/span&gt; --until&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$end&lt;/span&gt; &lt;span style=&#34;color: #19177C&#34;&gt;$wsp_dir&lt;/span&gt;/&lt;span style=&#34;color: #19177C&#34;&gt;$wsp&lt;/span&gt;.wsp | grep -v &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;None$&amp;#39;&lt;/span&gt; | awk &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;{print &amp;quot;insert into \&amp;quot;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$series&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;\&amp;quot; values (&amp;quot;$1&amp;quot;000,1,&amp;quot;$2&amp;quot;)&amp;quot;}&amp;#39;&lt;/span&gt; &amp;gt; &lt;span style=&#34;color: #19177C&#34;&gt;$pipe_path&lt;/span&gt;
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;done&lt;/span&gt; &amp;lt; &amp;lt;&lt;span style=&#34;color: #666666&#34;&gt;(&lt;/span&gt;find &lt;span style=&#34;color: #19177C&#34;&gt;$wsp_dir&lt;/span&gt; -name &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;*.wsp&amp;#39;&lt;/span&gt; | sed -e &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;s#&lt;/span&gt;&lt;span style=&#34;color: #19177C&#34;&gt;$wsp_dir&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;/##&amp;quot;&lt;/span&gt; -e &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;s/.wsp&lt;/span&gt;$&lt;span style=&#34;color: #BA2121&#34;&gt;//&amp;quot;&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;


It relies on the recently introduced asynchronous inserts feature of &lt;a href=&#34;https://github.com/Dieterbe/influx-cli&#34;&gt;influx-cli&lt;/a&gt; - which commits inserts in batches to improve the speed - and the whisper-fetch tool.
&lt;br/&gt;
You could probably also write a Go program using the unofficial &lt;a href=&#34;https://github.com/kisielk/whisper-go&#34;&gt;whisper-go&lt;/a&gt; bindings and the &lt;a href=&#34;https://github.com/influxdb/influxdb/tree/master/client&#34;&gt;influxdb Go client library&lt;/a&gt;.  But I wanted to keep it simple.  Especially when I found out that whisper-fetch is not a bottleneck: starting whisper-fetch, and reading out - in my case - 360 datapoints of a file always takes about 50ms, whereas InfluxDB at first only needed a few ms to flush hundreds of records, but that soon increased to seconds.
&lt;br/&gt;Maybe it&#39;s a bug in my code, I didn&#39;t test this much, because I didn&#39;t need to; but people keep asking for a tool so here you go.  Try it out and maybe you can fix a bug somewhere.  Something about the write performance here must be wrong.

&lt;h4&gt;A more powerful carbon-relay-ng&lt;/h4&gt;
&lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng&#34;&gt;carbon-relay-ng&lt;/a&gt; received a bunch of love and has been a great help in my graphite+influxdb experiments.
&lt;p&gt;
&lt;a href=&#34;http://dieter.plaetinck.be/files/carbon-relay-web-ui.png&#34;&gt;&lt;img width=&#34;441&#34; src=&#34;http://dieter.plaetinck.be/files/carbon-relay-web-ui.png&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
Here&#39;s what changed:
&lt;ul&gt;
&lt;li&gt;First I made it so that you can adjust routes at runtime while data is flowing through, via a telnet interface.&lt;/li&gt;
&lt;li&gt;Then &lt;a href=&#34;https://github.com/pauloconnor&#34;&gt;Paul O&#39;Connor&lt;/a&gt; built an embedded web interface to manage your routes in an easier and prettier way (pictured above)&lt;/li&gt;
&lt;li&gt;The relay now also emits performance metrics via statsd (I want to make this better by using &lt;a href=&#34;https://github.com/rcrowley/go-metrics&#34;&gt;go-metrics&lt;/a&gt; which will hopefully get &lt;a href=&#34;https://github.com/rcrowley/go-metrics/issues/68&#34;&gt;expvar support&lt;/a&gt; at some point - any takers?).&lt;/li&gt;
&lt;li&gt;Last but not least, I borrowed &lt;a href=&#34;https://github.com/graphite-ng/carbon-relay-ng/tree/master/nsqd&#34;&gt;the diskqueue&lt;/a&gt; code from &lt;a href=&#34;http://nsq.io/&#34;&gt;NSQ&lt;/a&gt; so now we can also spool to disk to bridge downtime of endpoints and re-fill them when they come back up&lt;/li&gt;
&lt;/ul&gt;
Beside our metrics storage, I also plan to put our anomaly detection (currently playing with &lt;a href=&#34;http://hekad.readthedocs.org/en/v0.7.1/&#34;&gt;heka&lt;/a&gt; and &lt;a href=&#34;http://codeascraft.com/2013/06/11/introducing-kale/&#34;&gt;kale&lt;/a&gt;) and &lt;a href=&#34;https://github.com/vimeo/carbon-tagger&#34;&gt;carbon-tagger&lt;/a&gt; behind the relay, centralizing all routing logic, making things more robust, and simplifying our system design.  The spooling should also help to deploy to our metrics gateways at other datacenters, to bridge outages of datacenter interconnects.
&lt;br/&gt;
&lt;br/&gt;
I used to think of carbon-relay-ng as the python carbon-relay but on steroids,
now it reminds me more of something like nsqd but with an ability to make packet routing decisions by introspecting the carbon protocol,
&lt;br/&gt;or perhaps Kafka but much simpler, single-node (no HA), and optimized for the domain of carbon streams.
&lt;br/&gt;I&#39;d like the HA stuff though, which is why I spend some of my spare time figuring out the intricacies of the increasingly popular &lt;a href=&#34;http://raftconsensus.github.io/&#34;&gt;raft&lt;/a&gt; consensus algorithm.   It seems opportune to have a simpler Kafka-like thing, in Go, using raft, for carbon streams.
(note: InfluxDB &lt;a href=&#34;https://github.com/influxdb/influxdb/pull/859&#34;&gt;might introduce such a component&lt;/a&gt;, so I&#39;m also a bit waiting to see what they come up with)
&lt;br/&gt;
&lt;br/&gt;
Reminder: notably missing from carbon-relay-ng is round robin and sharding.  I believe sharding/round robin/etc should be part of a broader HA design of the storage system, as I explained in &lt;a href=&#34;http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html&#34;&gt;On Graphite, Whisper and InfluxDB&lt;/a&gt;.  That said, both should be fairly easy to implement in carbon-relay-ng, and I&#39;m willing to assist those who want to contribute it.
</description>
    </item>
    
    <item>
      <title>Influx-cli: a commandline interface to Influxdb.</title>
      <link>http://dieter.plaetinck.be/post/influx-cli_a_commandline_interface_to_influxdb/</link>
      <pubDate>Mon, 08 Sep 2014 08:36:36 -0400</pubDate>
      
      <guid>influx-cli_a_commandline_interface_to_influxdb</guid>
      <description>&lt;p&gt;
Time for another side project:
&lt;a href=&#34;https://github.com/Dieterbe/influx-cli&#34;&gt;influx-cli&lt;/a&gt;,
a commandline interface to influxdb.
&lt;br/&gt;
Nothing groundbreaking, and it behaves pretty much as you would expect if you&#39;ve ever used
the mysql, pgsql, vsql, etc tools before.
&lt;br/&gt;But I did want to highlight a few interesting features.
&lt;/p&gt;
&lt;!--more--&gt;
&lt;br/&gt;

&lt;p&gt;
&lt;b&gt;You can do things like user management via SQL,
even though influxdb doesn&#39;t have an SQL interface for this.&lt;/b&gt;
&lt;br/&gt;This is much easier than doing curl http requests!
&lt;pre&gt;
influx&gt; create admin test test
influx&gt; list admin
## 0
                     name root
## 1
                     name test
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;You can change parameters and re-bind with the new values&lt;/b&gt;
&lt;pre&gt;
influx&gt; \user test
influx&gt; \pass test
influx&gt; \db graphite
influx&gt; bind
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Write your variables (user, pass, host, db, ...) to ~/.influxrc&lt;/b&gt;
&lt;pre&gt;
influx&gt; writerc
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;You can even do inserts via SQL, instead of http posts&lt;/b&gt;
&lt;br&gt;I use this often.  This is very useful to script test cases for bug reports etc.
&lt;pre&gt;
influx&gt; create db issue-1234
influx&gt; \db issue-1234
influx&gt; bind
influx&gt; insert into demo (time, value, tag) values (120000, 10, &#34;hi&#34;)
influx&gt; insert into demo (time, value, tag) values (180000, 20, &#34;hi again&#34;)
influx&gt; select * from demo
## demo
                time sequence_number               value                 tag
       120000.000000      70001.000000                  10                &#34;hi&#34;
       180000.000000      80001.000000                  20          &#34;hi again&#34;
influx&gt; delete db issue-1234
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;You can send queries on standard input, which is useful in shell commands and scripts.&lt;/b&gt;
&lt;pre&gt;
$ echo &#39;list series&#39; | influx-cli | wc -l
194722
$ influx-cli &lt;&lt;&lt; &#39;list series&#39; | wc -l
194722
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
But even better, &lt;b&gt;from inside an influx-cli session, you can send output from any query into any other command.&lt;/b&gt;
&lt;br&gt;In fact you can also &lt;b&gt;write output of queries into external files.&lt;/b&gt;
All this via familiar shell constructs
&lt;pre&gt;
$ influx-cli
influx&gt; list series | wc -l
194721
influx&gt; list series &gt; list-series.txt
&lt;/pre&gt;

(note: the discrepancy of one line is due to &lt;a href=&#34;https://github.com/shavac/readline/issues/2&#34;&gt;the Go readline library echoing the query&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;You can also toggle options, such as compression or display of timings.&lt;/b&gt;
&lt;br/&gt;This can be very useful to easily get insights of performance of different operations.
&lt;pre&gt;
influx&gt; \t
timing is now true
influx&gt; select * from foo | wc -l
64637
timing&gt;
query+network: 1.288792048s
displaying   : 457.091811ms
influx&gt; \comp
compression is now disabled
influx&gt; select * from foo | wc -l
64637
timing&gt;
query+network: 969.322374ms
displaying   : 670.736018ms
influx&gt; list series &gt;/dev/null
timing&gt;
query+network: 3.109178142s
displaying   : 65.712027ms
&lt;/pre&gt;
&lt;br/&gt;This has enabled me to pinpoint slow operations and provide evidence when &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/884&#34;&gt;when creating tickets&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Executing queries and debugging their result data format, works too&lt;/b&gt;
&lt;br/&gt;This is useful when you want to understand the api better or if the database gets support for new queries with a different output format that influx-cli doesn&#39;t understand yet.
&lt;pre&gt;
influx&gt; raw select * from foo limit 1
([]*client.Series) (len=1 cap=4) {
 (*client.Series)(0xc20b4f0480)({
  Name: (string) (len=51) &#34;foo&#34;,
  Columns: ([]string) (len=3 cap=4) {
   (string) (len=4) &#34;time&#34;,
   (string) (len=15) &#34;sequence_number&#34;,
   (string) (len=5) &#34;value&#34;
  },
  Points: ([][]interface {}) (len=1 cap=4) {
   ([]interface {}) (len=3 cap=4) {
    (float64) 1.410148588e+12,
    (float64) 1,
    (float64) 95.549995
   }
  }
 })
}
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
And that&#39;s about it.
I&#39;ve found this to be a much easier way to interface with InfluxDB then using the web interface and curl, but YMMV.
&lt;br/&gt;If you were wondering, this is of course built on top of the &lt;a href=&#34;https://github.com/influxdb/influxdb/tree/master/client&#34;&gt;influxdb go client library&lt;/a&gt;, which was overall pretty pleasant to work with.
&lt;br/&gt;Some ideas for future work:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Dieterbe/influx-cli/issues/2&#34;&gt;bulk insert performance could be better&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;once &lt;a href=&#34;https://github.com/influxdb/influxdb/issues/263&#34;&gt;influxdb can report query execution time&lt;/a&gt; and hopefully also serialization time, the timing output can be more useful.  Right now we can only measure query execution+serialization+network transfer time combined&lt;/li&gt;
&lt;li&gt;my gut feeling says that using something like msgpack instead of json, and/or even streaming the resultset as it is being generated (instead of first building the entire result, then serializing it, then sending it over, then having the client deserialize the entire thing) could really help performance, not just here, but basically anywhere you interface with influxdb.  Though I don&#39;t have hard numbers on this yet.&lt;/li&gt;
&lt;/p&gt;

</description>
    </item>
    
  </channel>
</rss>
